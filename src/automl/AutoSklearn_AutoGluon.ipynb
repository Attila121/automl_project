{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc729b0f-7ab3-48a7-ac0b-0e93e1dd8d75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 13:02:34,788\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.0 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-07-15 13:02:35,131\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.0 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import autosklearn.regression\n",
    "\n",
    "from flaml import AutoML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d75684-9a3c-41e9-b6b4-4b496d1cc9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba817d9-b7fa-4a26-958b-b2b3e1b32ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function for loading one of the 10 folds of the property dataset and concatinating the X and y values for train and test respectively.\n",
    "\n",
    "base_path = '../../data/361092' # Use this when running locally\n",
    "\n",
    "def load_fold(fold_number, random_seed=random_seed, auto_gluon=False):\n",
    "    df_X_train = pd.read_parquet(f'{base_path}/{fold_number}/X_train.parquet')\n",
    "    df_y_train = pd.read_parquet(f'{base_path}/{fold_number}/y_train.parquet')\n",
    "    df_X_test = pd.read_parquet(f'{base_path}/{fold_number}/X_test.parquet')\n",
    "    df_y_test = pd.read_parquet(f'{base_path}/{fold_number}/y_test.parquet')\n",
    "\n",
    "    # concatinating the X and y values for train (and test), but keeping for test\n",
    "    df_train = pd.concat([df_X_train, df_y_train], axis=1)\n",
    "\n",
    "\n",
    "    # Convert to AutoGluon's TabularDataset\n",
    "    if not auto_gluon:\n",
    "        return df_X_train, df_y_train, df_X_test, df_y_test\n",
    "    \n",
    "    else:\n",
    "        df_test = pd.concat([df_X_test, df_y_test], axis=1)\n",
    "        train_dataset = TabularDataset(df_train)\n",
    "        test_dataset = TabularDataset(df_test)\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "# Also instantiate the target column\n",
    "label = 'oz252'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac511618-d44c-4e2d-b79a-e36d17115047",
   "metadata": {},
   "source": [
    "# First we begin with AutoGluon run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a47e5f-337e-4f5b-84c0-3fcbbd86563b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make set out of half the folds\n",
    "\n",
    "full_train = None\n",
    "full_test = None\n",
    "\n",
    "for fold_number in range(1, 11):\n",
    "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed, auto_gluon=True)\n",
    "    if full_train is None:\n",
    "        full_train = train_dataset\n",
    "        full_test = test_dataset\n",
    "    else:\n",
    "        # Use pd.concat to combine TabularDatasets\n",
    "        full_train = pd.concat([full_train, train_dataset])\n",
    "        full_test = pd.concat([full_test, test_dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb588f47-c2ca-4d59-b086-4a1a1caed757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to fit the model using AutoGluon\n",
    "\n",
    "def fit_gluon(train_dataset, problem_type='regression', hyperparameters=None, eval_metric='r2', presets='medium_quality', time_limit=100, fit_weighted_ensemble=None, num_cpus = None, num_gpus=None, auto_stack=None, num_bag_folds=None, num_bag_sets=None, num_stack_levels=None, num_trials=None, verbosity=None, ag_args_fit=None, feature_prune=None, excluded_model_types=None, keep_only_best=None):\n",
    "    predictor = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric)\n",
    "\n",
    "    fit_args = {\n",
    "        'train_data': train_dataset,\n",
    "        'presets': presets,\n",
    "        'time_limit': time_limit,\n",
    "    }\n",
    "\n",
    "    if hyperparameters is not None:\n",
    "        fit_args['hyperparameters'] = hyperparameters\n",
    "    if auto_stack is not None:\n",
    "        fit_args['auto_stack'] = auto_stack\n",
    "    if num_bag_folds is not None:\n",
    "        fit_args['num_bag_folds'] = num_bag_folds\n",
    "    if num_bag_sets is not None:\n",
    "        fit_args['num_bag_sets'] = num_bag_sets\n",
    "    if num_stack_levels is not None:\n",
    "        fit_args['num_stack_levels'] = num_stack_levels\n",
    "    if num_trials is not None:\n",
    "        fit_args['num_trials'] = num_trials\n",
    "    if verbosity is not None:\n",
    "        fit_args['verbosity'] = verbosity\n",
    "    if ag_args_fit is not None:\n",
    "        fit_args['ag_args_fit'] = ag_args_fit\n",
    "    if feature_prune is not None:\n",
    "        fit_args['feature_prune'] = feature_prune\n",
    "    if excluded_model_types is not None:\n",
    "        fit_args['excluded_model_types'] = excluded_model_types\n",
    "    if fit_weighted_ensemble is not None:\n",
    "        fit_args['fit_weighted_ensemble'] = fit_weighted_ensemble\n",
    "    if num_cpus is not None:\n",
    "        fit_args['num_cpus'] = num_cpus\n",
    "    if num_gpus is not None:\n",
    "        fit_args['num_gpus'] = num_gpus\n",
    "    if keep_only_best is not None:\n",
    "        fit_args['keep_only_best'] = keep_only_best\n",
    "\n",
    "    predictor.fit(**fit_args)\n",
    "    return predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce73dd78-d12f-46b5-8e36-20fda6cc8962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240714_091252\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.9.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Tue Jun 18 14:00:06 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       13.63 GB / 15.43 GB (88.3%)\n",
      "Disk Space Avail:   6.66 GB / 24.99 GB (26.6%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240714_091252\"\n",
      "Train Data Rows:    79965\n",
      "Train Data Columns: 62\n",
      "Label Column:       oz252\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    13960.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 27.15 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 11): ['oz42', 'oz46', 'oz50', 'oz100', 'oz108', 'oz111', 'oz112', 'oz113', 'oz115', 'oz222', 'oz234']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 11 | ['oz42', 'oz46', 'oz50', 'oz100', 'oz108', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  9 | ['oz40', 'oz69', 'oz71', 'oz73', 'oz79', ...]\n",
      "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\t\t('int', ['bool']) :  9 | ['oz40', 'oz69', 'oz71', 'oz73', 'oz79', ...]\n",
      "\t1.9s = Fit runtime\n",
      "\t51 features in original data used to generate 51 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.31 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.98s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.03126367785906334, Train Rows: 77465, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 298.02s of the 298.02s of remaining time.\n",
      "\t0.9934\t = Validation score   (r2)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t5.79s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 291.75s of the 291.75s of remaining time.\n",
      "\t0.9934\t = Validation score   (r2)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t6.1s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 285.52s of the 285.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 0.000349576\tvalid_set's r2: 0.560283\n",
      "[2000]\tvalid_set's l2: 0.000223508\tvalid_set's r2: 0.718859\n",
      "[3000]\tvalid_set's l2: 0.00014857\tvalid_set's r2: 0.81312\n",
      "[4000]\tvalid_set's l2: 0.000104798\tvalid_set's r2: 0.868179\n",
      "[5000]\tvalid_set's l2: 7.54596e-05\tvalid_set's r2: 0.905082\n",
      "[6000]\tvalid_set's l2: 5.56457e-05\tvalid_set's r2: 0.930006\n",
      "[7000]\tvalid_set's l2: 4.17541e-05\tvalid_set's r2: 0.947479\n",
      "[8000]\tvalid_set's l2: 3.21521e-05\tvalid_set's r2: 0.959557\n",
      "[9000]\tvalid_set's l2: 2.51354e-05\tvalid_set's r2: 0.968383\n",
      "[10000]\tvalid_set's l2: 2.01413e-05\tvalid_set's r2: 0.974665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9747\t = Validation score   (r2)\n",
      "\t76.89s\t = Training   runtime\n",
      "\t1.21s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 206.58s of the 206.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 0.000148557\tvalid_set's r2: 0.813136\n",
      "[2000]\tvalid_set's l2: 4.78074e-05\tvalid_set's r2: 0.939865\n",
      "[3000]\tvalid_set's l2: 1.8812e-05\tvalid_set's r2: 0.976337\n",
      "[4000]\tvalid_set's l2: 9.20317e-06\tvalid_set's r2: 0.988424\n",
      "[5000]\tvalid_set's l2: 6.23347e-06\tvalid_set's r2: 0.992159\n",
      "[6000]\tvalid_set's l2: 5.20264e-06\tvalid_set's r2: 0.993456\n",
      "[7000]\tvalid_set's l2: 4.8503e-06\tvalid_set's r2: 0.993899\n",
      "[8000]\tvalid_set's l2: 4.71645e-06\tvalid_set's r2: 0.994067\n",
      "[9000]\tvalid_set's l2: 4.67143e-06\tvalid_set's r2: 0.994124\n",
      "[10000]\tvalid_set's l2: 4.65759e-06\tvalid_set's r2: 0.994141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9941\t = Validation score   (r2)\n",
      "\t70.93s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 134.01s of the 134.01s of remaining time.\n",
      "\tWarning: Exception caused RandomForestMSE to fail during training... Skipping this model.\n",
      "\t\t'squared_error'\n",
      "Detailed Traceback:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/_utils.py\", line 72, in __call__\n",
      "    return self.func(**kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 169, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 1252, in fit\n",
      "    super().fit(\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 351, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'squared_error'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 195, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 1754, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 1789, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 745, in get_result\n",
      "    return self._return_or_raise()\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 763, in _return_or_raise\n",
      "    raise self._result\n",
      "KeyError: 'squared_error'\n",
      "Fitting model: CatBoost ... Training model for up to 133.57s of the 133.57s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5517.\n",
      "\t0.9128\t = Validation score   (r2)\n",
      "\t133.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 298.02s of the -0.17s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t0.9941\t = Validation score   (r2)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 300.31s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3089.0 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240714_091252\")\n"
     ]
    }
   ],
   "source": [
    "autogluon_predictions = fit_gluon(full_train, time_limit=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5624d3-7602-44e0-b67f-2066e379c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r2': 0.9915471075229365, 'root_mean_squared_error': -0.002735305970449601, 'mean_squared_error': -7.481898751977232e-06, 'mean_absolute_error': -0.00033590307432918693, 'pearsonr': 0.9957650864257349, 'median_absolute_error': -3.381663131718504e-05}\n"
     ]
    }
   ],
   "source": [
    "pred_autogluon = autogluon_predictions.predict(full_test.drop(columns=[label_property]))\n",
    "eval = autogluon_predictions.evaluate(full_test)\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e52c18-4282-4fc8-b789-497dde1513c5",
   "metadata": {},
   "source": [
    "# Then we do the AutoSklearn run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "229c0827-d338-43f7-85bf-d528c2113886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make set out of all the folds\n",
    "\n",
    "full_train_X = None\n",
    "full_train_y = None\n",
    "full_test_X = None\n",
    "full_test_y = None\n",
    "\n",
    "for fold_number in range(1, 11):\n",
    "    train_dataset_X, train_dataset_y, test_dataset_X, test_dataset_y = load_fold(fold_number, random_seed=random_seed, auto_gluon=False, )\n",
    "    if full_train_X is None:\n",
    "        full_train_X = train_dataset_X\n",
    "        full_train_y = train_dataset_y\n",
    "        full_test_X = test_dataset_X\n",
    "        full_test_y = test_dataset_y\n",
    "    else:\n",
    "        # Use pd.concat to combine TabularDatasets\n",
    "        full_train_X = pd.concat([full_train_X, train_dataset_X])\n",
    "        full_train_y = pd.concat([full_train_y, train_dataset_y])\n",
    "        full_test_X = pd.concat([full_test_X, test_dataset_X])\n",
    "        full_test_y = pd.concat([full_test_y, test_dataset_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b957da2-23de-4bdd-a160-2048c977ee7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize AutoSklearnRegressor\n",
    "autosklearn_regressor = autosklearn.regression.AutoSklearnRegressor(\n",
    "    time_left_for_this_task=1200,#If running on big dataset, then it probably needs a lot of time\n",
    "    n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ad1f5b-463e-4835-84bf-d02c5174854d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoSklearnRegressor(ensemble_class=<class 'autosklearn.ensembles.ensemble_selection.EnsembleSelection'>,\n",
       "                     n_jobs=-1, per_run_time_limit=480,\n",
       "                     time_left_for_this_task=1200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on the full training data\n",
    "autosklearn_regressor.fit(full_train_X, full_train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e611a7a-fa9c-4b56-9d6f-580868956aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open('autosklearn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(autosklearn_regressor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf0b555c-a4c5-4b7f-bf5e-7a3fe6da8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open('autosklearn_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a6b784-180e-41f0-a281-3904f004992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoSklearn R2 score: 0.9883380014271478\n"
     ]
    }
   ],
   "source": [
    "autosklearn_predictions = autosklearn_regressor.predict(full_test_X)\n",
    "autosklearn_score = r2_score(full_test_y, autosklearn_predictions)\n",
    "print(\"AutoSklearn R2 score:\", autosklearn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a4682-c71c-4928-bf7a-aa77010539da",
   "metadata": {},
   "source": [
    "# Trying a third autoML-library: FLAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1441697b-800a-4eb3-a223-b93ddeb71817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flaml in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.1.2)\n",
      "Requirement already satisfied: NumPy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from flaml) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install flaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ea5048-e162-41b7-a21a-e7bd1229b378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "881b9e70-1066-439b-bc49-35d11f4cad6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to np.array\n",
    "\n",
    "full_train_X = np.array(full_train_X)\n",
    "full_train_y = np.array(full_train_y)\n",
    "full_test_X = np.array(full_test_X)\n",
    "full_test_y = np.array(full_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ef2ade7-9475-4d34-ab56-ca0eef0f9be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 07-14 09:38:09] {1680} INFO - task = regression\n",
      "[flaml.automl.logger: 07-14 09:38:09] {1691} INFO - Evaluation method: holdout\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'shuffle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_94/256887918.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mflaml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mflaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_budget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/flaml/automl/automl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, dataframe, label, metric, task, n_jobs, log_file_name, estimator_list, time_budget, max_iter, sample, ensemble, eval_method, log_type, model_history, split_ratio, n_splits, log_training_metric, mem_thres, pred_time_limit, train_time_limit, X_val, y_val, sample_weight_val, groups_val, groups, verbose, retrain_full, split_type, learner_selector, hpo_method, starting_points, seed, n_concurrent_trials, keep_search_state, preserve_checkpoint, early_stop, force_cancel, append_log, auto_augment, min_sample_size, use_ray, use_spark, free_mem_ratio, metric_constraints, custom_hp, time_col, cv_score_agg_func, skip_transform, mlflow_logging, fit_kwargs_by_estimator, **fit_kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_sample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sample_size_from_starting_points\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin_sample_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_sample_size_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_sample_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;31m# TODO pull this to task as decide_sample_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/flaml/automl/automl.py\u001b[0m in \u001b[0;36m_prepare_data\u001b[0;34m(self, eval_method, split_ratio, n_splits)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         self._state.task.prepare_data(\n\u001b[0m\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X_train_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/flaml/automl/task/generic_task.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, state, X_train_all, y_train_all, auto_augment, eval_method, split_type, split_ratio, n_splits, data_is_df, sample_weight_full)\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weight_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0mX_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_is_df\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mX_train_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shuffle' is not defined"
     ]
    }
   ],
   "source": [
    "flaml = AutoML()\n",
    "flaml.fit(full_train_X, full_train_y, task=\"regression\", time_budget=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f410fe-29e8-44c4-96cb-2d4873b7d57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open(\"flaml_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(flaml, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# At prediction time\n",
    "with open(\"flaml_model.pkl\", \"rb\") as f:\n",
    "    flaml = pickle.load(f)\n",
    "pred = flaml.predict(full_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab405dd7-3e29-43da-b81e-9b20ad194661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flaml_predictions = r2_score(full_test_y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c1655-a7fe-41a7-9cc0-85ec470edd11",
   "metadata": {},
   "source": [
    "# Comparison and ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03cc2d39-2ff1-4186-a33a-c87f2f1d403f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon R2 score: 0.9915471075229365\n",
      "AutoSklearn R2 score: 0.9883380014271478\n"
     ]
    }
   ],
   "source": [
    "print(\"AutoGluon R2 score:\", eval['r2'])  # Assuming 'eval' contains AutoGluon's evaluation results\n",
    "print(\"AutoSklearn R2 score:\", autosklearn_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc434c89-767a-4a2e-b29e-46dde9f2a446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble R2 score: 0.99075414632952\n"
     ]
    }
   ],
   "source": [
    "ensemble_predictions = (pred_autogluon + autosklearn_predictions) / 2\n",
    "ensemble_score = r2_score(full_test_y, ensemble_predictions)\n",
    "print(\"Ensemble R2 score:\", ensemble_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6adcf34-82eb-411b-b9b3-2215d1453a72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble 2 R2 score: 0.991198820472765\n"
     ]
    }
   ],
   "source": [
    "ensemble_predictions_2 = (2*pred_autogluon + autosklearn_predictions) / 3\n",
    "ensemble_score_2 = r2_score(full_test_y, ensemble_predictions_2)\n",
    "print(\"Ensemble 2 R2 score:\", ensemble_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d74c289-e044-49fc-ba8e-e73d0bcbb194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble 3 R2 score: 0.9915183700303908\n"
     ]
    }
   ],
   "source": [
    "ensemble_predictions_3 = (9*pred_autogluon + autosklearn_predictions) / 10\n",
    "ensemble_score_3 = r2_score(full_test_y, ensemble_predictions_3)\n",
    "print(\"Ensemble 3 R2 score:\", ensemble_score_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75540722-50a6-46a9-ad65-b94ea6b468c1",
   "metadata": {},
   "source": [
    "# Test the other datasets as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2909254f-c9e6-408a-86cb-38dde82f4abb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     city  area  rooms  bathroom  parking_spaces animal furniture  hoa_(BRL)  \\\n",
      "7259    4    45      2         1               0      1         1        162   \n",
      "4348    0   278      4         4               4      0         1       2400   \n",
      "653     3   280      3         2               2      0         0       3140   \n",
      "1595    4    90      3         2               1      0         1        934   \n",
      "1858    3    80      2         2               1      1         1       1050   \n",
      "\n",
      "      rent_amount_(BRL)  property_tax_(BRL)  fire_insurance_(BRL)  total_(BRL)  \n",
      "7259               1610                   8                    21     7.496652  \n",
      "4348              12000                 155                   160     9.597030  \n",
      "653                8000                1126                   104     9.423110  \n",
      "1595               1450                  65                    19     7.811568  \n",
      "1858               3500                 167                    46     8.468843  \n"
     ]
    }
   ],
   "source": [
    "# Brazilian houses gluon and autosklearn\n",
    "\n",
    "base_path = '../../data/361098' # [Brazilian Houses (361098)]\n",
    "\n",
    "full_train = None\n",
    "full_test = None\n",
    "\n",
    "for fold_number in range(1, 11):\n",
    "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed, auto_gluon=True)\n",
    "    if full_train is None:\n",
    "        full_train = train_dataset\n",
    "        full_test = test_dataset\n",
    "    else:\n",
    "        # Use pd.concat to combine TabularDatasets\n",
    "        full_train = pd.concat([full_train, train_dataset])\n",
    "        full_test = pd.concat([full_test, test_dataset])\n",
    "        \n",
    "# Make set out of all the folds\n",
    "\n",
    "full_train_X = None\n",
    "full_train_y = None\n",
    "full_test_X = None\n",
    "full_test_y = None\n",
    "\n",
    "for fold_number in range(1, 11):\n",
    "    train_dataset_X, train_dataset_y, test_dataset_X, test_dataset_y = load_fold(fold_number, random_seed=random_seed, auto_gluon=False, )\n",
    "    if full_train_X is None:\n",
    "        full_train_X = train_dataset_X\n",
    "        full_train_y = train_dataset_y\n",
    "        full_test_X = test_dataset_X\n",
    "        full_test_y = test_dataset_y\n",
    "    else:\n",
    "        # Use pd.concat to combine TabularDatasets\n",
    "        full_train_X = pd.concat([full_train_X, train_dataset_X])\n",
    "        full_train_y = pd.concat([full_train_y, train_dataset_y])\n",
    "        full_test_X = pd.concat([full_test_X, test_dataset_X])\n",
    "        full_test_y = pd.concat([full_test_y, test_dataset_y])\n",
    "        \n",
    "label = 'total_(BRL)'\n",
    "\n",
    "print(full_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb4cd86-9f10-48ef-b85c-155281d02dad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240714_120516\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.9.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Tue Jun 18 14:00:06 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       13.56 GB / 15.43 GB (87.9%)\n",
      "Disk Space Avail:   6.45 GB / 24.99 GB (25.8%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240714_120516\"\n",
      "Train Data Rows:    96228\n",
      "Train Data Columns: 11\n",
      "Label Column:       total_(BRL)\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    13884.00 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.22 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 3 | ['city', 'animal', 'furniture']\n",
      "\t\t('int', [])      : 8 | ['area', 'rooms', 'bathroom', 'parking_spaces', 'hoa_(BRL)', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 1 | ['city']\n",
      "\t\t('int', [])       : 8 | ['area', 'rooms', 'bathroom', 'parking_spaces', 'hoa_(BRL)', ...]\n",
      "\t\t('int', ['bool']) : 2 | ['animal', 'furniture']\n",
      "\t0.2s = Fit runtime\n",
      "\t11 features in original data used to generate 11 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.22 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.28s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.02597996425156919, Train Rows: 93728, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.72s of the 299.72s of remaining time.\n",
      "\t1.0\t = Validation score   (r2)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.34s of the 299.34s of remaining time.\n",
      "\t1.0\t = Validation score   (r2)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.06s of the 299.05s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 0.000462257\tvalid_set's r2: 0.999278\n",
      "[2000]\tvalid_set's l2: 0.000180341\tvalid_set's r2: 0.999718\n",
      "[3000]\tvalid_set's l2: 0.000106465\tvalid_set's r2: 0.999834\n",
      "[4000]\tvalid_set's l2: 7.59989e-05\tvalid_set's r2: 0.999881\n",
      "[5000]\tvalid_set's l2: 5.74621e-05\tvalid_set's r2: 0.99991\n",
      "[6000]\tvalid_set's l2: 4.5701e-05\tvalid_set's r2: 0.999929\n",
      "[7000]\tvalid_set's l2: 3.79761e-05\tvalid_set's r2: 0.999941\n",
      "[8000]\tvalid_set's l2: 3.18395e-05\tvalid_set's r2: 0.99995\n",
      "[9000]\tvalid_set's l2: 2.72457e-05\tvalid_set's r2: 0.999957\n",
      "[10000]\tvalid_set's l2: 2.34447e-05\tvalid_set's r2: 0.999963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.0\t = Validation score   (r2)\n",
      "\t50.13s\t = Training   runtime\n",
      "\t1.1s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 247.0s of the 247.0s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.82911e-05\tvalid_set's r2: 0.999956\n",
      "[2000]\tvalid_set's l2: 9.02244e-06\tvalid_set's r2: 0.999986\n",
      "[3000]\tvalid_set's l2: 3.76679e-06\tvalid_set's r2: 0.999994\n",
      "[4000]\tvalid_set's l2: 1.79252e-06\tvalid_set's r2: 0.999997\n",
      "[5000]\tvalid_set's l2: 8.96027e-07\tvalid_set's r2: 0.999999\n",
      "[6000]\tvalid_set's l2: 4.65609e-07\tvalid_set's r2: 0.999999\n",
      "[7000]\tvalid_set's l2: 2.53375e-07\tvalid_set's r2: 1\n",
      "[8000]\tvalid_set's l2: 1.3326e-07\tvalid_set's r2: 1\n",
      "[9000]\tvalid_set's l2: 7.60301e-08\tvalid_set's r2: 1\n",
      "[10000]\tvalid_set's l2: 4.33734e-08\tvalid_set's r2: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.0\t = Validation score   (r2)\n",
      "\t36.17s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 209.38s of the 209.38s of remaining time.\n",
      "\tWarning: Exception caused RandomForestMSE to fail during training... Skipping this model.\n",
      "\t\t'squared_error'\n",
      "Detailed Traceback:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/_utils.py\", line 72, in __call__\n",
      "    return self.func(**kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 169, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 1252, in fit\n",
      "    super().fit(\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 351, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'squared_error'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 195, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 1754, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 1789, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 745, in get_result\n",
      "    return self._return_or_raise()\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 763, in _return_or_raise\n",
      "    raise self._result\n",
      "KeyError: 'squared_error'\n",
      "Fitting model: CatBoost ... Training model for up to 209.06s of the 209.06s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6388.\n",
      "\t1.0\t = Validation score   (r2)\n",
      "\t209.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.72s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'KNeighborsUnif': 1.0}\n",
      "\t1.0\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 300.2s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 84861.6 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240714_120516\")\n"
     ]
    }
   ],
   "source": [
    "autogluon_prediction_brazil = fit_gluon(full_train, time_limit=300)\n",
    "pred_autogluon = autogluon_prediction_brazil.predict(full_test.drop(columns=[label]))\n",
    "eval = autogluon_prediction_brazil.evaluate(full_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d768b799-d51f-4497-8077-6f1243738c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize AutoSklearnRegressor\n",
    "autosklearn_regressor_brazil = autosklearn.regression.AutoSklearnRegressor(\n",
    "    time_left_for_this_task=1200,#If running on big dataset, then it probably needs a lot of time\n",
    "    n_jobs=-1,\n",
    "    tmp_folder='/tmp/autosklearn_regression_brazil_tmp',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e540a610-85f9-4590-9ed3-a4f104dd29a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autosklearn_regressor_brazil.fit(full_train_X, full_train_y)\n",
    "autosklearn_predictions = autosklearn_regressor_brazil.predict(full_test_X)\n",
    "autosklearn_score = r2_score(full_test_y, autosklearn_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34d36455-5f1f-46be-b14c-1b2992864573",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon R2 score: 0.9999999999999176\n",
      "AutoSklearn R2 score: 0.9999999987468355\n"
     ]
    }
   ],
   "source": [
    "print(\"AutoGluon R2 score:\", eval['r2'])  # Assuming 'eval' contains AutoGluon's evaluation results\n",
    "print(\"AutoSklearn R2 score:\", autosklearn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebeb4de4-83fb-489e-850d-ee793ea61626",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble brazil R2 score: 0.9999999998606933\n"
     ]
    }
   ],
   "source": [
    "ensemble_predictions_brazil = (2*pred_autogluon + autosklearn_predictions) / 3\n",
    "ensemble_score_brazil = r2_score(full_test_y, ensemble_predictions_brazil)\n",
    "print(\"Ensemble brazil R2 score:\", ensemble_score_brazil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb62c233-2c4a-40c8-810e-6138ba7011c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     season year  month  hour holiday workingday weather   temp  feel_temp  \\\n",
      "3937      2    0      6    23       0          1       0  25.42     28.790   \n",
      "460       1    0      1     5       0          1       0   9.84      9.850   \n",
      "2081      2    0      4    14       0          1       2  14.76     16.665   \n",
      "8823      1    1      1    11       0          0       0  14.76     17.425   \n",
      "6121      0    0      9    15       0          0       2  21.32     25.000   \n",
      "\n",
      "      humidity  windspeed  count  \n",
      "3937      0.83     8.9981    178  \n",
      "460       0.60    27.9993      5  \n",
      "2081      0.46    23.9994     87  \n",
      "8823      0.46    15.0013    256  \n",
      "6121      0.72    12.9980    354  \n",
      "156411\n"
     ]
    }
   ],
   "source": [
    "base_path = '../../data/361099' # Bike_Sharing_Demand (361099)\n",
    "full_train = None\n",
    "full_test = None\n",
    "\n",
    "for fold_number in range(1, 11):\n",
    "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed, auto_gluon=True)\n",
    "    if full_train is None:\n",
    "        full_train = train_dataset\n",
    "        full_test = test_dataset\n",
    "    else:\n",
    "        # Use pd.concat to combine TabularDatasets\n",
    "        full_train = pd.concat([full_train, train_dataset])\n",
    "        full_test = pd.concat([full_test, test_dataset])\n",
    "        \n",
    "# Make set out of all the folds\n",
    "\n",
    "full_train_X = None\n",
    "full_train_y = None\n",
    "full_test_X = None\n",
    "full_test_y = None\n",
    "\n",
    "for fold_number in range(1, 11):\n",
    "    train_dataset_X, train_dataset_y, test_dataset_X, test_dataset_y = load_fold(fold_number, random_seed=random_seed, auto_gluon=False, )\n",
    "    if full_train_X is None:\n",
    "        full_train_X = train_dataset_X\n",
    "        full_train_y = train_dataset_y\n",
    "        full_test_X = test_dataset_X\n",
    "        full_test_y = test_dataset_y\n",
    "    else:\n",
    "        # Use pd.concat to combine TabularDatasets\n",
    "        full_train_X = pd.concat([full_train_X, train_dataset_X])\n",
    "        full_train_y = pd.concat([full_train_y, train_dataset_y])\n",
    "        full_test_X = pd.concat([full_test_X, test_dataset_X])\n",
    "        full_test_y = pd.concat([full_test_y, test_dataset_y])\n",
    "        \n",
    "label = 'count'\n",
    "\n",
    "print(full_train.head())\n",
    "print(full_train_y['count'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cb05d4a-0cba-4eeb-8b35-af6e6bdf7f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240714_130126\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.9.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Tue Jun 18 14:00:06 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       11.86 GB / 15.43 GB (76.9%)\n",
      "Disk Space Avail:   6.36 GB / 24.99 GB (25.4%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240714_130126\"\n",
      "Train Data Rows:    156411\n",
      "Train Data Columns: 11\n",
      "Label Column:       count\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12150.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.82 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['season', 'year', 'holiday', 'workingday', 'weather']\n",
      "\t\t('float', [])    : 4 | ['temp', 'feel_temp', 'humidity', 'windspeed']\n",
      "\t\t('int', [])      : 2 | ['month', 'hour']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 2 | ['season', 'weather']\n",
      "\t\t('float', [])     : 4 | ['temp', 'feel_temp', 'humidity', 'windspeed']\n",
      "\t\t('int', [])       : 2 | ['month', 'hour']\n",
      "\t\t('int', ['bool']) : 3 | ['year', 'holiday', 'workingday']\n",
      "\t0.3s = Fit runtime\n",
      "\t11 features in original data used to generate 11 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.82 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.34s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.015983530570100567, Train Rows: 153911, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.66s of the 299.66s of remaining time.\n",
      "\t0.9914\t = Validation score   (r2)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.03s of the 299.03s of remaining time.\n",
      "\t0.9914\t = Validation score   (r2)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 298.39s of the 298.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 1258.62\tvalid_set's r2: 0.961215\n",
      "[2000]\tvalid_set's l2: 972.316\tvalid_set's r2: 0.970038\n",
      "[3000]\tvalid_set's l2: 840.709\tvalid_set's r2: 0.974093\n",
      "[4000]\tvalid_set's l2: 747.825\tvalid_set's r2: 0.976956\n",
      "[5000]\tvalid_set's l2: 674.932\tvalid_set's r2: 0.979202\n",
      "[6000]\tvalid_set's l2: 618.722\tvalid_set's r2: 0.980934\n",
      "[7000]\tvalid_set's l2: 569.293\tvalid_set's r2: 0.982457\n",
      "[8000]\tvalid_set's l2: 523.268\tvalid_set's r2: 0.983875\n",
      "[9000]\tvalid_set's l2: 486.17\tvalid_set's r2: 0.985019\n",
      "[10000]\tvalid_set's l2: 453.801\tvalid_set's r2: 0.986016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.986\t = Validation score   (r2)\n",
      "\t68.88s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 227.59s of the 227.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 712.072\tvalid_set's r2: 0.978057\n",
      "[2000]\tvalid_set's l2: 419.783\tvalid_set's r2: 0.987064\n",
      "[3000]\tvalid_set's l2: 279.87\tvalid_set's r2: 0.991376\n",
      "[4000]\tvalid_set's l2: 195.17\tvalid_set's r2: 0.993986\n",
      "[5000]\tvalid_set's l2: 139.671\tvalid_set's r2: 0.995696\n",
      "[6000]\tvalid_set's l2: 105.199\tvalid_set's r2: 0.996758\n",
      "[7000]\tvalid_set's l2: 79.9901\tvalid_set's r2: 0.997535\n",
      "[8000]\tvalid_set's l2: 61.8771\tvalid_set's r2: 0.998093\n",
      "[9000]\tvalid_set's l2: 48.3998\tvalid_set's r2: 0.998509\n",
      "[10000]\tvalid_set's l2: 38.9978\tvalid_set's r2: 0.998798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9988\t = Validation score   (r2)\n",
      "\t55.27s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 170.91s of the 170.91s of remaining time.\n",
      "\tWarning: Exception caused RandomForestMSE to fail during training... Skipping this model.\n",
      "\t\t'squared_error'\n",
      "Detailed Traceback:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/_utils.py\", line 72, in __call__\n",
      "    return self.func(**kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 169, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 1252, in fit\n",
      "    super().fit(\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 351, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'squared_error'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/autogluon/tabular/models/rf/rf_model.py\", line 195, in _fit\n",
      "    model = model.fit(X, y, sample_weight=sample_weight)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 387, in fit\n",
      "    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 1754, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 1789, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 745, in get_result\n",
      "    return self._return_or_raise()\n",
      "  File \"/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/joblib/parallel.py\", line 763, in _return_or_raise\n",
      "    raise self._result\n",
      "KeyError: 'squared_error'\n",
      "Fitting model: CatBoost ... Training model for up to 170.43s of the 170.43s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 3299.\n",
      "\t0.9729\t = Validation score   (r2)\n",
      "\t170.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.66s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.9, 'KNeighborsUnif': 0.1}\n",
      "\t0.9989\t = Validation score   (r2)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 300.26s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3567.2 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240714_130126\")\n"
     ]
    }
   ],
   "source": [
    "autogluon_regressor_bike = fit_gluon(full_train, time_limit=300)\n",
    "pred_autogluon_bike = autogluon_regressor_bike.predict(full_test.drop(columns=[label]))\n",
    "eval = autogluon_regressor_bike.evaluate(full_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6622c2fc-026f-4bfd-b5d8-ea4150e472bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autosklearn_regressor_bike' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1427/2430691715.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mautosklearn_regressor_bike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mautosklearn_predictions_bike\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautosklearn_regressor_bike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_test_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mautosklearn_score_bike\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautosklearn_predictions_bike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autosklearn_regressor_bike' is not defined"
     ]
    }
   ],
   "source": [
    "autosklearn_regressor_bike.fit(full_train_X, full_train_y)\n",
    "autosklearn_predictions_bike = autosklearn_regressor_bike.predict(full_test_X)\n",
    "autosklearn_score_bike = r2_score(full_test_y, autosklearn_predictions_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655ea3b-ff5d-406e-9ff2-f2e981358bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"AutoGluon R2 score:\", eval['r2'])  # Assuming 'eval' contains AutoGluon's evaluation results\n",
    "print(\"AutoSklearn R2 score:\", autosklearn_score_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58802b57-0755-44e5-9a7c-cbd64df088a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble_predictions_bike = (2*pred_autogluon_bike + autosklearn_predictions_bike) / 3\n",
    "ensemble_score_bike = r2_score(full_test_y, ensemble_predictions_bike)\n",
    "print(\"Ensemble bike R2 score:\", ensemble_score_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5df32-9acd-4b59-bc48-0573e8b34354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

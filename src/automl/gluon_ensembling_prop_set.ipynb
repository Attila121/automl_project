{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a script to to load the property dataset and run **AutoGluon** on it. First basic model run, then different configurations are tried. Then look at possible **third level ensembling** of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Isak\\miniconda3\\envs\\automl-tabular-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for loading one of the 10 folds of the property dataset and concatinating the X and y values for train and test respectively. \n",
    "import pandas as pd\n",
    "\n",
    "def load_fold(fold_number, random_seed=42, sample_size=None):\n",
    "    df_X_train = pd.read_parquet(f'../../data/361092/{fold_number}/X_train.parquet')\n",
    "    df_y_train = pd.read_parquet(f'../../data/361092/{fold_number}/y_train.parquet')\n",
    "    df_X_test = pd.read_parquet(f'../../data/361092/{fold_number}/X_test.parquet')\n",
    "    df_y_test = pd.read_parquet(f'../../data/361092/{fold_number}/y_test.parquet')\n",
    "\n",
    "    # concatinating the X and y values for train and test respectively\n",
    "    df_train = pd.concat([df_X_train, df_y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test, df_y_test], axis=1)\n",
    "\n",
    "    # Convert to AutoGluon's TabularDataset\n",
    "    if sample_size:\n",
    "        train_dataset = TabularDataset(df_train).sample(n=sample_size, random_state=random_seed)\n",
    "        test_dataset = TabularDataset(df_test).sample(n=sample_size, random_state=random_seed)\n",
    "    else:\n",
    "        train_dataset = TabularDataset(df_train)\n",
    "        test_dataset = TabularDataset(df_test)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Also instantiate the target column\n",
    "label_property = 'oz252'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to fit the model, with most of the hyperparameters present and set to default/None. (Add more hyperparameters if desirable)\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "def fit_gluon(train_dataset, problem_type='regression', hyperparameters=None, eval_metric='r2', presets='medium_quality', time_limit=100, auto_stack=None, num_bag_folds=None, num_bag_sets=None, num_stack_levels=None, num_trials=None, verbosity=None, ag_args_fit=None, feature_prune=None, excluded_model_types=None):\n",
    "    predictor = TabularPredictor(label=label_property, problem_type=problem_type, eval_metric=eval_metric)\n",
    "    \n",
    "    fit_args = {\n",
    "        'train_data': train_dataset,\n",
    "        'presets': presets,\n",
    "        'time_limit': time_limit,\n",
    "    }\n",
    "\n",
    "    if hyperparameters is not None:\n",
    "        fit_args['hyperparameters'] = hyperparameters\n",
    "    if auto_stack is not None:\n",
    "        fit_args['auto_stack'] = auto_stack\n",
    "    if num_bag_folds is not None:\n",
    "        fit_args['num_bag_folds'] = num_bag_folds\n",
    "    if num_bag_sets is not None:\n",
    "        fit_args['num_bag_sets'] = num_bag_sets\n",
    "    if num_stack_levels is not None:\n",
    "        fit_args['num_stack_levels'] = num_stack_levels\n",
    "    if num_trials is not None:\n",
    "        fit_args['num_trials'] = num_trials\n",
    "    if verbosity is not None:\n",
    "        fit_args['verbosity'] = verbosity\n",
    "    if ag_args_fit is not None:\n",
    "        fit_args['ag_args_fit'] = ag_args_fit\n",
    "    if feature_prune is not None:\n",
    "        fit_args['feature_prune'] = feature_prune\n",
    "    if excluded_model_types is not None:\n",
    "        fit_args['excluded_model_types'] = excluded_model_types\n",
    "\n",
    "    predictor.fit(**fit_args)\n",
    "    return predictor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to evaluate a fitted model and training set. \n",
    "\n",
    "def evaluate_gluon(model, test_dataset):\n",
    "    test_score = model.evaluate(test_dataset)\n",
    "    leaderboard = model.leaderboard(test_dataset)\n",
    "    return test_score, leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary containing the hyperparameters for the different AutoGluon models\n",
    "# First we make one dict, but later on trying different variaties for the lvl 1 and 2 models\n",
    "\n",
    "# Think about doing this instead:\n",
    "# from autogluon.common import space...\n",
    "\n",
    "hyperparameters = {\n",
    "    'GBM': [\n",
    "        {'learning_rate': 0.1, 'num_leaves': 31, 'feature_fraction': 0.9},\n",
    "        {'learning_rate': 0.05, 'num_leaves': 45, 'feature_fraction': 0.8},\n",
    "    ],\n",
    "    'CAT': {\n",
    "        'iterations': 1000,\n",
    "        'depth': 7,\n",
    "        'learning_rate': 0.1,\n",
    "        'l2_leaf_reg': 3,\n",
    "    },\n",
    "    'XGB': {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "    },\n",
    "    'NN_TORCH': {\n",
    "        'num_epochs': 10,\n",
    "        'learning_rate': 1e-3,\n",
    "        'layers': [100, 100],\n",
    "    },\n",
    "    'RF': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 15,\n",
    "        'min_samples_split': 2,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240630_131314\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          8\n",
      "Memory Avail:       4.24 GB / 15.80 GB (26.8%)\n",
      "Disk Space Avail:   180.40 GB / 952.46 GB (18.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240630_131314\"\n",
      "Train Data Rows:    7996\n",
      "Train Data Columns: 62\n",
      "Label Column:       oz252\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4340.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 6): ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', 'oz115']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 6 | ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
      "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\t\t('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t56 features in original data used to generate 56 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.17s of the 299.16s of remaining time.\n",
      "\t-0.1747\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.07s of the 299.07s of remaining time.\n",
      "\t-0.1786\t = Validation score   (r2)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.01s of the 299.01s of remaining time.\n",
      "\t0.0871\t = Validation score   (r2)\n",
      "\t1.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.13s of the 297.13s of remaining time.\n",
      "\t0.078\t = Validation score   (r2)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.89s of the 295.89s of remaining time.\n",
      "\t0.0786\t = Validation score   (r2)\n",
      "\t10.63s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 284.82s of the 284.81s of remaining time.\n",
      "\t0.0897\t = Validation score   (r2)\n",
      "\t4.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 280.0s of the 280.0s of remaining time.\n",
      "\t0.0864\t = Validation score   (r2)\n",
      "\t4.68s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 274.96s of the 274.96s of remaining time.\n",
      "\t0.0655\t = Validation score   (r2)\n",
      "\t12.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 262.8s of the 262.8s of remaining time.\n",
      "\t0.0654\t = Validation score   (r2)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 261.01s of the 261.0s of remaining time.\n",
      "\t-0.295\t = Validation score   (r2)\n",
      "\t23.11s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 237.83s of the 237.82s of remaining time.\n",
      "\t0.0878\t = Validation score   (r2)\n",
      "\t3.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.17s of the 233.99s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.333, 'ExtraTreesMSE': 0.238, 'LightGBMLarge': 0.19, 'RandomForestMSE': 0.143, 'LightGBMXT': 0.048, 'CatBoost': 0.048}\n",
      "\t0.1199\t = Validation score   (r2)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 66.4s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3509.7 rows/s (800 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240630_131314\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240630_131424\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          8\n",
      "Memory Avail:       3.85 GB / 15.80 GB (24.3%)\n",
      "Disk Space Avail:   180.03 GB / 952.46 GB (18.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240630_131424\"\n",
      "Train Data Rows:    7996\n",
      "Train Data Columns: 62\n",
      "Label Column:       oz252\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: {'r2': 0.09103472251615252, 'root_mean_squared_error': -0.025396391264573628, 'mean_squared_error': -0.0006449766892633117, 'mean_absolute_error': -0.018279427089901424, 'pearsonr': 0.3077854135530402, 'median_absolute_error': -0.013118152965545637}\n",
      "                  model  score_test  score_val eval_metric  pred_time_test  \\\n",
      "0   WeightedEnsemble_L2    0.091035   0.119931          r2        1.377175   \n",
      "1              CatBoost    0.089001   0.089654          r2        0.075433   \n",
      "2         LightGBMLarge    0.079066   0.087833          r2        0.013531   \n",
      "3               XGBoost    0.077817   0.065368          r2        0.053601   \n",
      "4            LightGBMXT    0.077281   0.087110          r2        0.021812   \n",
      "5              LightGBM    0.072527   0.078010          r2        0.040905   \n",
      "6         ExtraTreesMSE    0.062175   0.086427          r2        0.585132   \n",
      "7       RandomForestMSE    0.048230   0.078550          r2        0.564182   \n",
      "8       NeuralNetFastAI    0.009104   0.065459          r2        0.106010   \n",
      "9        KNeighborsUnif   -0.118886  -0.174653          r2        0.041758   \n",
      "10       KNeighborsDist   -0.192583  -0.178570          r2        0.041142   \n",
      "11       NeuralNetTorch   -0.309894  -0.295047          r2        0.080002   \n",
      "\n",
      "    pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
      "0        0.227939  37.996304                 0.011075                0.001509   \n",
      "1        0.010588   4.792387                 0.075433                0.010588   \n",
      "2        0.004001   3.748688                 0.013531                0.004001   \n",
      "3        0.006510   1.758596                 0.053601                0.006510   \n",
      "4        0.007474   1.838683                 0.021812                0.007474   \n",
      "5        0.003001   1.213456                 0.040905                0.003001   \n",
      "6        0.084758   4.681923                 0.585132                0.084758   \n",
      "7        0.096120  10.626553                 0.564182                0.096120   \n",
      "8        0.023490  12.093991                 0.106010                0.023490   \n",
      "9        0.036730   0.043185                 0.041758                0.036730   \n",
      "10       0.025016   0.024312                 0.041142                0.025016   \n",
      "11       0.055804  23.107981                 0.080002                0.055804   \n",
      "\n",
      "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
      "0            0.214079            2       True         12  \n",
      "1            4.792387            1       True          6  \n",
      "2            3.748688            1       True         11  \n",
      "3            1.758596            1       True          9  \n",
      "4            1.838683            1       True          3  \n",
      "5            1.213456            1       True          4  \n",
      "6            4.681923            1       True          7  \n",
      "7           10.626553            1       True          5  \n",
      "8           12.093991            1       True          8  \n",
      "9            0.043185            1       True          1  \n",
      "10           0.024312            1       True          2  \n",
      "11          23.107981            1       True         10  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3949.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['oz115']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 6): ['oz46', 'oz50', 'oz69', 'oz100', 'oz107', 'oz111']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 6 | ['oz46', 'oz50', 'oz69', 'oz100', 'oz107', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 13 | ['oz40', 'oz42', 'oz71', 'oz73', 'oz79', ...]\n",
      "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\t\t('int', ['bool']) : 13 | ['oz40', 'oz42', 'oz71', 'oz73', 'oz79', ...]\n",
      "\t1.4s = Fit runtime\n",
      "\t55 features in original data used to generate 55 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.66 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 298.54s of the 298.53s of remaining time.\n",
      "\t-0.1693\t = Validation score   (r2)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 298.44s of the 298.44s of remaining time.\n",
      "\t-0.1557\t = Validation score   (r2)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 298.37s of the 298.37s of remaining time.\n",
      "\t0.0672\t = Validation score   (r2)\n",
      "\t2.89s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 295.42s of the 295.42s of remaining time.\n",
      "\t0.0592\t = Validation score   (r2)\n",
      "\t1.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 293.57s of the 293.57s of remaining time.\n",
      "\t0.0651\t = Validation score   (r2)\n",
      "\t17.76s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 274.75s of the 274.74s of remaining time.\n",
      "\t0.0615\t = Validation score   (r2)\n",
      "\t6.56s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 268.13s of the 268.13s of remaining time.\n",
      "\t0.0745\t = Validation score   (r2)\n",
      "\t18.98s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 247.91s of the 247.91s of remaining time.\n",
      "\t0.0369\t = Validation score   (r2)\n",
      "\t61.06s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 186.62s of the 186.62s of remaining time.\n",
      "\t0.056\t = Validation score   (r2)\n",
      "\t8.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 177.81s of the 177.8s of remaining time.\n",
      "\t-0.191\t = Validation score   (r2)\n",
      "\t71.59s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 105.99s of the 105.99s of remaining time.\n",
      "\t0.0916\t = Validation score   (r2)\n",
      "\t13.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 298.54s of the 91.98s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge': 0.5, 'ExtraTreesMSE': 0.222, 'NeuralNetFastAI': 0.222, 'NeuralNetTorch': 0.056}\n",
      "\t0.1045\t = Validation score   (r2)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 208.77s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1247.8 rows/s (800 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240630_131424\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240630_131800\"\n",
      "Verbosity: 2 (Standard Logging)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: {'r2': 0.11939901741047843, 'root_mean_squared_error': -0.026706333421830396, 'mean_squared_error': -0.0007132282448379752, 'mean_absolute_error': -0.019365957073507745, 'pearsonr': 0.3510551809526267, 'median_absolute_error': -0.014009070114135769}\n",
      "                  model  score_test  score_val eval_metric  pred_time_test  \\\n",
      "0   WeightedEnsemble_L2    0.119399   0.104477          r2        1.843945   \n",
      "1         LightGBMLarge    0.104824   0.091638          r2        0.059174   \n",
      "2         ExtraTreesMSE    0.101813   0.074513          r2        1.306422   \n",
      "3       RandomForestMSE    0.099283   0.065050          r2        1.221158   \n",
      "4              LightGBM    0.088354   0.059196          r2        0.064543   \n",
      "5              CatBoost    0.078540   0.061511          r2        0.211820   \n",
      "6               XGBoost    0.074381   0.056009          r2        0.133981   \n",
      "7            LightGBMXT    0.073264   0.067152          r2        0.116357   \n",
      "8       NeuralNetFastAI    0.064369   0.036929          r2        0.236800   \n",
      "9        KNeighborsUnif    0.001391  -0.169261          r2        0.125129   \n",
      "10       KNeighborsDist   -0.002061  -0.155670          r2        0.126378   \n",
      "11       NeuralNetTorch   -0.161303  -0.190995          r2        0.219023   \n",
      "\n",
      "    pred_time_val    fit_time  pred_time_test_marginal  \\\n",
      "0        0.641146  165.914634                 0.022526   \n",
      "1        0.017174   13.710172                 0.059174   \n",
      "2        0.329145   18.983213                 1.306422   \n",
      "3        0.275600   17.755146                 1.221158   \n",
      "4        0.004013    1.816895                 0.064543   \n",
      "5        0.026623    6.559894                 0.211820   \n",
      "6        0.022693    8.727892                 0.133981   \n",
      "7        0.015362    2.894329                 0.116357   \n",
      "8        0.107637   61.055687                 0.236800   \n",
      "9        0.028087    0.036847                 0.125129   \n",
      "10       0.024237    0.027021                 0.126378   \n",
      "11       0.183378   71.586543                 0.219023   \n",
      "\n",
      "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                 0.003812           0.579018            2       True   \n",
      "1                 0.017174          13.710172            1       True   \n",
      "2                 0.329145          18.983213            1       True   \n",
      "3                 0.275600          17.755146            1       True   \n",
      "4                 0.004013           1.816895            1       True   \n",
      "5                 0.026623           6.559894            1       True   \n",
      "6                 0.022693           8.727892            1       True   \n",
      "7                 0.015362           2.894329            1       True   \n",
      "8                 0.107637          61.055687            1       True   \n",
      "9                 0.028087           0.036847            1       True   \n",
      "10                0.024237           0.027021            1       True   \n",
      "11                0.183378          71.586543            1       True   \n",
      "\n",
      "    fit_order  \n",
      "0          12  \n",
      "1          11  \n",
      "2           7  \n",
      "3           5  \n",
      "4           4  \n",
      "5           6  \n",
      "6           9  \n",
      "7           3  \n",
      "8           8  \n",
      "9           1  \n",
      "10          2  \n",
      "11         10  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          8\n",
      "Memory Avail:       3.95 GB / 15.80 GB (25.0%)\n",
      "Disk Space Avail:   179.66 GB / 952.46 GB (18.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240630_131800\"\n",
      "Train Data Rows:    7996\n",
      "Train Data Columns: 62\n",
      "Label Column:       oz252\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4021.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['oz222']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['oz50', 'oz100', 'oz107', 'oz111', 'oz115']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 5 | ['oz50', 'oz100', 'oz107', 'oz111', 'oz115']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 14 | ['oz40', 'oz42', 'oz46', 'oz69', 'oz71', ...]\n",
      "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\t\t('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz69', 'oz71', ...]\n",
      "\t3.6s = Fit runtime\n",
      "\t56 features in original data used to generate 56 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 296.29s of the 296.28s of remaining time.\n",
      "\t-0.0855\t = Validation score   (r2)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 296.12s of the 296.1s of remaining time.\n",
      "\t-0.0807\t = Validation score   (r2)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 295.93s of the 295.91s of remaining time.\n",
      "\t0.0881\t = Validation score   (r2)\n",
      "\t9.04s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 286.78s of the 286.77s of remaining time.\n",
      "\t0.079\t = Validation score   (r2)\n",
      "\t6.11s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 280.58s of the 280.57s of remaining time.\n",
      "\t0.0681\t = Validation score   (r2)\n",
      "\t28.03s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 251.51s of the 251.5s of remaining time.\n",
      "\t0.0902\t = Validation score   (r2)\n",
      "\t10.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 240.68s of the 240.67s of remaining time.\n",
      "\t0.0976\t = Validation score   (r2)\n",
      "\t6.35s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 233.56s of the 233.55s of remaining time.\n",
      "\t0.0276\t = Validation score   (r2)\n",
      "\t17.67s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 215.81s of the 215.8s of remaining time.\n",
      "\t0.0737\t = Validation score   (r2)\n",
      "\t2.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 213.14s of the 213.13s of remaining time.\n",
      "\t-0.2339\t = Validation score   (r2)\n",
      "\t22.8s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 190.27s of the 190.26s of remaining time.\n",
      "\t0.0943\t = Validation score   (r2)\n",
      "\t3.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 296.29s of the 186.77s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE': 0.35, 'LightGBMLarge': 0.3, 'CatBoost': 0.2, 'NeuralNetFastAI': 0.15}\n",
      "\t0.1131\t = Validation score   (r2)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 113.52s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3798.5 rows/s (800 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240630_131800\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: {'r2': 0.11276829470738381, 'root_mean_squared_error': -0.026501213816332244, 'mean_squared_error': -0.0007023143337389591, 'mean_absolute_error': -0.01921755821923169, 'pearsonr': 0.3369754256798945, 'median_absolute_error': -0.013829783905029314}\n",
      "                  model  score_test  score_val eval_metric  pred_time_test  \\\n",
      "0   WeightedEnsemble_L2    0.112768   0.113052          r2        0.501587   \n",
      "1         ExtraTreesMSE    0.103558   0.097630          r2        0.355873   \n",
      "2         LightGBMLarge    0.094741   0.094309          r2        0.018516   \n",
      "3               XGBoost    0.084844   0.073653          r2        0.042531   \n",
      "4              LightGBM    0.081558   0.079029          r2        0.026268   \n",
      "5       RandomForestMSE    0.081055   0.068119          r2        0.436017   \n",
      "6            LightGBMXT    0.080822   0.088135          r2        0.028094   \n",
      "7              CatBoost    0.072188   0.090169          r2        0.053167   \n",
      "8       NeuralNetFastAI    0.055000   0.027564          r2        0.071032   \n",
      "9        KNeighborsDist   -0.090813  -0.080713          r2        0.040424   \n",
      "10       KNeighborsUnif   -0.107683  -0.085502          r2        0.041395   \n",
      "11       NeuralNetTorch   -0.167269  -0.233932          r2        0.081164   \n",
      "\n",
      "    pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
      "0        0.210608  38.354690                 0.002999                0.003217   \n",
      "1        0.167826   6.352293                 0.355873                0.167826   \n",
      "2        0.003997   3.390869                 0.018516                0.003997   \n",
      "3        0.009001   2.620930                 0.042531                0.009001   \n",
      "4        0.017483   6.110868                 0.026268                0.017483   \n",
      "5        0.119478  28.032200                 0.436017                0.119478   \n",
      "6        0.022586   9.035071                 0.028094                0.022586   \n",
      "7        0.005999  10.788523                 0.053167                0.005999   \n",
      "8        0.029570  17.673716                 0.071032                0.029570   \n",
      "9        0.060495   0.075642                 0.040424                0.060495   \n",
      "10       0.062249   0.075696                 0.041395                0.062249   \n",
      "11       0.054348  22.803159                 0.081164                0.054348   \n",
      "\n",
      "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
      "0            0.149289            2       True         12  \n",
      "1            6.352293            1       True          7  \n",
      "2            3.390869            1       True         11  \n",
      "3            2.620930            1       True          9  \n",
      "4            6.110868            1       True          4  \n",
      "5           28.032200            1       True          5  \n",
      "6            9.035071            1       True          3  \n",
      "7           10.788523            1       True          6  \n",
      "8           17.673716            1       True          8  \n",
      "9            0.075642            1       True          2  \n",
      "10           0.075696            1       True          1  \n",
      "11          22.803159            1       True         10  \n"
     ]
    }
   ],
   "source": [
    "## First training the model speratly on all 10 folds to see that it is consistent\n",
    "\n",
    "for fold_number in range(1, 4): # jsut the three first to start with\n",
    "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed)\n",
    "    model = fit_gluon(train_dataset, time_limit=300)\n",
    "    test_score, leaderboard = evaluate_gluon(model, test_dataset)\n",
    "    print(f'Fold {fold_number}: {test_score}')\n",
    "    print(leaderboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240630_140716\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          8\n",
      "Memory Avail:       4.86 GB / 15.80 GB (30.8%)\n",
      "Disk Space Avail:   178.03 GB / 952.46 GB (18.7%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240630_140716\"\n",
      "Train Data Rows:    7996\n",
      "Train Data Columns: 62\n",
      "Label Column:       oz252\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4965.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 6): ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', 'oz115']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 6 | ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
      "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
      "\t\t('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
      "\t0.9s = Fit runtime\n",
      "\t56 features in original data used to generate 56 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.95s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 899.05s of the 899.05s of remaining time.\n",
      "\t-0.1747\t = Validation score   (r2)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 898.99s of the 898.99s of remaining time.\n",
      "\t-0.1786\t = Validation score   (r2)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 898.94s of the 898.93s of remaining time.\n",
      "\t0.0871\t = Validation score   (r2)\n",
      "\t2.99s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 895.87s of the 895.86s of remaining time.\n",
      "\t0.078\t = Validation score   (r2)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 894.55s of the 894.54s of remaining time.\n",
      "\t0.0786\t = Validation score   (r2)\n",
      "\t12.7s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 881.43s of the 881.43s of remaining time.\n",
      "\t0.0897\t = Validation score   (r2)\n",
      "\t5.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 875.99s of the 875.98s of remaining time.\n",
      "\t0.0864\t = Validation score   (r2)\n",
      "\t4.69s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 870.95s of the 870.95s of remaining time.\n"
     ]
    }
   ],
   "source": [
    "## First training the model speratly on all 10 folds to see that it is consistent\n",
    "\n",
    "for fold_number in range(1, 4): # jsut the three first to start with\n",
    "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed)\n",
    "    model = fit_gluon(train_dataset, time_limit=900)\n",
    "    test_score, leaderboard = evaluate_gluon(model, test_dataset)\n",
    "    print(f'Fold {fold_number}: {test_score}')\n",
    "    print(leaderboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to plot the results a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the model on all 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating an ensemble of the 10 models??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl-tabular-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht4hg37xc-51"
      },
      "source": [
        "### This is a script to to load the property dataset and run **AutoGluon** on it. First basic model run, then different configurations are tried. Then look at possible **third level ensembling** of the models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### installments\n",
        "\n",
        "!pip install autogluon\n",
        "!sudo apt-get install graphviz graphviz-dev\n",
        "!pip install pygraphviz\n",
        "!pip install tabpfn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "vWxQZEEH2uHi",
        "outputId": "97867842-4242-4171-bbba-ab92979f522d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon\n",
            "  Downloading autogluon-1.1.1-py3-none-any.whl (9.7 kB)\n",
            "Collecting autogluon.core[all]==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.features==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.tabular[all]==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.multimodal==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.multimodal-1.1.1-py3-none-any.whl (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.0/428.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.timeseries[all]==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.timeseries-1.1.1-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.29,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (1.25.2)\n",
            "Requirement already satisfied: scipy<1.13,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (1.11.4)\n",
            "Collecting scikit-learn<1.4.1,>=1.3.0 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (3.3)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (3.7.1)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading boto3-1.34.137-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.common==1.1.1 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ray[default,tune]<2.11,>=2.10.0 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (0.2.7)\n",
            "Collecting Pillow<11,>=10.0.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.3.0+cu121)\n",
            "Collecting lightning<2.4,>=2.2 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading lightning-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers[sentencepiece]<4.41.0,>=4.38.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate<0.22.0,>=0.21.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema<4.22,>=4.18 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (4.19.2)\n",
            "Collecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm<0.10.0,>=0.9.5 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision<0.19.0,>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.18.0+cu121)\n",
            "Requirement already satisfied: scikit-image<0.21.0,>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.19.3)\n",
            "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.3)\n",
            "Collecting torchmetrics<1.3.0,>=1.2.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nptyping<2.5.0,>=1.4.4 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
            "Collecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-metric-learning<2.4,>=1.3.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk<4.0.0,>=3.4.5 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.8.1)\n",
            "Collecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.1.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.15.2)\n",
            "Collecting pytesseract<0.3.11,>=0.3.9 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2image<1.19,>=1.17.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: xgboost<2.1,>=1.6 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.0.3)\n",
            "Requirement already satisfied: fastai<2.8,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.7.15)\n",
            "Requirement already satisfied: lightgbm<4.4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (4.1.0)\n",
            "Collecting catboost<1.3,>=1.1 (from autogluon.tabular[all]==1.1.1->autogluon)\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib<2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries[all]==1.1.1->autogluon) (1.4.2)\n",
            "Collecting pytorch-lightning<2.4,>=2.2 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading pytorch_lightning-2.3.1-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonts==0.15.1 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading gluonts-0.15.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting statsforecast<1.5,>=1.4.0 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading statsforecast-1.4.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlforecast<0.10.1,>=0.10.0 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading mlforecast-0.10.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting utilsforecast<0.0.11,>=0.0.10 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading utilsforecast-0.0.10-py3-none-any.whl (30 kB)\n",
            "Collecting orjson~=3.9 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optimum[onnxruntime]<1.19,>=1.17 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading optimum-1.18.1-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.0/410.0 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core[all]==1.1.1->autogluon) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core[all]==1.1.1->autogluon) (67.7.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.7.4)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (6.0.1)\n",
            "Collecting botocore<1.35.0,>=1.34.137 (from boto3<2,>=1.10->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading botocore-1.34.137-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2,>=1.10->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (1.16.0)\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.23.4)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (23.1.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.48)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.7.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.10.9.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.1.1->autogluon) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.18.1)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries[all]==1.1.1->autogluon) (0.58.1)\n",
            "Collecting window-ops (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (5.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (2024.5.15)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Collecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (13.7.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.9.0)\n",
            "Collecting coloredlogs (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon) (1.12.1)\n",
            "Collecting transformers[sentencepiece]<4.41.0,>=4.38.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.11.0 (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core[all]==1.1.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core[all]==1.1.1->autogluon) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core[all]==1.1.1->autogluon) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (3.15.4)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.0.8)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.4.1)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (3.9.5)\n",
            "Collecting aiohttp-cors (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-spy>=0.2.0 (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencensus (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (0.20.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (7.0.4)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading virtualenv-20.26.3-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.64.1)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (14.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (2024.6.2)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2024.6.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (1.6.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.core[all]==1.1.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from statsforecast<1.5,>=1.4.0->autogluon.timeseries[all]==1.1.1->autogluon) (0.14.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.0.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm<0.10.0,>=0.9.5->autogluon.multimodal==1.1.1->autogluon) (0.4.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.1.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (3.1.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (4.0.3)\n",
            "Collecting pyarrow>=6.0.1 (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.6)\n",
            "Collecting requests (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (4.12.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.3.1)\n",
            "Collecting Pillow<11,>=10.0.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading Pillow-10.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->mlforecast<0.10.1,>=0.10.0->autogluon.timeseries[all]==1.1.1->autogluon) (0.41.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon) (24.3.25)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.18.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.12.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.4.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast<1.5,>=1.4.0->autogluon.timeseries[all]==1.1.1->autogluon) (0.5.6)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (4.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (2.11.1)\n",
            "Collecting pycryptodome (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openxlab-0.1.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.6/308.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (8.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.14.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.63.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.18.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (2.5)\n",
            "Collecting filelock (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openxlab-0.1.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.4/307.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.38-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.37-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.36-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.5/302.5 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.35-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.5/302.5 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.34-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.33-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.5/299.5 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading openxlab-0.0.32-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.9/298.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.31-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.9/298.9 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.30-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.5/298.5 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.29-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.28-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading openxlab-0.0.27-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.8/296.8 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.26-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.9/295.9 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.25-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.1/295.1 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.24-py3-none-any.whl (291 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.2/291.2 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.23-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.22-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.21-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.20-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.19-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.18-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.17-py3-none-any.whl (291 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.6/291.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.16-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.15-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.2/289.2 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.14-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.13-py3-none-any.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.0/282.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.12-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of requests[socks] to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (1.7.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Building wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19172 sha256=373b921ee9b122feafad955c7181820631d8ee9181c1697174443a399fb10efa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=60cb0b039895440789bdf6114708926f373be9a88284d44f7985353a58504c76\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=8f9cb94f5f95c1a2b996b5937c0b017d9801a79427a9543832dd317ec125c303\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py3, distlib, colorful, antlr4-python3-runtime, xxhash, virtualenv, tensorboardX, requests, pycryptodome, pyarrow, Pillow, orjson, ordered-set, openxlab, onnx, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nptyping, lightning-utilities, jmespath, humanfriendly, dill, colorama, window-ops, scikit-learn, pytesseract, pdf2image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, model-index, coloredlogs, botocore, utilsforecast, tokenizers, seqeval, s3transfer, opendatalab, onnxruntime, nvidia-cusolver-cu12, gluonts, catboost, aiohttp-cors, transformers, statsforecast, ray, openmim, opencensus, nlpaug, mlforecast, datasets, boto3, torchmetrics, pytorch-metric-learning, evaluate, autogluon.common, accelerate, timm, pytorch-lightning, optimum, autogluon.features, autogluon.core, lightning, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.0.1 accelerate-0.21.0 aiohttp-cors-0.7.0 antlr4-python3-runtime-4.9.3 autogluon-1.1.1 autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.multimodal-1.1.1 autogluon.tabular-1.1.1 autogluon.timeseries-1.1.1 boto3-1.34.137 botocore-1.34.137 catboost-1.2.5 colorama-0.4.6 coloredlogs-15.0.1 colorful-0.5.6 datasets-2.20.0 dill-0.3.8 distlib-0.3.8 evaluate-0.4.2 gluonts-0.15.1 humanfriendly-10.0 jmespath-1.0.1 lightning-2.3.1 lightning-utilities-0.11.3.post0 mlforecast-0.10.0 model-index-0.1.11 multiprocess-0.70.16 nlpaug-1.1.11 nptyping-2.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py3-7.352.0 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.2.3 onnx-1.16.1 onnxruntime-1.18.1 opencensus-0.11.4 opencensus-context-0.1.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optimum-1.18.1 ordered-set-4.1.0 orjson-3.10.5 pdf2image-1.17.0 py-spy-0.3.14 pyarrow-16.1.0 pycryptodome-3.20.0 pytesseract-0.3.10 pytorch-lightning-2.3.1 pytorch-metric-learning-2.3.0 ray-2.10.0 requests-2.32.3 s3transfer-0.10.2 scikit-learn-1.4.0 seqeval-1.2.2 statsforecast-1.4.0 tensorboardX-2.6.2.2 timm-0.9.16 tokenizers-0.15.2 torchmetrics-1.2.1 transformers-4.39.3 utilsforecast-0.0.10 virtualenv-20.26.3 window-ops-0.0.15 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pydevd_plugins"
                ]
              },
              "id": "e7a2cc7f24fe4266a0c42a64362896cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libgraphviz-dev' instead of 'graphviz-dev'\n",
            "graphviz is already the newest version (2.42.2-6).\n",
            "The following additional packages will be installed:\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libgvc6-plugins-gtk librsvg2-common libxdot4\n",
            "Suggested packages:\n",
            "  gvfs\n",
            "The following NEW packages will be installed:\n",
            "  libgail-common libgail18 libgraphviz-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libgvc6-plugins-gtk librsvg2-common libxdot4\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 2,433 kB of archives.\n",
            "After this operation, 7,694 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2 [125 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2 [2,037 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgail18 amd64 2.24.33-2ubuntu2 [15.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgail-common amd64 2.24.33-2ubuntu2 [132 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libxdot4 amd64 2.42.2-6 [16.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgvc6-plugins-gtk amd64 2.42.2-6 [22.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgraphviz-dev amd64 2.42.2-6 [58.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2 [7,932 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Fetched 2,433 kB in 0s (18.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgtk2.0-common_2.24.33-2ubuntu2_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../1-libgtk2.0-0_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../2-libgail18_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../3-libgail-common_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libxdot4:amd64.\n",
            "Preparing to unpack .../4-libxdot4_2.42.2-6_amd64.deb ...\n",
            "Unpacking libxdot4:amd64 (2.42.2-6) ...\n",
            "Selecting previously unselected package libgvc6-plugins-gtk.\n",
            "Preparing to unpack .../5-libgvc6-plugins-gtk_2.42.2-6_amd64.deb ...\n",
            "Unpacking libgvc6-plugins-gtk (2.42.2-6) ...\n",
            "Selecting previously unselected package libgraphviz-dev:amd64.\n",
            "Preparing to unpack .../6-libgraphviz-dev_2.42.2-6_amd64.deb ...\n",
            "Unpacking libgraphviz-dev:amd64 (2.42.2-6) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../7-libgtk2.0-bin_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../8-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libxdot4:amd64 (2.42.2-6) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2) ...\n",
            "Setting up libgvc6-plugins-gtk (2.42.2-6) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2) ...\n",
            "Setting up libgraphviz-dev:amd64 (2.42.2-6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Vb0n1Byc-53"
      },
      "outputs": [],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j1XO9F5dc-55"
      },
      "outputs": [],
      "source": [
        "random_seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/My Drive/361092'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzT9Zhk1ebZZ",
        "outputId": "b7250b48-f8c3-4fdc-f091-b4e53cca033c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0hLSX-NEc-55"
      },
      "outputs": [],
      "source": [
        "## Function for loading one of the 10 folds of the property dataset and concatinating the X and y values for train and test respectively.\n",
        "import pandas as pd\n",
        "\n",
        "def load_fold(fold_number, random_seed=42, sample_size=None):\n",
        "    df_X_train = pd.read_parquet(f'{base_path}/{fold_number}/X_train.parquet')\n",
        "    df_y_train = pd.read_parquet(f'{base_path}/{fold_number}/y_train.parquet')\n",
        "    df_X_test = pd.read_parquet(f'{base_path}/{fold_number}/X_test.parquet')\n",
        "    df_y_test = pd.read_parquet(f'{base_path}/{fold_number}/y_test.parquet')\n",
        "\n",
        "    # concatinating the X and y values for train and test respectively\n",
        "    df_train = pd.concat([df_X_train, df_y_train], axis=1)\n",
        "    df_test = pd.concat([df_X_test, df_y_test], axis=1)\n",
        "\n",
        "    # Convert to AutoGluon's TabularDataset\n",
        "    if sample_size:\n",
        "        train_dataset = TabularDataset(df_train).sample(n=sample_size, random_state=random_seed)\n",
        "        test_dataset = TabularDataset(df_test).sample(n=sample_size, random_state=random_seed)\n",
        "    else:\n",
        "        train_dataset = TabularDataset(df_train)\n",
        "        test_dataset = TabularDataset(df_test)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# Also instantiate the target column\n",
        "label_property = 'oz252'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fOvxTvSNc-56"
      },
      "outputs": [],
      "source": [
        "## Function to fit the model, with most of the hyperparameters present and set to default/None. (Add more hyperparameters if desirable)\n",
        "\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "def fit_gluon(train_dataset, problem_type='regression', hyperparameters=None, eval_metric='r2', presets='medium_quality', time_limit=100, fit_weighted_ensemble=None, num_cpus = None, num_gpus=None, auto_stack=None, num_bag_folds=None, num_bag_sets=None, num_stack_levels=None, num_trials=None, verbosity=None, ag_args_fit=None, feature_prune=None, excluded_model_types=None, keep_only_best=None):\n",
        "    predictor = TabularPredictor(label=label_property, problem_type=problem_type, eval_metric=eval_metric)\n",
        "\n",
        "    fit_args = {\n",
        "        'train_data': train_dataset,\n",
        "        'presets': presets,\n",
        "        'time_limit': time_limit,\n",
        "    }\n",
        "\n",
        "    if hyperparameters is not None:\n",
        "        fit_args['hyperparameters'] = hyperparameters\n",
        "    if auto_stack is not None:\n",
        "        fit_args['auto_stack'] = auto_stack\n",
        "    if num_bag_folds is not None:\n",
        "        fit_args['num_bag_folds'] = num_bag_folds\n",
        "    if num_bag_sets is not None:\n",
        "        fit_args['num_bag_sets'] = num_bag_sets\n",
        "    if num_stack_levels is not None:\n",
        "        fit_args['num_stack_levels'] = num_stack_levels\n",
        "    if num_trials is not None:\n",
        "        fit_args['num_trials'] = num_trials\n",
        "    if verbosity is not None:\n",
        "        fit_args['verbosity'] = verbosity\n",
        "    if ag_args_fit is not None:\n",
        "        fit_args['ag_args_fit'] = ag_args_fit\n",
        "    if feature_prune is not None:\n",
        "        fit_args['feature_prune'] = feature_prune\n",
        "    if excluded_model_types is not None:\n",
        "        fit_args['excluded_model_types'] = excluded_model_types\n",
        "    if fit_weighted_ensemble is not None:\n",
        "        fit_args['fit_weighted_ensemble'] = fit_weighted_ensemble\n",
        "    if num_cpus is not None:\n",
        "        fit_args['num_cpus'] = num_cpus\n",
        "    if num_gpus is not None:\n",
        "        fit_args['num_gpus'] = num_gpus\n",
        "    if keep_only_best is not None:\n",
        "        fit_args['keep_only_best'] = keep_only_best\n",
        "\n",
        "    predictor.fit(**fit_args)\n",
        "    return predictor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sQIUKVDbc-56"
      },
      "outputs": [],
      "source": [
        "## Function to evaluate a fitted model and training set.\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def evaluate_gluon(model, test_dataset):\n",
        "\n",
        "    leaderboard = model.leaderboard(test_dataset, only_pareto_frontier=True)\n",
        "\n",
        "    y_test = test_dataset[label_property]\n",
        "    x_test = test_dataset.drop(columns=[label_property])\n",
        "    y_pred = model.predict(x_test)\n",
        "    test_score = model.evaluate_predictions(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "    path_to_png = model.plot_ensemble_model()\n",
        "    L2_diagram = display(Image(filename=path_to_png))\n",
        "\n",
        "\n",
        "    return test_score, leaderboard, L2_diagram"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpu_info = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available: \", len(gpu_info))\n",
        "\n",
        "!cat /proc/cpuinfo | grep \"processor\" | wc -l"
      ],
      "metadata": {
        "id": "sVZQ4qBtpP6q",
        "outputId": "34694aec-0436-447e-df60-a0e317bcdd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1QLLrP5uc-57",
        "outputId": "d3dc84d9-14bd-4594-f586-a814edf8adc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_142645\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.93 GB / 12.67 GB (78.4%)\n",
            "Disk Space Avail:   46.67 GB / 78.19 GB (59.7%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_142645\"\n",
            "Train Data Rows:    7996\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10175.99 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 6): ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', 'oz115']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 6 | ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
            "\t0.6s = Fit runtime\n",
            "\t56 features in original data used to generate 56 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.66s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 99.34s of the 99.34s of remaining time.\n",
            "\t-0.1737\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 99.28s of the 99.27s of remaining time.\n",
            "\t-0.1778\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 99.22s of the 99.22s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0871\t = Validation score   (r2)\n",
            "\t1.74s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 97.44s of the 97.44s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.078\t = Validation score   (r2)\n",
            "\t1.27s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 96.15s of the 96.14s of remaining time.\n",
            "\t0.0786\t = Validation score   (r2)\n",
            "\t36.42s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 58.89s of the 58.89s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.0694\t = Validation score   (r2)\n",
            "\t3.08s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 55.79s of the 55.79s of remaining time.\n",
            "\t0.0865\t = Validation score   (r2)\n",
            "\t16.36s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 38.62s of the 38.62s of remaining time.\n",
            "\t0.0405\t = Validation score   (r2)\n",
            "\t9.27s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 29.29s of the 29.29s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:27:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:27:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.0666\t = Validation score   (r2)\n",
            "\t1.57s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 27.69s of the 27.69s of remaining time.\n",
            "\t-0.1703\t = Validation score   (r2)\n",
            "\t10.72s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 16.91s of the 16.91s of remaining time.\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0878\t = Validation score   (r2)\n",
            "\t3.54s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 86.68s ... Best model: LightGBMLarge | Estimated inference throughput: 157584.3 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_142645/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_142645/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_142645/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20240702_142645/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_142645/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_142645/models/CatBoost will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20240702_142645/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_142645/models/NeuralNetFastAI will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20240702_142645/models/XGBoost will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_142645/models/NeuralNetTorch will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_142645\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "\n",
            ": {'r2': 0.07906580501336258, 'root_mean_squared_error': -0.025563049563564377, 'mean_squared_error': -0.0006534695029892488, 'mean_absolute_error': -0.018306527783066674, 'pearsonr': 0.2851379005905823, 'median_absolute_error': -0.013085137493133558} \n",
            "\n",
            "\n",
            "           model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0  LightGBMLarge    0.079066   0.087833          r2        0.018678   \n",
            "\n",
            "   pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0       0.005077  3.537299                 0.018678                0.005077   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           3.537299            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "           model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  LightGBMLarge   0.087833          r2       0.005077  3.537299                0.005077           3.537299            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'LGBModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_142645SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'LightGBMLarge': 'LGBModel'}, 'model_performance': {'LightGBMLarge': 0.08783287941768636}, 'model_best': 'LightGBMLarge', 'model_paths': {'LightGBMLarge': ['LightGBMLarge']}, 'model_fit_times': {'LightGBMLarge': 3.5372989177703857}, 'model_pred_times': {'LightGBMLarge': 0.00507664680480957}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'LightGBMLarge': {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}}, 'leaderboard':            model  score_val eval_metric  pred_time_val  fit_time  \\\n",
            "0  LightGBMLarge   0.087833          r2       0.005077  3.537299   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.005077           3.537299            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_142816\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.79 GB / 12.67 GB (77.2%)\n",
            "Disk Space Avail:   46.66 GB / 78.19 GB (59.7%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_142816\"\n",
            "Train Data Rows:    7996\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10024.67 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['oz115']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 6): ['oz46', 'oz50', 'oz69', 'oz100', 'oz107', 'oz111']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 6 | ['oz46', 'oz50', 'oz69', 'oz100', 'oz107', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 13 | ['oz40', 'oz42', 'oz71', 'oz73', 'oz79', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 13 | ['oz40', 'oz42', 'oz71', 'oz73', 'oz79', ...]\n",
            "\t0.7s = Fit runtime\n",
            "\t55 features in original data used to generate 55 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.66 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.76s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 99.24s of the 99.24s of remaining time.\n",
            "\t-0.1691\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 99.18s of the 99.18s of remaining time.\n",
            "\t-0.1555\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 99.13s of the 99.13s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0672\t = Validation score   (r2)\n",
            "\t1.7s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 97.39s of the 97.39s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0592\t = Validation score   (r2)\n",
            "\t1.18s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 96.2s of the 96.19s of remaining time.\n",
            "\t0.0651\t = Validation score   (r2)\n",
            "\t37.01s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 58.66s of the 58.65s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.0599\t = Validation score   (r2)\n",
            "\t2.55s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 56.08s of the 56.07s of remaining time.\n",
            "\t0.0745\t = Validation score   (r2)\n",
            "\t14.7s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 40.82s of the 40.82s of remaining time.\n",
            "\t0.0165\t = Validation score   (r2)\n",
            "\t9.9s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 30.88s of the 30.88s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:29:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:29:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.0555\t = Validation score   (r2)\n",
            "\t1.25s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 29.6s of the 29.6s of remaining time.\n",
            "\t-0.2314\t = Validation score   (r2)\n",
            "\t12.31s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 17.25s of the 17.24s of remaining time.\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0916\t = Validation score   (r2)\n",
            "\t2.53s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 85.37s ... Best model: LightGBMLarge | Estimated inference throughput: 132176.9 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_142816/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_142816/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_142816/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20240702_142816/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_142816/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_142816/models/CatBoost will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20240702_142816/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_142816/models/NeuralNetFastAI will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20240702_142816/models/XGBoost will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_142816/models/NeuralNetTorch will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_142816\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2\n",
            "\n",
            ": {'r2': 0.10482412750410353, 'root_mean_squared_error': -0.026926435645650552, 'mean_squared_error': -0.0007250329365793606, 'mean_absolute_error': -0.019656305393266193, 'pearsonr': 0.32838120716246755, 'median_absolute_error': -0.013856546951293902} \n",
            "\n",
            "\n",
            "           model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0  LightGBMLarge    0.104824   0.091638          r2        0.019156   \n",
            "\n",
            "   pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0       0.006052  2.530495                 0.019156                0.006052   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           2.530495            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "           model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  LightGBMLarge   0.091638          r2       0.006052  2.530495                0.006052           2.530495            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'LGBModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 13 | ['oz40', 'oz42', 'oz71', 'oz73', 'oz79', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_142816SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'LightGBMLarge': 'LGBModel'}, 'model_performance': {'LightGBMLarge': 0.09163812797018545}, 'model_best': 'LightGBMLarge', 'model_paths': {'LightGBMLarge': ['LightGBMLarge']}, 'model_fit_times': {'LightGBMLarge': 2.5304946899414062}, 'model_pred_times': {'LightGBMLarge': 0.006052494049072266}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'LightGBMLarge': {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}}, 'leaderboard':            model  score_val eval_metric  pred_time_val  fit_time  \\\n",
            "0  LightGBMLarge   0.091638          r2       0.006052  2.530495   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.006052           2.530495            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_142944\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.80 GB / 12.67 GB (77.3%)\n",
            "Disk Space Avail:   46.66 GB / 78.19 GB (59.7%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_142944\"\n",
            "Train Data Rows:    7996\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10040.54 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['oz222']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 5): ['oz50', 'oz100', 'oz107', 'oz111', 'oz115']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 5 | ['oz50', 'oz100', 'oz107', 'oz111', 'oz115']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 14 | ['oz40', 'oz42', 'oz46', 'oz69', 'oz71', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz69', 'oz71', ...]\n",
            "\t0.8s = Fit runtime\n",
            "\t56 features in original data used to generate 56 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.81s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 99.19s of the 99.19s of remaining time.\n",
            "\t-0.0855\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 99.13s of the 99.13s of remaining time.\n",
            "\t-0.0808\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 99.07s of the 99.07s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0881\t = Validation score   (r2)\n",
            "\t2.71s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 96.29s of the 96.28s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.079\t = Validation score   (r2)\n",
            "\t2.43s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 93.82s of the 93.82s of remaining time.\n",
            "\t0.0681\t = Validation score   (r2)\n",
            "\t38.11s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 55.17s of the 55.17s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.0758\t = Validation score   (r2)\n",
            "\t9.78s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 45.37s of the 45.36s of remaining time.\n",
            "\t0.0976\t = Validation score   (r2)\n",
            "\t15.37s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 29.36s of the 29.35s of remaining time.\n",
            "\t0.0321\t = Validation score   (r2)\n",
            "\t10.15s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 19.13s of the 19.13s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:31:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:31:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.0729\t = Validation score   (r2)\n",
            "\t1.86s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 17.24s of the 17.23s of remaining time.\n",
            "\t-0.0853\t = Validation score   (r2)\n",
            "\t8.08s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 9.1s of the 9.1s of remaining time.\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0943\t = Validation score   (r2)\n",
            "\t6.52s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 97.53s ... Best model: ExtraTreesMSE | Estimated inference throughput: 5366.8 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_142944/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_142944/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_142944/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20240702_142944/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_142944/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_142944/models/CatBoost will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_142944/models/NeuralNetFastAI will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20240702_142944/models/XGBoost will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_142944/models/NeuralNetTorch will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20240702_142944/models/LightGBMLarge will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_142944\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3\n",
            "\n",
            ": {'r2': 0.10358836607720923, 'root_mean_squared_error': -0.02663796118440007, 'mean_squared_error': -0.0007095809760616047, 'mean_absolute_error': -0.019457127183663162, 'pearsonr': 0.3266601809236527, 'median_absolute_error': -0.013986476684570315} \n",
            "\n",
            "\n",
            "           model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0  ExtraTreesMSE    0.103588   0.097649          r2        0.444366   \n",
            "\n",
            "   pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0       0.149064  15.374551                 0.444366                0.149064   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0          15.374551            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "           model  score_val eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  ExtraTreesMSE   0.097649          r2       0.149064  15.374551                0.149064          15.374551            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'XTModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz69', 'oz71', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_142944SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'ExtraTreesMSE': 'XTModel'}, 'model_performance': {'ExtraTreesMSE': 0.09764947827351977}, 'model_best': 'ExtraTreesMSE', 'model_paths': {'ExtraTreesMSE': ['ExtraTreesMSE']}, 'model_fit_times': {'ExtraTreesMSE': 15.374550580978394}, 'model_pred_times': {'ExtraTreesMSE': 0.1490640640258789}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'ExtraTreesMSE': {'n_estimators': 300, 'max_leaf_nodes': 15000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'criterion': 'squared_error'}}, 'leaderboard':            model  score_val eval_metric  pred_time_val   fit_time  \\\n",
            "0  ExtraTreesMSE   0.097649          r2       0.149064  15.374551   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.149064          15.374551            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_143127\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.66 GB / 12.67 GB (76.2%)\n",
            "Disk Space Avail:   46.47 GB / 78.19 GB (59.4%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_143127\"\n",
            "Train Data Rows:    7996\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9895.49 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['oz111']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 6): ['oz42', 'oz50', 'oz100', 'oz107', 'oz115', 'oz222']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 6 | ['oz42', 'oz50', 'oz100', 'oz107', 'oz115', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 13 | ['oz40', 'oz46', 'oz69', 'oz71', 'oz73', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 13 | ['oz40', 'oz46', 'oz69', 'oz71', 'oz73', ...]\n",
            "\t0.7s = Fit runtime\n",
            "\t55 features in original data used to generate 55 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.66 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.8s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 99.2s of the 99.2s of remaining time.\n",
            "\t-0.1215\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 99.13s of the 99.12s of remaining time.\n",
            "\t-0.128\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 99.05s of the 99.05s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0733\t = Validation score   (r2)\n",
            "\t2.0s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 96.99s of the 96.99s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0751\t = Validation score   (r2)\n",
            "\t3.49s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 93.46s of the 93.46s of remaining time.\n",
            "\t0.0509\t = Validation score   (r2)\n",
            "\t38.75s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 54.17s of the 54.17s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.0686\t = Validation score   (r2)\n",
            "\t4.53s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 49.62s of the 49.62s of remaining time.\n",
            "\t0.0736\t = Validation score   (r2)\n",
            "\t15.65s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 33.1s of the 33.1s of remaining time.\n",
            "\t0.0453\t = Validation score   (r2)\n",
            "\t8.67s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 24.39s of the 24.39s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:32:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:32:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.0753\t = Validation score   (r2)\n",
            "\t1.43s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 22.94s of the 22.93s of remaining time.\n",
            "\t-0.1699\t = Validation score   (r2)\n",
            "\t9.92s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 12.98s of the 12.97s of remaining time.\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0751\t = Validation score   (r2)\n",
            "\t2.4s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 89.5s ... Best model: XGBoost | Estimated inference throughput: 125879.5 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_143127/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_143127/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_143127/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20240702_143127/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_143127/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_143127/models/CatBoost will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20240702_143127/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_143127/models/NeuralNetFastAI will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_143127/models/NeuralNetTorch will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20240702_143127/models/LightGBMLarge will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_143127\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4\n",
            "\n",
            ": {'r2': 0.08052636327082996, 'root_mean_squared_error': -0.027927349016354932, 'mean_squared_error': -0.0007799368230813008, 'mean_absolute_error': -0.01979536623245257, 'pearsonr': 0.28487799951292037, 'median_absolute_error': -0.013212386871337856} \n",
            "\n",
            "\n",
            "     model  score_test  score_val eval_metric  pred_time_test  pred_time_val  \\\n",
            "0  XGBoost    0.080526   0.075258          r2        0.015709       0.006355   \n",
            "\n",
            "   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0  1.428626                 0.015709                0.006355   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           1.428626            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "     model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  XGBoost   0.075258          r2       0.006355  1.428626                0.006355           1.428626            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'XGBoostModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 13 | ['oz40', 'oz46', 'oz69', 'oz71', 'oz73', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_143127SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'XGBoost': 'XGBoostModel'}, 'model_performance': {'XGBoost': 0.0752575216053144}, 'model_best': 'XGBoost', 'model_paths': {'XGBoost': ['XGBoost']}, 'model_fit_times': {'XGBoost': 1.4286260604858398}, 'model_pred_times': {'XGBoost': 0.00635528564453125}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'XGBoost': {'n_estimators': 10000, 'learning_rate': 0.1, 'n_jobs': -1, 'proc.max_category_levels': 100, 'objective': 'reg:squarederror', 'booster': 'gbtree'}}, 'leaderboard':      model  score_val eval_metric  pred_time_val  fit_time  \\\n",
            "0  XGBoost   0.075258          r2       0.006355  1.428626   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.006355           1.428626            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_143300\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.77 GB / 12.67 GB (77.1%)\n",
            "Disk Space Avail:   46.47 GB / 78.19 GB (59.4%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_143300\"\n",
            "Train Data Rows:    7996\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10007.87 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 5): ['oz100', 'oz107', 'oz115', 'oz222', 'oz234']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 5 | ['oz100', 'oz107', 'oz115', 'oz222', 'oz234']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t1.4s = Fit runtime\n",
            "\t57 features in original data used to generate 57 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.68 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.49s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 98.51s of the 98.51s of remaining time.\n",
            "\t-0.0692\t = Validation score   (r2)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 98.4s of the 98.4s of remaining time.\n",
            "\t-0.0726\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 98.29s of the 98.29s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.1083\t = Validation score   (r2)\n",
            "\t4.17s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 93.97s of the 93.97s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.1106\t = Validation score   (r2)\n",
            "\t2.31s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 91.58s of the 91.57s of remaining time.\n",
            "\t0.0851\t = Validation score   (r2)\n",
            "\t37.73s\t = Training   runtime\n",
            "\t0.22s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 53.09s of the 53.09s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.0751\t = Validation score   (r2)\n",
            "\t3.69s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 49.38s of the 49.38s of remaining time.\n",
            "\t0.0964\t = Validation score   (r2)\n",
            "\t15.08s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 33.73s of the 33.72s of remaining time.\n",
            "\t0.0408\t = Validation score   (r2)\n",
            "\t8.0s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 25.69s of the 25.68s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:34:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:34:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.1228\t = Validation score   (r2)\n",
            "\t2.01s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 23.64s of the 23.64s of remaining time.\n",
            "\tRan out of time, stopping training early. (Stopping on epoch 68)\n",
            "\t0.0529\t = Validation score   (r2)\n",
            "\t23.4s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 99.84s ... Best model: XGBoost | Estimated inference throughput: 108443.0 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_143300/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_143300/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_143300/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20240702_143300/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_143300/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_143300/models/CatBoost will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20240702_143300/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_143300/models/NeuralNetFastAI will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_143300/models/NeuralNetTorch will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_143300\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5\n",
            "\n",
            ": {'r2': 0.04415555379663505, 'root_mean_squared_error': -0.041919793294837415, 'mean_squared_error': -0.001757269069881896, 'mean_absolute_error': -0.02158484743659381, 'pearsonr': 0.21235925567541766, 'median_absolute_error': -0.014706150741577106} \n",
            "\n",
            "\n",
            "     model  score_test  score_val eval_metric  pred_time_test  pred_time_val  \\\n",
            "0  XGBoost    0.044156   0.122827          r2        0.018947       0.007377   \n",
            "\n",
            "   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0  2.007095                 0.018947                0.007377   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           2.007095            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "     model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  XGBoost   0.122827          r2       0.007377  2.007095                0.007377           2.007095            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'XGBoostModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_143300SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'XGBoost': 'XGBoostModel'}, 'model_performance': {'XGBoost': 0.12282683324427612}, 'model_best': 'XGBoost', 'model_paths': {'XGBoost': ['XGBoost']}, 'model_fit_times': {'XGBoost': 2.0070953369140625}, 'model_pred_times': {'XGBoost': 0.007377147674560547}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'XGBoost': {'n_estimators': 10000, 'learning_rate': 0.1, 'n_jobs': -1, 'proc.max_category_levels': 100, 'objective': 'reg:squarederror', 'booster': 'gbtree'}}, 'leaderboard':      model  score_val eval_metric  pred_time_val  fit_time  \\\n",
            "0  XGBoost   0.122827          r2       0.007377  2.007095   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.007377           2.007095            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_143443\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.76 GB / 12.67 GB (77.0%)\n",
            "Disk Space Avail:   46.46 GB / 78.19 GB (59.4%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_143443\"\n",
            "Train Data Rows:    7997\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9999.67 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 6): ['oz73', 'oz100', 'oz107', 'oz115', 'oz222', 'oz234']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 6 | ['oz73', 'oz100', 'oz107', 'oz115', 'oz222', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 14 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t1.3s = Fit runtime\n",
            "\t56 features in original data used to generate 56 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.36s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7197, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 98.64s of the 98.63s of remaining time.\n",
            "\t-0.0145\t = Validation score   (r2)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 98.52s of the 98.52s of remaining time.\n",
            "\t-0.0161\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 98.42s of the 98.42s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0376\t = Validation score   (r2)\n",
            "\t2.25s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 96.12s of the 96.12s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0248\t = Validation score   (r2)\n",
            "\t1.2s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 94.9s of the 94.9s of remaining time.\n",
            "\t0.036\t = Validation score   (r2)\n",
            "\t35.6s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 58.72s of the 58.72s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.0278\t = Validation score   (r2)\n",
            "\t3.19s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 55.52s of the 55.52s of remaining time.\n",
            "\t0.0354\t = Validation score   (r2)\n",
            "\t14.06s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 40.61s of the 40.61s of remaining time.\n",
            "\t0.026\t = Validation score   (r2)\n",
            "\t8.57s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 32.0s of the 31.99s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:35:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:35:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.0406\t = Validation score   (r2)\n",
            "\t1.34s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 30.63s of the 30.63s of remaining time.\n",
            "\t0.0242\t = Validation score   (r2)\n",
            "\t14.3s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 16.29s of the 16.28s of remaining time.\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0346\t = Validation score   (r2)\n",
            "\t4.13s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 87.98s ... Best model: XGBoost | Estimated inference throughput: 113570.6 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_143443/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_143443/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_143443/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20240702_143443/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_143443/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_143443/models/CatBoost will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20240702_143443/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_143443/models/NeuralNetFastAI will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_143443/models/NeuralNetTorch will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20240702_143443/models/LightGBMLarge will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_143443\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJQAAABQCAIAAABwJ8shAAAABmJLR0QA/wD/AP+gvaeTAAAMcElEQVR4nO2dd1xT5xrHvwEkICoCbkVUll6t26ogDhy1WrXWFrHXVRcqiIrbol7rwA7FRdWrFXtdgNqPWqXeWwRtnXVUa5WIC+tCLZUhyArcP4iMcDIIBBJ7vh/+SJ73ec95c37JyTnv732IJC8vDxHjxKSyByCiO6J4RowonhFjpr45Ojo6MTGxYoYioh65XO7u7m5vb18YylPN6tWrK2+oIgLs2bOnqEAqP3nBwcGzZs0KGsr8/hU5PBEBgqMI2AdgampaNC78nRccHBwQECAqZwjkKxc0VKBJQDxROcOhQDlBLZTFE5UzHNQrB8UvWKKioip0dCJqqW/N6o/I26L4A8LDw1VesOTfFYRPrJShihTj7F3WHiegj7ocgatNr476GpBGztxhUAh/pWFvQ3QApib0+IqEFJYPYe47APJcgqP49iy3nwPYWdG5KQcmA6z8gWVHycgGMDWhkQ2Tuxv3yX/tcQ0JGm7SKxg3R2RLabOMCd1wqgMwvBP9W9K7uSJh4k6iYlnvTQ8XzE25l8jKSEXTwndJeYUsgYNTeZnJd78yJpRuTnRzKtOQ/MNY712mLegPg5seq12dXeNYHsnJOLafxt6mULlbzwg9w67xvN8Wm6pYSWnVgD0TBDZSTcroLlhWITGtTIPJzePwb2Xagl4xOPEAz+bM7ofXvzl/D3/Pwvip29SwwEOLT1JaJl+foLEtfVsoIhtjaLKQ6v70XUt8osrgs1Tcv8DCl/pziE/kg03cT0Tiw/7L5fsSywdDFA+Y1osX6bxILxb8Kw1bKyQSdR0PXUXiQzV/5n7HqC6K4KX7LPmeg1N4+DkNa+K3V2Vw/yVc6pK8jsO+VJOyeSRSM/K28GF7fbzKsmKI4uXk4h/GYV+OXuOb04XxmlX5K40C8/hREs0+ReKD6+LCnCFtyNtCZggXFvDzbYZtBjh1mx4utLXH2pJpvTh9R2XQzZFDV5i8m4xsalWroNerM4Yo3twD+PWif0s2eDM9nLiningPZ9KziPxd8bRhTS5/CnBoqvIWzM1oUZ/x7hyXobRSIA9KfnQLgm3tiVtGRwe8txF2oRxfk14wOPF2nMGxNj1cAMa5M7g1I7aRlQPgVIdZfZm4k4NXSMkgIxtZgnL33DzF353nbPmJTk2QSPBw5sRNrjwg+RUbYxQbFwxeiCcrB5/ujOrM5T+QQLacx0mKOxBDw7BuFWZEsCEa13p84kZVc2JuEiXjeSrOizgzj4Y1CRpKIxuWfI/3VsxMqVeDj99WnN9WHWNdNBnZmE4GsLakbwu++RCgfWOWDmZwCEmvcHNk6yiVQVkC76wjLYvWDdk7gVrVaNcYl8V8OYwpPSrrqKhEUnQBUkRExPDhw/NnYkQql4iLDN9KUS0kPoSHh3t5eRVEDO60KaI9onhGjCieESOKZ8SI4hkxb7J4/mGYTdEwLXlchlMgVf0YE0q2vBStKRk4LOD2M3WZf75k2Gaq+mE7k1XHyuU1FeNNFm+9N31aqEtIy8R7K59/wJ0VxCYQHFWK1s+O8CRZQ+bI7bRpxNOv+DWwrP6GIG+yeBqJuUmd6gxrT31rpnuy75K2rYev8nYTzM3UZcY95cZjFg2kugUOdnw5rPzHX57iKfkpQOgZnAKR+lJ/DtcegZAL47MLiQ+bf8J1MefvsTySenOwncmKSJU7sp+PyWQm7gSYcwCzKWz9GaDr55hPxXYmG2IEenVZxfbTxSI3n+JYW/HYuW7hJKr61uepHJcVW28gmPlLPC8zab4ESz+cAomWqTt0ulGe4in5KVce4LeX0DGkrGPjCOS5wi7MlpGYmtC5KZs+5txdwi7wayA3lhJygjvPhXf04wwsq/CvQQCBAxjblYkeAGfnkfU1336i0FKJc/MZ514skp6FRRXFYytz0rO0al3yPYvf05yZlE4jG4758yKYiR6M+4+mw1d6ylM8JT/lp1v0csXDGakZw9rT1l7YhcmnqR2ezTEx4fpjGsyl/hyeJAvMO+fTvB6DWhP0A0DICab2BHieyoANWE9n6CbFRLZGrKSFM85pWdhV09y68xz9W2JnpTnTSoqZCU1rYVGFUZ25n0hScXuy7JSneEp+Sm4eJqqNU2FrJo9BrQsXuw18S2X3RQP55jRxT7mZQPvGANvPkJvH3ZWcnaftgF3rcuv15aIsAQdbza3bTzPkayQ+SHxIy8R5EUd+E85s2QBZApk5ADm5SCRYmms7MC0pT/GU/BQPJ6Jiifyd9CxSM0h+JezCFKVTE6JvcjKOrByepyLPVbmvlg0Y0Io+wYzvpohkZmNhhtSs8DhqpJcrf75k/2WeJLM+mqHtNLfGzCp8b1lJubWM91oLZ3ZyoEktlh4hJYPQM7g1Q1reFk55iidLoNVSrKZxXMYkDzo4EOyFfxg2M/Fcw8MXhS6M/XwSUhSrsnx2Ic+lyyqSX9G1GXP6MWIbtgHM3k+u2orrRQNxsKO7s+Lp8E5c/oMmCzgRR9wzvvgvMyKIimXKbk7EAXQOKubLA1XN2TuBuQdw/JRWDRSLJF9m0nAep+8ItwoimCmRsN+H47HUnc2x62wbrfNxVYloCRko2lhChmXGKvHwBfbzlYPj3fXyLjZGDFq8RjaIpwE1/K1nWIwdUTwjRhTPiKkE8dIyeXc9Fr4cvkrHlYrr+NKic0c1xNyk+WKB+NMUXBYh9aVWAKNDeWUwywArQbxd56lvzauNDG7DxYX0fH2r7h+moWPRhKIdy4WNMYRfVFSOKfEqG6+OJK3lciAX4gkRmvWuFCpBPFkCzWorlxxorMfRd8GOXy8CBwg3NbFj+RAsq9DYlu7OPEzS4zBKRUWLF3KCTSdZdAiJD8uOUmWKYlW5xnqcogkbYgo75jtKDguQ+mI1jbeWUmc25lMZsEExQVPSYyrpDWlDTi6yBE7GMaQNCPlfFU9F3+f59iT2CfWsFW/zAmNh80iOXScjRGVHpYSj1xQPtozk27NETsO5DpN2Ic8l5GOS0nEK5FESj5MUHpNEQvvleHfCsTbnStz4a4NTII9eMKMPHs7w2v+KDuC3h1ST6rLBsvOGXG3WrYG5Ga0aIDWjhgWNbbG1IjObX+K18pi04dYy7qwg8SXeW8Ew6oneEPFUob3HpJEqpjS2xac7x66DYdQTGYp4GutxdCvY0d5jUsOu8xy+SracxDS2naJNIyjhf1UKFf2dd+gqO84CpGaQmUNULBfjqWeNh5OGepyiBTs3nxZ2DLtAZg6ea9j0T5ZHkplDdxd+vMHTFMbu4NRchceUksGwdmwbjSl0DmJSd8YXXxIxOpTIa8hzaTiPsV1Z8T4vM3FdTMQk7G0YtZ2EFKqa4+7IznFQop6oUhAtIQPFKC0h0QbSHoMTT7SBtMdQLlhEdEAUz4ipUPH2XaLGdCZovfxUH9aBRvRXnqLKtdCZChXvow4sHqghR6/WgTboqTxFjWuhMxV92jRRu0MD/19f+ehWnqLGtdCZUos3eTdmU3D/Aks/uqxSFFVoLBZZd5ymC5H68tkRdRsvR+ugJJVbnlKSsvsSpRZvrRfyXMIn8uwrWtRnRgRoKhb5JZ6FB9k9npfrmd1P3caL/q+vab3o/fr0tWUkUjMip5G6jo860Nae28u5vZyoWB4lcf6eEZSnlESpLkcHdLzPq2eNmQlTe9BvXWGwqR3t7Ln+RDGRn48sgd8f0+8fuDkCWJThxrLAOpAlUMOCGhbK1kHBHgve+0UpKE/ZOKJYecqYHZy+TVoWTkK9SqJDeYogbo4EHgIY56ajL1Gm77wsOaYlqkVKTuTLc8t/mb76PaqisspTBCm7L6GjeJnZpGawPpqerspNJSfyOzTmfze4+pCkdC6pnYDXt3VQWeUpgpTdl9DxE9FwHtly3B3ZPgaKFIucX1BYLFIwkf9OS8a64bkGiyq41uXcXUZ1ESgRolytA1UsGsi0sGLlKdtO0WQBH7RXlKc8TiYqlkv3qVWNni4CFkR+WcmkXSQk49WxsDwl34JwdxRoRci1KLsvUWpXISMbSz/km9XV3omUHT26CuoLJzWiV+vg7+NLlFq80aEAbT7jcqDulyF6tQ7+Pr5EqQ9/xCR9DENEF0RXwThYEwVgZ2dXNCiKZwQERzFrH0FBQb179y4aF8UzdBS/5RUUNH++8mWYKJ5Bo0Y5RPEMHDXKQfHfzwsPD6/YsYloYPXq1Wp+kbnYDMuDBw/OnTtXiWMVKYqdnZ2np6eahGLiiRgX4neeESOKZ8SI4hkx/wfA48F95P0eGgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 6\n",
            "\n",
            ": {'r2': 0.08702254088089911, 'root_mean_squared_error': -0.025533496703585398, 'mean_squared_error': -0.0006519594539120063, 'mean_absolute_error': -0.018374559557188738, 'pearsonr': 0.29517600300685415, 'median_absolute_error': -0.012902207195281956} \n",
            "\n",
            "\n",
            "     model  score_test  score_val eval_metric  pred_time_test  pred_time_val  \\\n",
            "0  XGBoost    0.087023   0.040568          r2        0.037823       0.007044   \n",
            "\n",
            "   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0  1.341221                 0.037823                0.007044   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           1.341221            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "     model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  XGBoost   0.040568          r2       0.007044  1.341221                0.007044           1.341221            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'XGBoostModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_143443SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'XGBoost': 'XGBoostModel'}, 'model_performance': {'XGBoost': 0.04056829287911956}, 'model_best': 'XGBoost', 'model_paths': {'XGBoost': ['XGBoost']}, 'model_fit_times': {'XGBoost': 1.3412210941314697}, 'model_pred_times': {'XGBoost': 0.007044076919555664}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'XGBoost': {'n_estimators': 10000, 'learning_rate': 0.1, 'n_jobs': -1, 'proc.max_category_levels': 100, 'objective': 'reg:squarederror', 'booster': 'gbtree'}}, 'leaderboard':      model  score_val eval_metric  pred_time_val  fit_time  \\\n",
            "0  XGBoost   0.040568          r2       0.007044  1.341221   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.007044           1.341221            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_143615\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.78 GB / 12.67 GB (77.2%)\n",
            "Disk Space Avail:   46.46 GB / 78.19 GB (59.4%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_143615\"\n",
            "Train Data Rows:    7997\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10016.83 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 4): ['oz73', 'oz115', 'oz222', 'oz234']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 4 | ['oz73', 'oz115', 'oz222', 'oz234']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 16 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 16 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t0.9s = Fit runtime\n",
            "\t58 features in original data used to generate 58 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.68 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.98s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7197, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 99.02s of the 99.02s of remaining time.\n",
            "\t0.0129\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 98.95s of the 98.95s of remaining time.\n",
            "\t0.0096\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 98.89s of the 98.89s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0467\t = Validation score   (r2)\n",
            "\t1.97s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 96.85s of the 96.84s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0426\t = Validation score   (r2)\n",
            "\t1.53s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 95.29s of the 95.29s of remaining time.\n",
            "\t0.0388\t = Validation score   (r2)\n",
            "\t37.84s\t = Training   runtime\n",
            "\t0.22s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 56.77s of the 56.77s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.0324\t = Validation score   (r2)\n",
            "\t2.89s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 53.86s of the 53.86s of remaining time.\n",
            "\t0.0449\t = Validation score   (r2)\n",
            "\t14.86s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 38.4s of the 38.4s of remaining time.\n",
            "No improvement since epoch 7: early stopping\n",
            "\t0.0156\t = Validation score   (r2)\n",
            "\t7.63s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 30.71s of the 30.71s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:37:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:37:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.0438\t = Validation score   (r2)\n",
            "\t1.88s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 28.74s of the 28.74s of remaining time.\n",
            "\t0.0309\t = Validation score   (r2)\n",
            "\t17.1s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 11.6s of the 11.6s of remaining time.\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0481\t = Validation score   (r2)\n",
            "\t2.6s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 91.09s ... Best model: LightGBMLarge | Estimated inference throughput: 136783.8 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_143615/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_143615/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_143615/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20240702_143615/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_143615/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_143615/models/CatBoost will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20240702_143615/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_143615/models/NeuralNetFastAI will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20240702_143615/models/XGBoost will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_143615/models/NeuralNetTorch will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_143615\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 7\n",
            "\n",
            ": {'r2': 0.0960121802768803, 'root_mean_squared_error': -0.026058858102279064, 'mean_squared_error': -0.0006790640855947153, 'mean_absolute_error': -0.01892059174367114, 'pearsonr': 0.310423419324385, 'median_absolute_error': -0.01357273092269895} \n",
            "\n",
            "\n",
            "           model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0  LightGBMLarge    0.096012   0.048065          r2        0.019268   \n",
            "\n",
            "   pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0       0.005849  2.604801                 0.019268                0.005849   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           2.604801            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "           model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  LightGBMLarge   0.048065          r2       0.005849  2.604801                0.005849           2.604801            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'LGBModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 16 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_143615SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'LightGBMLarge': 'LGBModel'}, 'model_performance': {'LightGBMLarge': 0.048064571184843086}, 'model_best': 'LightGBMLarge', 'model_paths': {'LightGBMLarge': ['LightGBMLarge']}, 'model_fit_times': {'LightGBMLarge': 2.6048007011413574}, 'model_pred_times': {'LightGBMLarge': 0.00584864616394043}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'LightGBMLarge': {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}}, 'leaderboard':            model  score_val eval_metric  pred_time_val  fit_time  \\\n",
            "0  LightGBMLarge   0.048065          r2       0.005849  2.604801   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.005849           2.604801            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_143750\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.78 GB / 12.67 GB (77.1%)\n",
            "Disk Space Avail:   46.45 GB / 78.19 GB (59.4%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_143750\"\n",
            "Train Data Rows:    7997\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10015.17 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 5): ['oz73', 'oz112', 'oz115', 'oz222', 'oz234']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 5 | ['oz73', 'oz112', 'oz115', 'oz222', 'oz234']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t1.1s = Fit runtime\n",
            "\t57 features in original data used to generate 57 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.68 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7197, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 98.88s of the 98.88s of remaining time.\n",
            "\t0.0256\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 98.79s of the 98.78s of remaining time.\n",
            "\t0.0194\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 98.68s of the 98.67s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0495\t = Validation score   (r2)\n",
            "\t4.85s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 93.75s of the 93.74s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0477\t = Validation score   (r2)\n",
            "\t1.27s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 92.47s of the 92.46s of remaining time.\n",
            "\t0.0405\t = Validation score   (r2)\n",
            "\t35.66s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 56.23s of the 56.23s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.0396\t = Validation score   (r2)\n",
            "\t8.75s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 47.46s of the 47.46s of remaining time.\n",
            "\t0.0501\t = Validation score   (r2)\n",
            "\t14.54s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 32.4s of the 32.4s of remaining time.\n",
            "\t0.0372\t = Validation score   (r2)\n",
            "\t8.06s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 24.27s of the 24.27s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:39:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:39:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.0501\t = Validation score   (r2)\n",
            "\t1.86s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 22.36s of the 22.36s of remaining time.\n",
            "\t0.0225\t = Validation score   (r2)\n",
            "\t17.26s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 5.06s of the 5.05s of remaining time.\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.051\t = Validation score   (r2)\n",
            "\t2.64s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 97.67s ... Best model: LightGBMLarge | Estimated inference throughput: 147485.5 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_143750/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_143750/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_143750/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20240702_143750/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_143750/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_143750/models/CatBoost will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20240702_143750/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_143750/models/NeuralNetFastAI will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20240702_143750/models/XGBoost will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_143750/models/NeuralNetTorch will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_143750\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 8\n",
            "\n",
            ": {'r2': 0.10879713165444072, 'root_mean_squared_error': -0.02549827928046031, 'mean_squared_error': -0.0006501622462643516, 'mean_absolute_error': -0.01841056793662235, 'pearsonr': 0.33320294479274654, 'median_absolute_error': -0.01283192190361021} \n",
            "\n",
            "\n",
            "           model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0  LightGBMLarge    0.108797   0.050999          r2        0.013586   \n",
            "\n",
            "   pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0       0.005424  2.643405                 0.013586                0.005424   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           2.643405            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "           model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  LightGBMLarge   0.050999          r2       0.005424  2.643405                0.005424           2.643405            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'LGBModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_143750SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'LightGBMLarge': 'LGBModel'}, 'model_performance': {'LightGBMLarge': 0.0509986780850783}, 'model_best': 'LightGBMLarge', 'model_paths': {'LightGBMLarge': ['LightGBMLarge']}, 'model_fit_times': {'LightGBMLarge': 2.6434054374694824}, 'model_pred_times': {'LightGBMLarge': 0.0054242610931396484}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'LightGBMLarge': {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}}, 'leaderboard':            model  score_val eval_metric  pred_time_val  fit_time  \\\n",
            "0  LightGBMLarge   0.050999          r2       0.005424  2.643405   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.005424           2.643405            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_143932\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.78 GB / 12.67 GB (77.2%)\n",
            "Disk Space Avail:   46.45 GB / 78.19 GB (59.4%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_143932\"\n",
            "Train Data Rows:    7997\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10016.99 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 5): ['oz73', 'oz112', 'oz115', 'oz222', 'oz234']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 5 | ['oz73', 'oz112', 'oz115', 'oz222', 'oz234']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t0.9s = Fit runtime\n",
            "\t57 features in original data used to generate 57 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.68 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.93s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7197, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 99.07s of the 99.06s of remaining time.\n",
            "\t0.0342\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 98.99s of the 98.98s of remaining time.\n",
            "\t0.0314\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 98.9s of the 98.9s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0484\t = Validation score   (r2)\n",
            "\t5.25s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 93.57s of the 93.57s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0424\t = Validation score   (r2)\n",
            "\t1.5s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 92.04s of the 92.03s of remaining time.\n",
            "\t0.0436\t = Validation score   (r2)\n",
            "\t35.42s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 56.1s of the 56.1s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.037\t = Validation score   (r2)\n",
            "\t5.11s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 50.97s of the 50.97s of remaining time.\n",
            "\t0.0498\t = Validation score   (r2)\n",
            "\t14.23s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 35.97s of the 35.96s of remaining time.\n",
            "\t0.0514\t = Validation score   (r2)\n",
            "\t8.36s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 27.57s of the 27.56s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:40:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:40:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.039\t = Validation score   (r2)\n",
            "\t1.35s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 26.19s of the 26.19s of remaining time.\n",
            "\t0.0281\t = Validation score   (r2)\n",
            "\t12.49s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 13.65s of the 13.64s of remaining time.\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0632\t = Validation score   (r2)\n",
            "\t7.46s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 94.08s ... Best model: LightGBMLarge | Estimated inference throughput: 25104.3 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_143932/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_143932/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_143932/models/LightGBMXT will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20240702_143932/models/LightGBM will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_143932/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_143932/models/CatBoost will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20240702_143932/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_143932/models/NeuralNetFastAI will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20240702_143932/models/XGBoost will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_143932/models/NeuralNetTorch will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_143932\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 9\n",
            "\n",
            ": {'r2': 0.058844957992771896, 'root_mean_squared_error': -0.02802624178224852, 'mean_squared_error': -0.0007854702284370526, 'mean_absolute_error': -0.020259715278659853, 'pearsonr': 0.2841126931579655, 'median_absolute_error': -0.013968532127380362} \n",
            "\n",
            "\n",
            "           model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0  LightGBMLarge    0.058845   0.063201          r2        0.076765   \n",
            "\n",
            "   pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0       0.031867  7.464749                 0.076765                0.031867   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           7.464749            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "           model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  LightGBMLarge   0.063201          r2       0.031867  7.464749                0.031867           7.464749            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'LGBModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_143932SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'LightGBMLarge': 'LGBModel'}, 'model_performance': {'LightGBMLarge': 0.06320138350259297}, 'model_best': 'LightGBMLarge', 'model_paths': {'LightGBMLarge': ['LightGBMLarge']}, 'model_fit_times': {'LightGBMLarge': 7.464748859405518}, 'model_pred_times': {'LightGBMLarge': 0.031867027282714844}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'LightGBMLarge': {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5}}, 'leaderboard':            model  score_val eval_metric  pred_time_val  fit_time  \\\n",
            "0  LightGBMLarge   0.063201          r2       0.031867  7.464749   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.031867           7.464749            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_144109\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.76 GB / 12.67 GB (77.0%)\n",
            "Disk Space Avail:   46.44 GB / 78.19 GB (59.4%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_144109\"\n",
            "Train Data Rows:    7997\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9994.49 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 5): ['oz79', 'oz112', 'oz115', 'oz222', 'oz234']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 5 | ['oz79', 'oz112', 'oz115', 'oz222', 'oz234']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\t0.9s = Fit runtime\n",
            "\t57 features in original data used to generate 57 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.68 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.9s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7197, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 99.1s of the 99.09s of remaining time.\n",
            "\t0.0257\t = Validation score   (r2)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 98.99s of the 98.99s of remaining time.\n",
            "\t0.0241\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 98.92s of the 98.92s of remaining time.\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0476\t = Validation score   (r2)\n",
            "\t2.47s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 96.35s of the 96.35s of remaining time.\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0544\t = Validation score   (r2)\n",
            "\t2.87s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 93.36s of the 93.36s of remaining time.\n",
            "\t0.0428\t = Validation score   (r2)\n",
            "\t36.74s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 56.09s of the 56.08s of remaining time.\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "Default metric period is 5 because R2 is/are not implemented for GPU\n",
            "Metric R2 is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
            "\t0.041\t = Validation score   (r2)\n",
            "\t5.04s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 51.02s of the 51.02s of remaining time.\n",
            "\t0.0509\t = Validation score   (r2)\n",
            "\t14.78s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 35.7s of the 35.7s of remaining time.\n",
            "\t0.045\t = Validation score   (r2)\n",
            "\t9.13s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 26.53s of the 26.53s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:42:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [14:42:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\t0.0517\t = Validation score   (r2)\n",
            "\t1.39s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 25.13s of the 25.12s of remaining time.\n",
            "\t0.018\t = Validation score   (r2)\n",
            "\t12.56s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 12.51s of the 12.51s of remaining time.\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t0.0507\t = Validation score   (r2)\n",
            "\t3.23s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 90.86s ... Best model: LightGBM | Estimated inference throughput: 17615.1 rows/s (800 batch size)\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20240702_144109/models/KNeighborsUnif will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20240702_144109/models/KNeighborsDist will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20240702_144109/models/LightGBMXT will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20240702_144109/models/RandomForestMSE will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20240702_144109/models/CatBoost will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20240702_144109/models/ExtraTreesMSE will be removed.\n",
            "Deleting model NeuralNetFastAI. All files under AutogluonModels/ag-20240702_144109/models/NeuralNetFastAI will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20240702_144109/models/XGBoost will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20240702_144109/models/NeuralNetTorch will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20240702_144109/models/LightGBMLarge will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_144109\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 10\n",
            "\n",
            ": {'r2': 0.05030349481768315, 'root_mean_squared_error': -0.027903836615179478, 'mean_squared_error': -0.0007786240978466309, 'mean_absolute_error': -0.0198661476565782, 'pearsonr': 0.2549410453239828, 'median_absolute_error': -0.013330840948104827} \n",
            "\n",
            "\n",
            "      model  score_test  score_val eval_metric  pred_time_test  pred_time_val  \\\n",
            "0  LightGBM    0.050303   0.054396          r2        0.069686       0.045416   \n",
            "\n",
            "   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0  2.873367                 0.069686                0.045416   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           2.873367            1       True          1  \n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "      model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  LightGBM   0.054396          r2       0.045416  2.873367                0.045416           2.873367            1       True          1\n",
            "Number of models trained: 1\n",
            "Types of models trained:\n",
            "{'LGBModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "('int', ['bool']) : 15 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240702_144109SummaryOfModels.html\n",
            "*** End of fit() summary ***\n",
            "{'model_types': {'LightGBM': 'LGBModel'}, 'model_performance': {'LightGBM': 0.05439628706193245}, 'model_best': 'LightGBM', 'model_paths': {'LightGBM': ['LightGBM']}, 'model_fit_times': {'LightGBM': 2.8733673095703125}, 'model_pred_times': {'LightGBM': 0.045415639877319336}, 'num_bag_folds': 0, 'max_stack_level': 1, 'model_hyperparams': {'LightGBM': {'learning_rate': 0.05}}, 'leaderboard':       model  score_val eval_metric  pred_time_val  fit_time  \\\n",
            "0  LightGBM   0.054396          r2       0.045416  2.873367   \n",
            "\n",
            "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                0.045416           2.873367            1       True   \n",
            "\n",
            "   fit_order  \n",
            "0          1  }\n"
          ]
        }
      ],
      "source": [
        "## First training the model speratly on all 10 folds to see that it is consistent\n",
        "\n",
        "for fold_number in range(1, 11): # jsut the first to start with\n",
        "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed)\n",
        "    model = fit_gluon(train_dataset, time_limit=100, fit_weighted_ensemble=False, num_gpus=1, num_cpus=2, keep_only_best=True)\n",
        "    test_score, leaderboard, L2_diagram = evaluate_gluon(model, test_dataset)\n",
        "    print(f'Fold {fold_number}\\n\\n: {test_score} \\n\\n')\n",
        "    print(leaderboard)\n",
        "    print(model.fit_summary())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBt09mtJc-57",
        "outputId": "249f54ba-66db-4ebb-fa7f-8e76be1b97c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_093443\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.92 GB / 12.67 GB (78.3%)\n",
            "Disk Space Avail:   45.84 GB / 78.19 GB (58.6%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_093443\"\n",
            "Train Data Rows:    7996\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10163.55 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 6): ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', 'oz115']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 6 | ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
            "\t0.8s = Fit runtime\n",
            "\t56 features in original data used to generate 56 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.84s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 99.16s of the 99.16s of remaining time.\n",
            "\t-0.1737\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 99.09s of the 99.09s of remaining time.\n",
            "\t-0.1778\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 99.03s of the 99.03s of remaining time.\n",
            "\t0.0871\t = Validation score   (r2)\n",
            "\t1.84s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 97.13s of the 97.13s of remaining time.\n",
            "\t0.078\t = Validation score   (r2)\n",
            "\t1.33s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 95.77s of the 95.77s of remaining time.\n",
            "\t0.0786\t = Validation score   (r2)\n",
            "\t39.66s\t = Training   runtime\n",
            "\t0.23s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 55.01s of the 55.01s of remaining time.\n",
            "\t0.0897\t = Validation score   (r2)\n",
            "\t4.85s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 50.15s of the 50.15s of remaining time.\n",
            "\t0.0865\t = Validation score   (r2)\n",
            "\t15.01s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 34.62s of the 34.62s of remaining time.\n",
            "\t0.0536\t = Validation score   (r2)\n",
            "\t10.14s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 24.4s of the 24.4s of remaining time.\n",
            "\t0.0654\t = Validation score   (r2)\n",
            "\t2.44s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 21.93s of the 21.93s of remaining time.\n",
            "\t-0.2942\t = Validation score   (r2)\n",
            "\t9.61s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 12.26s of the 12.26s of remaining time.\n",
            "\t0.0878\t = Validation score   (r2)\n",
            "\t4.15s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 99.16s of the 8.05s of remaining time.\n",
            "\tEnsemble Weights: {'ExtraTreesMSE': 0.286, 'NeuralNetFastAI': 0.19, 'LightGBMLarge': 0.19, 'LightGBMXT': 0.143, 'RandomForestMSE': 0.143, 'CatBoost': 0.048}\n",
            "\t0.1103\t = Validation score   (r2)\n",
            "\t0.22s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 92.23s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1950.9 rows/s (800 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_093443\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: {'r2': 0.08900780402347996, 'root_mean_squared_error': -0.025424691438009624, 'mean_squared_error': -0.0006464149347179998, 'mean_absolute_error': -0.018338991526840775, 'pearsonr': 0.3020016838586487, 'median_absolute_error': -0.013147535247802766}\n",
            "                  model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0   WeightedEnsemble_L2    0.089008   0.110349          r2        0.655178   \n",
            "1              CatBoost    0.089001   0.089654          r2        0.009564   \n",
            "2         LightGBMLarge    0.079066   0.087833          r2        0.011603   \n",
            "3               XGBoost    0.077817   0.065368          r2        0.016926   \n",
            "4            LightGBMXT    0.077281   0.087110          r2        0.021298   \n",
            "5              LightGBM    0.072527   0.078010          r2        0.008025   \n",
            "6         ExtraTreesMSE    0.062203   0.086513          r2        0.251512   \n",
            "7       RandomForestMSE    0.048234   0.078561          r2        0.310097   \n",
            "8       NeuralNetFastAI    0.008027   0.053579          r2        0.046535   \n",
            "9        KNeighborsUnif   -0.119236  -0.173739          r2        0.047116   \n",
            "10       KNeighborsDist   -0.192835  -0.177775          r2        0.040982   \n",
            "11       NeuralNetTorch   -0.305328  -0.294174          r2        0.040774   \n",
            "\n",
            "    pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0        0.410075  75.862564                 0.004568                0.000766   \n",
            "1        0.005538   4.848163                 0.009564                0.005538   \n",
            "2        0.007546   4.146707                 0.011603                0.007546   \n",
            "3        0.006859   2.444231                 0.016926                0.006859   \n",
            "4        0.013212   1.835371                 0.021298                0.013212   \n",
            "5        0.005351   1.332516                 0.008025                0.005351   \n",
            "6        0.126851  15.012934                 0.251512                0.126851   \n",
            "7        0.225204  39.660418                 0.310097                0.225204   \n",
            "8        0.030959  10.143753                 0.046535                0.030959   \n",
            "9        0.044363   0.015936                 0.047116                0.044363   \n",
            "10       0.039468   0.014273                 0.040982                0.039468   \n",
            "11       0.050454   9.612742                 0.040774                0.050454   \n",
            "\n",
            "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0            0.215217            2       True         12  \n",
            "1            4.848163            1       True          6  \n",
            "2            4.146707            1       True         11  \n",
            "3            2.444231            1       True          9  \n",
            "4            1.835371            1       True          3  \n",
            "5            1.332516            1       True          4  \n",
            "6           15.012934            1       True          7  \n",
            "7           39.660418            1       True          5  \n",
            "8           10.143753            1       True          8  \n",
            "9            0.015936            1       True          1  \n",
            "10           0.014273            1       True          2  \n",
            "11           9.612742            1       True         10  \n"
          ]
        }
      ],
      "source": [
        "for fold_number in range(1, 2):\n",
        "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed)\n",
        "    model = fit_gluon(train_dataset, time_limit=100)\n",
        "    test_score, leaderboard = evaluate_gluon(model, test_dataset)\n",
        "    print(f'Fold {fold_number}: {test_score}')\n",
        "    print(leaderboard)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "93vNiSqRc-58",
        "outputId": "abf7c73d-1365-43a8-d1fb-b4da2a70d5bd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_fold' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d7833e3bc953>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfull_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfull_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_fold' is not defined"
          ]
        }
      ],
      "source": [
        "## Making sets of all 10 folds\n",
        "\n",
        "full_train = None\n",
        "full_test = None\n",
        "\n",
        "for fold_number in range(1, 11):\n",
        "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed)\n",
        "    if full_train is None:\n",
        "        full_train = train_dataset\n",
        "        full_test = test_dataset\n",
        "    else:\n",
        "        # Use pd.concat to combine TabularDatasets\n",
        "        full_train = pd.concat([full_train, train_dataset])\n",
        "        full_test = pd.concat([full_test, test_dataset])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kroZsfJbc-59"
      },
      "outputs": [],
      "source": [
        "## Creating ensemble of the trained AutoGluon models on each fold\n",
        "\n",
        "predictor1 = TabularPredictor.load('AutoGluonModels/...')\n",
        "predictor2 = TabularPredictor.load('path/to/second/model')\n",
        "predictor3 = TabularPredictor.load('path/to/third/model')\n",
        "predictor4 = TabularPredictor.load('path/to/fourth/model')\n",
        "predictor5 = TabularPredictor.load('path/to/fifth/model')\n",
        "predictor6 = TabularPredictor.load('path/to/sixth/model')\n",
        "predictor7 = TabularPredictor.load('path/to/seventh/model')\n",
        "predictor8 = TabularPredictor.load('path/to/eighth/model')\n",
        "predictor9 = TabularPredictor.load('path/to/ninth/model')\n",
        "predictor10 = TabularPredictor.load('path/to/tenth/model')\n",
        "\n",
        "preds1 = predictor1.predict(full_test)\n",
        "preds2 = predictor2.predict(full_test)\n",
        "preds3 = predictor3.predict(full_test)\n",
        "preds4 = predictor4.predict(full_test)\n",
        "preds5 = predictor5.predict(full_test)\n",
        "preds6 = predictor6.predict(full_test)\n",
        "preds7 = predictor7.predict(full_test)\n",
        "preds8 = predictor8.predict(full_test)\n",
        "preds9 = predictor9.predict(full_test)\n",
        "preds10 = predictor10.predict(full_test)\n",
        "\n",
        "# Simple averaging ensemble\n",
        "10_fold_ensemble = (preds1 + preds2 + preds3 + preds4 + preds5 + preds6 + preds7\n",
        "                    + preds8 + preds9 + preds10) / 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Display the feature importance on the whole dataset trained for a longer\n",
        "## period so we interpret the dataset more correctly\n",
        "\n",
        "model.feature_importance(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "LJ6ZM33xsU1u",
        "outputId": "aa6dafb0-27c2-4238-d208-3d1b784a1e88"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "These features in provided data are not utilized by the predictor and will be ignored: ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', 'oz115']\n",
            "Computing feature importance via permutation shuffling for 56 features using 888 rows with 5 shuffle sets...\n",
            "\t372.15s\t= Expected runtime (74.43s per shuffle set)\n",
            "\t44.29s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       importance    stddev       p_value  n  p99_high   p99_low\n",
              "oz251    0.215600  0.014740  2.605287e-06  5  0.245950  0.185251\n",
              "oz249    0.151272  0.006864  5.072224e-07  5  0.165405  0.137140\n",
              "oz151    0.117863  0.006892  1.396675e-06  5  0.132054  0.103672\n",
              "oz246    0.102660  0.002641  5.251960e-08  5  0.108098  0.097222\n",
              "oz247    0.096256  0.006216  2.075437e-06  5  0.109055  0.083457\n",
              "oz13     0.084232  0.004374  8.690227e-07  5  0.093238  0.075227\n",
              "oz250    0.081146  0.004392  1.026169e-06  5  0.090190  0.072102\n",
              "oz248    0.069838  0.001882  6.316304e-08  5  0.073712  0.065963\n",
              "oz175    0.069092  0.003302  6.237928e-07  5  0.075890  0.062294\n",
              "oz150    0.061265  0.004218  2.680104e-06  5  0.069951  0.052579\n",
              "oz125    0.057687  0.005007  6.742344e-06  5  0.067996  0.047378\n",
              "oz3      0.051098  0.002003  2.828814e-07  5  0.055223  0.046973\n",
              "oz124    0.048523  0.002653  1.067399e-06  5  0.053985  0.043062\n",
              "oz183    0.044821  0.001177  5.692769e-08  5  0.047244  0.042398\n",
              "oz149    0.043490  0.002905  2.375076e-06  5  0.049471  0.037508\n",
              "oz172    0.042754  0.002868  2.415690e-06  5  0.048659  0.036849\n",
              "oz165    0.042494  0.004623  1.654058e-05  5  0.052012  0.032976\n",
              "oz12     0.037574  0.001374  2.139291e-07  5  0.040402  0.034746\n",
              "oz5      0.033078  0.002984  7.866451e-06  5  0.039224  0.026933\n",
              "oz4      0.032903  0.001961  1.507264e-06  5  0.036941  0.028865\n",
              "oz1      0.032837  0.001528  5.610529e-07  5  0.035984  0.029691\n",
              "oz171    0.030378  0.001500  7.106469e-07  5  0.033466  0.027290\n",
              "oz176    0.028000  0.001538  1.087360e-06  5  0.031166  0.024833\n",
              "oz197    0.026069  0.001477  1.230490e-06  5  0.029110  0.023028\n",
              "oz177    0.023733  0.001706  3.181206e-06  5  0.027245  0.020220\n",
              "oz127    0.023245  0.000515  2.894701e-08  5  0.024306  0.022184\n",
              "oz6      0.022917  0.000573  4.694663e-08  5  0.024097  0.021737\n",
              "oz126    0.022333  0.001042  5.670859e-07  5  0.024478  0.020187\n",
              "oz131    0.022045  0.001002  5.101462e-07  5  0.024108  0.019983\n",
              "oz128    0.020824  0.001962  9.351807e-06  5  0.024865  0.016784\n",
              "oz10     0.020284  0.001455  3.151173e-06  5  0.023279  0.017289\n",
              "oz185    0.019685  0.001771  7.776118e-06  5  0.023331  0.016039\n",
              "oz9      0.017086  0.001100  2.051568e-06  5  0.019352  0.014821\n",
              "oz133    0.015005  0.001824  2.571430e-05  5  0.018762  0.011249\n",
              "oz11     0.014792  0.000882  1.510784e-06  5  0.016608  0.012975\n",
              "oz173    0.012770  0.000529  3.538008e-07  5  0.013861  0.011680\n",
              "oz2      0.012520  0.001234  1.118505e-05  5  0.015061  0.009979\n",
              "oz31     0.010701  0.001460  4.057554e-05  5  0.013708  0.007695\n",
              "oz178    0.007664  0.000465  1.611096e-06  5  0.008621  0.006708\n",
              "oz181    0.006454  0.000965  5.831186e-05  5  0.008442  0.004467\n",
              "oz87     0.006312  0.000266  3.802878e-07  5  0.006861  0.005763\n",
              "oz83     0.004763  0.001080  2.964808e-04  5  0.006986  0.002539\n",
              "oz234    0.000797  0.000151  1.469081e-04  5  0.001108  0.000487\n",
              "oz79     0.000679  0.000679  4.457621e-02  5  0.002077 -0.000720\n",
              "oz96     0.000326  0.000092  6.806423e-04  5  0.000515  0.000137\n",
              "oz135    0.000186  0.000085  4.081597e-03  5  0.000361  0.000010\n",
              "oz71     0.000153  0.000437  2.389816e-01  5  0.001052 -0.000747\n",
              "oz206    0.000004  0.000184  4.809513e-01  5  0.000384 -0.000375\n",
              "oz113    0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz112    0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz108    0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz73     0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz42     0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz40     0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz46     0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz222    0.000000  0.000000  5.000000e-01  5  0.000000  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-041948df-158f-444a-98d8-0694ab7cb3e1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "      <th>stddev</th>\n",
              "      <th>p_value</th>\n",
              "      <th>n</th>\n",
              "      <th>p99_high</th>\n",
              "      <th>p99_low</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>oz251</th>\n",
              "      <td>0.215600</td>\n",
              "      <td>0.014740</td>\n",
              "      <td>2.605287e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.245950</td>\n",
              "      <td>0.185251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz249</th>\n",
              "      <td>0.151272</td>\n",
              "      <td>0.006864</td>\n",
              "      <td>5.072224e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.165405</td>\n",
              "      <td>0.137140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz151</th>\n",
              "      <td>0.117863</td>\n",
              "      <td>0.006892</td>\n",
              "      <td>1.396675e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.132054</td>\n",
              "      <td>0.103672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz246</th>\n",
              "      <td>0.102660</td>\n",
              "      <td>0.002641</td>\n",
              "      <td>5.251960e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.108098</td>\n",
              "      <td>0.097222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz247</th>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.006216</td>\n",
              "      <td>2.075437e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.109055</td>\n",
              "      <td>0.083457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz13</th>\n",
              "      <td>0.084232</td>\n",
              "      <td>0.004374</td>\n",
              "      <td>8.690227e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.093238</td>\n",
              "      <td>0.075227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz250</th>\n",
              "      <td>0.081146</td>\n",
              "      <td>0.004392</td>\n",
              "      <td>1.026169e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.090190</td>\n",
              "      <td>0.072102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz248</th>\n",
              "      <td>0.069838</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>6.316304e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.073712</td>\n",
              "      <td>0.065963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz175</th>\n",
              "      <td>0.069092</td>\n",
              "      <td>0.003302</td>\n",
              "      <td>6.237928e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.075890</td>\n",
              "      <td>0.062294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz150</th>\n",
              "      <td>0.061265</td>\n",
              "      <td>0.004218</td>\n",
              "      <td>2.680104e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.069951</td>\n",
              "      <td>0.052579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz125</th>\n",
              "      <td>0.057687</td>\n",
              "      <td>0.005007</td>\n",
              "      <td>6.742344e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.067996</td>\n",
              "      <td>0.047378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz3</th>\n",
              "      <td>0.051098</td>\n",
              "      <td>0.002003</td>\n",
              "      <td>2.828814e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.055223</td>\n",
              "      <td>0.046973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz124</th>\n",
              "      <td>0.048523</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>1.067399e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.053985</td>\n",
              "      <td>0.043062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz183</th>\n",
              "      <td>0.044821</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>5.692769e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.047244</td>\n",
              "      <td>0.042398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz149</th>\n",
              "      <td>0.043490</td>\n",
              "      <td>0.002905</td>\n",
              "      <td>2.375076e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.049471</td>\n",
              "      <td>0.037508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz172</th>\n",
              "      <td>0.042754</td>\n",
              "      <td>0.002868</td>\n",
              "      <td>2.415690e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.048659</td>\n",
              "      <td>0.036849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz165</th>\n",
              "      <td>0.042494</td>\n",
              "      <td>0.004623</td>\n",
              "      <td>1.654058e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.052012</td>\n",
              "      <td>0.032976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz12</th>\n",
              "      <td>0.037574</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>2.139291e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.040402</td>\n",
              "      <td>0.034746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz5</th>\n",
              "      <td>0.033078</td>\n",
              "      <td>0.002984</td>\n",
              "      <td>7.866451e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.039224</td>\n",
              "      <td>0.026933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz4</th>\n",
              "      <td>0.032903</td>\n",
              "      <td>0.001961</td>\n",
              "      <td>1.507264e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.036941</td>\n",
              "      <td>0.028865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz1</th>\n",
              "      <td>0.032837</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>5.610529e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.035984</td>\n",
              "      <td>0.029691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz171</th>\n",
              "      <td>0.030378</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>7.106469e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.033466</td>\n",
              "      <td>0.027290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz176</th>\n",
              "      <td>0.028000</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>1.087360e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.031166</td>\n",
              "      <td>0.024833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz197</th>\n",
              "      <td>0.026069</td>\n",
              "      <td>0.001477</td>\n",
              "      <td>1.230490e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.029110</td>\n",
              "      <td>0.023028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz177</th>\n",
              "      <td>0.023733</td>\n",
              "      <td>0.001706</td>\n",
              "      <td>3.181206e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.027245</td>\n",
              "      <td>0.020220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz127</th>\n",
              "      <td>0.023245</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>2.894701e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024306</td>\n",
              "      <td>0.022184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz6</th>\n",
              "      <td>0.022917</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>4.694663e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024097</td>\n",
              "      <td>0.021737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz126</th>\n",
              "      <td>0.022333</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>5.670859e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024478</td>\n",
              "      <td>0.020187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz131</th>\n",
              "      <td>0.022045</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>5.101462e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024108</td>\n",
              "      <td>0.019983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz128</th>\n",
              "      <td>0.020824</td>\n",
              "      <td>0.001962</td>\n",
              "      <td>9.351807e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024865</td>\n",
              "      <td>0.016784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz10</th>\n",
              "      <td>0.020284</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>3.151173e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.023279</td>\n",
              "      <td>0.017289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz185</th>\n",
              "      <td>0.019685</td>\n",
              "      <td>0.001771</td>\n",
              "      <td>7.776118e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.023331</td>\n",
              "      <td>0.016039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz9</th>\n",
              "      <td>0.017086</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>2.051568e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.019352</td>\n",
              "      <td>0.014821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz133</th>\n",
              "      <td>0.015005</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>2.571430e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.018762</td>\n",
              "      <td>0.011249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz11</th>\n",
              "      <td>0.014792</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>1.510784e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.016608</td>\n",
              "      <td>0.012975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz173</th>\n",
              "      <td>0.012770</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>3.538008e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.013861</td>\n",
              "      <td>0.011680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz2</th>\n",
              "      <td>0.012520</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>1.118505e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.015061</td>\n",
              "      <td>0.009979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz31</th>\n",
              "      <td>0.010701</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>4.057554e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.013708</td>\n",
              "      <td>0.007695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz178</th>\n",
              "      <td>0.007664</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>1.611096e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.008621</td>\n",
              "      <td>0.006708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz181</th>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.000965</td>\n",
              "      <td>5.831186e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.008442</td>\n",
              "      <td>0.004467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz87</th>\n",
              "      <td>0.006312</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>3.802878e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.006861</td>\n",
              "      <td>0.005763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz83</th>\n",
              "      <td>0.004763</td>\n",
              "      <td>0.001080</td>\n",
              "      <td>2.964808e-04</td>\n",
              "      <td>5</td>\n",
              "      <td>0.006986</td>\n",
              "      <td>0.002539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz234</th>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>1.469081e-04</td>\n",
              "      <td>5</td>\n",
              "      <td>0.001108</td>\n",
              "      <td>0.000487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz79</th>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>4.457621e-02</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002077</td>\n",
              "      <td>-0.000720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz96</th>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>6.806423e-04</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.000137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz135</th>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>4.081597e-03</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz71</th>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>2.389816e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.001052</td>\n",
              "      <td>-0.000747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz206</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>4.809513e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000384</td>\n",
              "      <td>-0.000375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz113</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz112</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz108</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz42</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz40</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz46</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz222</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-041948df-158f-444a-98d8-0694ab7cb3e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-041948df-158f-444a-98d8-0694ab7cb3e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-041948df-158f-444a-98d8-0694ab7cb3e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-56fa2c50-5342-4a5d-9c9c-9444f1cb0cfd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-56fa2c50-5342-4a5d-9c9c-9444f1cb0cfd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-56fa2c50-5342-4a5d-9c9c-9444f1cb0cfd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"model\",\n  \"rows\": 56,\n  \"fields\": [\n    {\n      \"column\": \"importance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.041489531815625945,\n        \"min\": 0.0,\n        \"max\": 0.2156004769030802,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.044820989343228625,\n          0.00018577897714791013,\n          4.189857937464403e-06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stddev\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002472255254947716,\n        \"min\": 0.0,\n        \"max\": 0.01473987211071361,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.0011765672200878216,\n          8.513356626801783e-05,\n          0.0001843387783256428\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18508670257496138,\n        \"min\": 2.894700773908691e-08,\n        \"max\": 0.5,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          5.692768928615341e-08,\n          0.004081596718087056,\n          0.48095130413361187\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p99_high\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04626988176072916,\n        \"min\": 0.0,\n        \"max\": 0.2459500773057223,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.047243557511276596\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p99_low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03679245906880237,\n        \"min\": -0.0007468321459798294,\n        \"max\": 0.18525087650043812,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.04239842117518065\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Training the model on all 10 folds\n",
        "\n",
        "model = fit_gluon(full_train, time_limit=100)\n",
        "test_score, leaderboard = evaluate_gluon(model, full_test)\n",
        "print(f'Full Train: {test_score}')\n",
        "display(Image(filename=model.plot_ensemble_model()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f6AT7fM8Q7E",
        "outputId": "20123959-4591-43b0-a183-3ea511364623"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_102619\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.06 GB / 12.67 GB (79.4%)\n",
            "Disk Space Avail:   45.45 GB / 78.19 GB (58.1%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_102619\"\n",
            "Train Data Rows:    79965\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10330.72 MB\n",
            "\tTrain Data (Original)  Memory Usage: 27.15 MB (0.3% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 11): ['oz42', 'oz46', 'oz50', 'oz100', 'oz108', 'oz111', 'oz112', 'oz113', 'oz115', 'oz222', 'oz234']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 11 | ['oz42', 'oz46', 'oz50', 'oz100', 'oz108', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  9 | ['oz40', 'oz69', 'oz71', 'oz73', 'oz79', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) :  9 | ['oz40', 'oz69', 'oz71', 'oz73', 'oz79', ...]\n",
            "\t1.6s = Fit runtime\n",
            "\t51 features in original data used to generate 51 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 26.31 MB (0.3% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.66s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.03126367785906334, Train Rows: 77465, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 98.34s of the 98.34s of remaining time.\n",
            "\t0.9947\t = Validation score   (r2)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t1.22s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 97.01s of the 97.01s of remaining time.\n",
            "\t0.9947\t = Validation score   (r2)\n",
            "\t0.1s\t = Training   runtime\n",
            "\t1.22s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 95.66s of the 95.66s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's l2: 0.000349576\tvalid_set's r2: 0.560283\n",
            "[2000]\tvalid_set's l2: 0.000223508\tvalid_set's r2: 0.718859\n",
            "[3000]\tvalid_set's l2: 0.00014857\tvalid_set's r2: 0.81312\n",
            "[4000]\tvalid_set's l2: 0.000104798\tvalid_set's r2: 0.868179\n",
            "[5000]\tvalid_set's l2: 7.54596e-05\tvalid_set's r2: 0.905082\n",
            "[6000]\tvalid_set's l2: 5.56457e-05\tvalid_set's r2: 0.930006\n",
            "[7000]\tvalid_set's l2: 4.17541e-05\tvalid_set's r2: 0.947479\n",
            "[8000]\tvalid_set's l2: 3.21521e-05\tvalid_set's r2: 0.959557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tRan out of time, early stopping on iteration 8280. Best iteration is:\n",
            "\t[8280]\tvalid_set's l2: 2.9896e-05\tvalid_set's r2: 0.962395\n",
            "\t0.9624\t = Validation score   (r2)\n",
            "\t97.06s\t = Training   runtime\n",
            "\t1.33s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 98.34s of the -3.91s of remaining time.\n",
            "\tEnsemble Weights: {'KNeighborsUnif': 1.0}\n",
            "\t0.9947\t = Validation score   (r2)\n",
            "\t0.05s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 104.04s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2053.2 rows/s (2500 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_102619\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Train: {'r2': 0.9903062537074445, 'root_mean_squared_error': -0.0029292002708376065, 'mean_squared_error': -8.580214226675108e-06, 'mean_absolute_error': -0.0002855074196261087, 'pearsonr': 0.9951420695870777, 'median_absolute_error': -1.546478267666629e-08}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Training the model again without the least important columns\n",
        "\n",
        "columns_to_drop = ['oz173', 'oz1', 'oz4', 'oz181', 'oz11', 'oz31', 'oz178', 'oz83', 'oz87', 'oz234', 'oz96', 'oz135', 'oz206', 'oz40', 'oz73', 'oz42', 'oz112', 'oz113', 'oz71', 'oz222', 'oz108', 'oz46' ]\n",
        "\n",
        "full_train_drop = full_train.drop(columns=columns_to_drop)\n",
        "full_test_drop = full_test.drop(columns=columns_to_drop)\n",
        "\n",
        "model = fit_gluon(full_train_drop, time_limit=100)\n",
        "test_score, leaderboard = evaluate_gluon(model, full_test_drop)\n",
        "print(f'Full Train: {test_score}')\n",
        "display(Image(filename=model.plot_ensemble_model()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "XntN0zoN-4kC",
        "outputId": "d7a5b102-029b-4c63-a4ea-f0619360fa96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'full_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6e5efe152274>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'oz173'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz181'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz11'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz31'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz178'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz83'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz87'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz234'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz96'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz135'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz206'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz40'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz73'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz42'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz112'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz113'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz71'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz222'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz108'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz46'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfull_train_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfull_test_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'full_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing to see if TabPFN works well\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tabpfn import TabPFNClassifier\n",
        "\n"
      ],
      "metadata": {
        "id": "S9V5pNpp-MiG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqrx-Gv5c-58"
      },
      "outputs": [],
      "source": [
        "## Creating an ensemble of the 10 models??"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
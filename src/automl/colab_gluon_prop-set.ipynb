{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht4hg37xc-51"
      },
      "source": [
        "### This is a script to to load the property dataset and run **AutoGluon** on it. First basic model run, then different configurations are tried. Then look at possible **third level ensembling** of the models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### installments\n",
        "\n",
        "!pip install autogluon\n",
        "!sudo apt-get install graphviz graphviz-dev\n",
        "!pip install pygraphviz\n",
        "!pip install tabpfn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vWxQZEEH2uHi",
        "outputId": "97867842-4242-4171-bbba-ab92979f522d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon\n",
            "  Downloading autogluon-1.1.1-py3-none-any.whl (9.7 kB)\n",
            "Collecting autogluon.core[all]==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.features==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.tabular[all]==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.multimodal==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.multimodal-1.1.1-py3-none-any.whl (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.0/428.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.timeseries[all]==1.1.1 (from autogluon)\n",
            "  Downloading autogluon.timeseries-1.1.1-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.29,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (1.25.2)\n",
            "Requirement already satisfied: scipy<1.13,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (1.11.4)\n",
            "Collecting scikit-learn<1.4.1,>=1.3.0 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (3.3)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (3.7.1)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading boto3-1.34.137-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.common==1.1.1 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ray[default,tune]<2.11,>=2.10.0 (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1->autogluon) (0.2.7)\n",
            "Collecting Pillow<11,>=10.0.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.3.0+cu121)\n",
            "Collecting lightning<2.4,>=2.2 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading lightning-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers[sentencepiece]<4.41.0,>=4.38.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate<0.22.0,>=0.21.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema<4.22,>=4.18 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (4.19.2)\n",
            "Collecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm<0.10.0,>=0.9.5 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision<0.19.0,>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.18.0+cu121)\n",
            "Requirement already satisfied: scikit-image<0.21.0,>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.19.3)\n",
            "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (1.3)\n",
            "Collecting torchmetrics<1.3.0,>=1.2.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nptyping<2.5.0,>=1.4.4 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
            "Collecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-metric-learning<2.4,>=1.3.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk<4.0.0,>=3.4.5 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.8.1)\n",
            "Collecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (3.1.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.10/dist-packages (from autogluon.multimodal==1.1.1->autogluon) (2.15.2)\n",
            "Collecting pytesseract<0.3.11,>=0.3.9 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2image<1.19,>=1.17.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: xgboost<2.1,>=1.6 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.0.3)\n",
            "Requirement already satisfied: fastai<2.8,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.7.15)\n",
            "Requirement already satisfied: lightgbm<4.4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==1.1.1->autogluon) (4.1.0)\n",
            "Collecting catboost<1.3,>=1.1 (from autogluon.tabular[all]==1.1.1->autogluon)\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib<2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.timeseries[all]==1.1.1->autogluon) (1.4.2)\n",
            "Collecting pytorch-lightning<2.4,>=2.2 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading pytorch_lightning-2.3.1-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonts==0.15.1 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading gluonts-0.15.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting statsforecast<1.5,>=1.4.0 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading statsforecast-1.4.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlforecast<0.10.1,>=0.10.0 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading mlforecast-0.10.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting utilsforecast<0.0.11,>=0.0.10 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading utilsforecast-0.0.10-py3-none-any.whl (30 kB)\n",
            "Collecting orjson~=3.9 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optimum[onnxruntime]<1.19,>=1.17 (from autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading optimum-1.18.1-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.0/410.0 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core[all]==1.1.1->autogluon) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core[all]==1.1.1->autogluon) (67.7.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.7.4)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (6.0.1)\n",
            "Collecting botocore<1.35.0,>=1.34.137 (from boto3<2,>=1.10->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading botocore-1.34.137-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2,>=1.10->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (1.16.0)\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.23.4)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (23.1.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.48)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.7.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.10.9.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.1.1->autogluon) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.18.1)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries[all]==1.1.1->autogluon) (0.58.1)\n",
            "Collecting window-ops (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (5.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (2024.5.15)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Collecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (13.7.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.9.0)\n",
            "Collecting coloredlogs (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon) (1.12.1)\n",
            "Collecting transformers[sentencepiece]<4.41.0,>=4.38.0 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.11.0 (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core[all]==1.1.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core[all]==1.1.1->autogluon) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core[all]==1.1.1->autogluon) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (3.15.4)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.0.8)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.4.1)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (3.9.5)\n",
            "Collecting aiohttp-cors (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-spy>=0.2.0 (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencensus (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (0.20.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (7.0.4)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading virtualenv-20.26.3-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.64.1)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (14.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (2024.6.2)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2024.6.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (1.6.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.core[all]==1.1.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from statsforecast<1.5,>=1.4.0->autogluon.timeseries[all]==1.1.1->autogluon) (0.14.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.0.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm<0.10.0,>=0.9.5->autogluon.multimodal==1.1.1->autogluon) (0.4.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.1.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core[all]==1.1.1->autogluon) (3.1.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (4.0.3)\n",
            "Collecting pyarrow>=6.0.1 (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.6)\n",
            "Collecting requests (from autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (4.12.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.3.1)\n",
            "Collecting Pillow<11,>=10.0.1 (from autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading Pillow-10.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->mlforecast<0.10.1,>=0.10.0->autogluon.timeseries[all]==1.1.1->autogluon) (0.41.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon) (24.3.25)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.18.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.12.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.4.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast<1.5,>=1.4.0->autogluon.timeseries[all]==1.1.1->autogluon) (0.5.6)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (4.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (2.11.1)\n",
            "Collecting pycryptodome (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openxlab-0.1.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.6/308.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (8.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.14.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum[onnxruntime]<1.19,>=1.17->autogluon.timeseries[all]==1.1.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon) (1.63.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.18.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (2.5)\n",
            "Collecting filelock (from ray[default,tune]<2.11,>=2.10.0->autogluon.core[all]==1.1.1->autogluon)\n",
            "  Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n",
            "  Downloading openxlab-0.1.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.4/307.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.38-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.37-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.36-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.5/302.5 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.35-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.5/302.5 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.34-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.33-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.5/299.5 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading openxlab-0.0.32-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.9/298.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.31-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.9/298.9 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.30-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.5/298.5 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.29-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.28-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading openxlab-0.0.27-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.8/296.8 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.26-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.9/295.9 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.25-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.1/295.1 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.24-py3-none-any.whl (291 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.2/291.2 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.23-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.22-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.21-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.20-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.19-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.18-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.17-py3-none-any.whl (291 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.6/291.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.16-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.15-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.2/289.2 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.14-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.13-py3-none-any.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.0/282.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.12-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openxlab-0.0.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of requests[socks] to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core[all]==1.1.1->autogluon) (1.7.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\n",
            "Building wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19172 sha256=373b921ee9b122feafad955c7181820631d8ee9181c1697174443a399fb10efa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=60cb0b039895440789bdf6114708926f373be9a88284d44f7985353a58504c76\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=8f9cb94f5f95c1a2b996b5937c0b017d9801a79427a9543832dd317ec125c303\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py3, distlib, colorful, antlr4-python3-runtime, xxhash, virtualenv, tensorboardX, requests, pycryptodome, pyarrow, Pillow, orjson, ordered-set, openxlab, onnx, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nptyping, lightning-utilities, jmespath, humanfriendly, dill, colorama, window-ops, scikit-learn, pytesseract, pdf2image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, model-index, coloredlogs, botocore, utilsforecast, tokenizers, seqeval, s3transfer, opendatalab, onnxruntime, nvidia-cusolver-cu12, gluonts, catboost, aiohttp-cors, transformers, statsforecast, ray, openmim, opencensus, nlpaug, mlforecast, datasets, boto3, torchmetrics, pytorch-metric-learning, evaluate, autogluon.common, accelerate, timm, pytorch-lightning, optimum, autogluon.features, autogluon.core, lightning, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.0.1 accelerate-0.21.0 aiohttp-cors-0.7.0 antlr4-python3-runtime-4.9.3 autogluon-1.1.1 autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.multimodal-1.1.1 autogluon.tabular-1.1.1 autogluon.timeseries-1.1.1 boto3-1.34.137 botocore-1.34.137 catboost-1.2.5 colorama-0.4.6 coloredlogs-15.0.1 colorful-0.5.6 datasets-2.20.0 dill-0.3.8 distlib-0.3.8 evaluate-0.4.2 gluonts-0.15.1 humanfriendly-10.0 jmespath-1.0.1 lightning-2.3.1 lightning-utilities-0.11.3.post0 mlforecast-0.10.0 model-index-0.1.11 multiprocess-0.70.16 nlpaug-1.1.11 nptyping-2.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py3-7.352.0 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.2.3 onnx-1.16.1 onnxruntime-1.18.1 opencensus-0.11.4 opencensus-context-0.1.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optimum-1.18.1 ordered-set-4.1.0 orjson-3.10.5 pdf2image-1.17.0 py-spy-0.3.14 pyarrow-16.1.0 pycryptodome-3.20.0 pytesseract-0.3.10 pytorch-lightning-2.3.1 pytorch-metric-learning-2.3.0 ray-2.10.0 requests-2.32.3 s3transfer-0.10.2 scikit-learn-1.4.0 seqeval-1.2.2 statsforecast-1.4.0 tensorboardX-2.6.2.2 timm-0.9.16 tokenizers-0.15.2 torchmetrics-1.2.1 transformers-4.39.3 utilsforecast-0.0.10 virtualenv-20.26.3 window-ops-0.0.15 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pydevd_plugins"
                ]
              },
              "id": "e7a2cc7f24fe4266a0c42a64362896cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libgraphviz-dev' instead of 'graphviz-dev'\n",
            "graphviz is already the newest version (2.42.2-6).\n",
            "The following additional packages will be installed:\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libgvc6-plugins-gtk librsvg2-common libxdot4\n",
            "Suggested packages:\n",
            "  gvfs\n",
            "The following NEW packages will be installed:\n",
            "  libgail-common libgail18 libgraphviz-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libgvc6-plugins-gtk librsvg2-common libxdot4\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 2,433 kB of archives.\n",
            "After this operation, 7,694 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2 [125 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2 [2,037 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgail18 amd64 2.24.33-2ubuntu2 [15.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgail-common amd64 2.24.33-2ubuntu2 [132 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libxdot4 amd64 2.42.2-6 [16.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgvc6-plugins-gtk amd64 2.42.2-6 [22.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgraphviz-dev amd64 2.42.2-6 [58.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2 [7,932 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Fetched 2,433 kB in 0s (18.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgtk2.0-common_2.24.33-2ubuntu2_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../1-libgtk2.0-0_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../2-libgail18_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../3-libgail-common_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package libxdot4:amd64.\n",
            "Preparing to unpack .../4-libxdot4_2.42.2-6_amd64.deb ...\n",
            "Unpacking libxdot4:amd64 (2.42.2-6) ...\n",
            "Selecting previously unselected package libgvc6-plugins-gtk.\n",
            "Preparing to unpack .../5-libgvc6-plugins-gtk_2.42.2-6_amd64.deb ...\n",
            "Unpacking libgvc6-plugins-gtk (2.42.2-6) ...\n",
            "Selecting previously unselected package libgraphviz-dev:amd64.\n",
            "Preparing to unpack .../6-libgraphviz-dev_2.42.2-6_amd64.deb ...\n",
            "Unpacking libgraphviz-dev:amd64 (2.42.2-6) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../7-libgtk2.0-bin_2.24.33-2ubuntu2_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../8-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libxdot4:amd64 (2.42.2-6) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2) ...\n",
            "Setting up libgvc6-plugins-gtk (2.42.2-6) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2) ...\n",
            "Setting up libgraphviz-dev:amd64 (2.42.2-6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "0Vb0n1Byc-53",
        "outputId": "be88e977-f91b-4ab4-f969-f003deefc7f1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'autogluon'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-974358aaf144>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautogluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabular\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTabularDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTabularPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autogluon'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j1XO9F5dc-55"
      },
      "outputs": [],
      "source": [
        "random_seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/My Drive/361092'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzT9Zhk1ebZZ",
        "outputId": "4a778af7-112a-4ca2-ffab-da102ed160c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0hLSX-NEc-55"
      },
      "outputs": [],
      "source": [
        "## Function for loading one of the 10 folds of the property dataset and concatinating the X and y values for train and test respectively.\n",
        "import pandas as pd\n",
        "\n",
        "def load_fold(fold_number, random_seed=42, sample_size=None):\n",
        "    df_X_train = pd.read_parquet(f'{base_path}/{fold_number}/X_train.parquet')\n",
        "    df_y_train = pd.read_parquet(f'{base_path}/{fold_number}/y_train.parquet')\n",
        "    df_X_test = pd.read_parquet(f'{base_path}/{fold_number}/X_test.parquet')\n",
        "    df_y_test = pd.read_parquet(f'{base_path}/{fold_number}/y_test.parquet')\n",
        "\n",
        "    # concatinating the X and y values for train and test respectively\n",
        "    df_train = pd.concat([df_X_train, df_y_train], axis=1)\n",
        "    df_test = pd.concat([df_X_test, df_y_test], axis=1)\n",
        "\n",
        "    # Convert to AutoGluon's TabularDataset\n",
        "    if sample_size:\n",
        "        train_dataset = TabularDataset(df_train).sample(n=sample_size, random_state=random_seed)\n",
        "        test_dataset = TabularDataset(df_test).sample(n=sample_size, random_state=random_seed)\n",
        "    else:\n",
        "        train_dataset = TabularDataset(df_train)\n",
        "        test_dataset = TabularDataset(df_test)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# Also instantiate the target column\n",
        "label_property = 'oz252'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fOvxTvSNc-56"
      },
      "outputs": [],
      "source": [
        "## Function to fit the model, with most of the hyperparameters present and set to default/None. (Add more hyperparameters if desirable)\n",
        "\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "def fit_gluon(train_dataset, problem_type='regression', hyperparameters=None, eval_metric='r2', presets='medium_quality', time_limit=100, auto_stack=None, num_bag_folds=None, num_bag_sets=None, num_stack_levels=None, num_trials=None, verbosity=None, ag_args_fit=None, feature_prune=None, excluded_model_types=None):\n",
        "    predictor = TabularPredictor(label=label_property, problem_type=problem_type, eval_metric=eval_metric)\n",
        "\n",
        "    fit_args = {\n",
        "        'train_data': train_dataset,\n",
        "        'presets': presets,\n",
        "        'time_limit': time_limit,\n",
        "    }\n",
        "\n",
        "    if hyperparameters is not None:\n",
        "        fit_args['hyperparameters'] = hyperparameters\n",
        "    if auto_stack is not None:\n",
        "        fit_args['auto_stack'] = auto_stack\n",
        "    if num_bag_folds is not None:\n",
        "        fit_args['num_bag_folds'] = num_bag_folds\n",
        "    if num_bag_sets is not None:\n",
        "        fit_args['num_bag_sets'] = num_bag_sets\n",
        "    if num_stack_levels is not None:\n",
        "        fit_args['num_stack_levels'] = num_stack_levels\n",
        "    if num_trials is not None:\n",
        "        fit_args['num_trials'] = num_trials\n",
        "    if verbosity is not None:\n",
        "        fit_args['verbosity'] = verbosity\n",
        "    if ag_args_fit is not None:\n",
        "        fit_args['ag_args_fit'] = ag_args_fit\n",
        "    if feature_prune is not None:\n",
        "        fit_args['feature_prune'] = feature_prune\n",
        "    if excluded_model_types is not None:\n",
        "        fit_args['excluded_model_types'] = excluded_model_types\n",
        "\n",
        "    predictor.fit(**fit_args)\n",
        "    return predictor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sQIUKVDbc-56"
      },
      "outputs": [],
      "source": [
        "## Function to evaluate a fitted model and training set.\n",
        "\n",
        "def evaluate_gluon(model, test_dataset):\n",
        "    test_score = model.evaluate(test_dataset)\n",
        "    leaderboard = model.leaderboard(test_dataset)\n",
        "    return test_score, leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1QLLrP5uc-57",
        "outputId": "78a3cf23-394f-4592-c8ab-938187b3cc8e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_082123\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       11.16 GB / 12.67 GB (88.0%)\n",
            "Disk Space Avail:   46.70 GB / 78.19 GB (59.7%)\n",
            "===================================================\n",
            "Presets specified: ['high_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
            "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 500s of the 2000s of remaining time (25%).\n",
            "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
            "2024-07-02 08:21:27,076\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\t\tContext path: \"AutogluonModels/ag-20240702_082123/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Running DyStack sub-fit ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Beginning AutoGluon training ... Time limit = 494s\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20240702_082123/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Train Data Rows:    7107\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Train Data Columns: 62\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Label Column:       oz252\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Problem Type:       regression\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Preprocessing data ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Using Feature Generators to preprocess the data ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tAvailable Memory:                    10972.78 MB\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tTrain Data (Original)  Memory Usage: 2.42 MB (0.0% of available memory)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tStage 1 Generators:\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tStage 2 Generators:\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tStage 3 Generators:\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tStage 4 Generators:\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tStage 5 Generators:\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tUseless Original Features (Count: 1): ['oz222']\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tUnused Original Features (Count: 3): ['oz107', 'oz115', 'oz234']\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\t('category', []) : 3 | ['oz107', 'oz115', 'oz234']\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\t('category', []) : 16 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t\t('int', ['bool']) : 16 | ['oz40', 'oz42', 'oz46', 'oz50', 'oz69', ...]\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.8s = Fit runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t58 features in original data used to generate 58 features in processed data.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tTrain Data (Processed) Memory Usage: 2.39 MB (0.0% of available memory)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Data preprocessing and feature engineering runtime = 0.83s ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m User-specified model hyperparameters to be fit:\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m {\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m }\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 108 L1 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 328.6s of the 493.01s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t-0.0335\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.01s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.33s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 328.23s of the 492.65s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t-0.0258\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.01s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.35s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 327.85s of the 492.26s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.24%)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0729\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t31.43s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.25s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 293.97s of the 458.39s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.25%)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0697\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t28.78s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.07s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 262.45s of the 426.86s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0672\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t37.4s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.47s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 224.21s of the 388.62s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.35%)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0733\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t42.21s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.06s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 178.45s of the 342.87s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0835\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t17.63s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.44s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 160.03s of the 324.45s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.25%)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0269\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t82.97s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.38s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 73.13s of the 237.54s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.42%)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0645\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t29.53s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.09s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 39.73s of the 204.14s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t-0.123\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t57.65s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t1.88s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 143.43s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.32, 'NeuralNetFastAI_BAG_L1': 0.2, 'XGBoost_BAG_L1': 0.16, 'KNeighborsDist_BAG_L1': 0.12, 'RandomForestMSE_BAG_L1': 0.08, 'LightGBMXT_BAG_L1': 0.04, 'LightGBM_BAG_L1': 0.04, 'CatBoost_BAG_L1': 0.04}\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.1\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.37s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 106 L2 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 143.01s of the 142.69s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.30%)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.1016\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t29.65s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.11s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 108.13s of the 107.8s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.28%)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0936\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t33.26s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.07s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 71.11s of the 70.77s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0698\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t73.93s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.64s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -5.1s of remaining time.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.455, 'ExtraTreesMSE_BAG_L1': 0.136, 'RandomForestMSE_BAG_L2': 0.136, 'NeuralNetFastAI_BAG_L1': 0.091, 'KNeighborsDist_BAG_L1': 0.045, 'RandomForestMSE_BAG_L1': 0.045, 'XGBoost_BAG_L1': 0.045, 'LightGBM_BAG_L2': 0.045}\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.1058\t = Validation score   (r2)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.4s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m AutoGluon training complete, total runtime = 499.38s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 278.5 rows/s (889 batch size)\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.01s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.33s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.01s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.35s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t2.08s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.32s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t37.4s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.47s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t2.33s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t17.63s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.44s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tStopping at the best epoch learned earlier - 10.\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t2.76s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.23s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 1 L1 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t4.92s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.32, 'NeuralNetFastAI_BAG_L1': 0.2, 'XGBoost_BAG_L1': 0.16, 'KNeighborsDist_BAG_L1': 0.12, 'RandomForestMSE_BAG_L1': 0.08, 'LightGBMXT_BAG_L1': 0.04, 'LightGBM_BAG_L1': 0.04, 'CatBoost_BAG_L1': 0.04}\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.37s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.62s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting 1 L2 models ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.42s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t73.93s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.64s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.455, 'ExtraTreesMSE_BAG_L1': 0.136, 'RandomForestMSE_BAG_L2': 0.136, 'NeuralNetFastAI_BAG_L1': 0.091, 'KNeighborsDist_BAG_L1': 0.045, 'RandomForestMSE_BAG_L1': 0.045, 'XGBoost_BAG_L1': 0.045, 'LightGBM_BAG_L2': 0.045}\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m \t0.4s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Refit complete, total runtime = 17.59s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_082123/ds_sub_fit/sub_fit_ho\")\n",
            "\u001b[36m(_dystack pid=5146)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                          model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0          LightGBM_BAG_L1_FULL       0.108107   0.069725          r2        0.007973            NaN    0.323371                 0.007973                     NaN           0.323371            1       True          4\n",
            "1      WeightedEnsemble_L2_FULL       0.092190   0.099983          r2        0.732838            NaN   63.137561                 0.004555                     NaN           0.373300            2       True         11\n",
            "2      WeightedEnsemble_L3_FULL       0.086951   0.105810          r2        1.140416            NaN  143.052255                 0.004986                     NaN           0.397032            3       True         15\n",
            "3          CatBoost_BAG_L1_FULL       0.086178   0.073283          r2        0.006459            NaN    2.326554                 0.006459                     NaN           2.326554            1       True          6\n",
            "4           XGBoost_BAG_L1_FULL       0.085858   0.064498          r2        0.008939            NaN    0.234107                 0.008939                     NaN           0.234107            1       True          9\n",
            "5        LightGBMXT_BAG_L1_FULL       0.084987   0.072927          r2        0.020950            NaN    2.079231                 0.020950                     NaN           2.079231            1       True          3\n",
            "6   RandomForestMSE_BAG_L1_FULL       0.084620   0.067175          r2        0.330465       0.470766   37.396228                 0.330465                0.470766          37.396228            1       True          5\n",
            "7        LightGBMXT_BAG_L2_FULL       0.081475   0.101598          r2        0.827765            NaN   68.311223                 0.006384                     NaN           0.617906            2       True         12\n",
            "8     ExtraTreesMSE_BAG_L1_FULL       0.079941   0.083455          r2        0.287409       0.437800   17.629398                 0.287409                0.437800          17.629398            1       True          7\n",
            "9          LightGBM_BAG_L2_FULL       0.076332   0.093612          r2        0.826029            NaN   68.112066                 0.004647                     NaN           0.418749            2       True         13\n",
            "10  RandomForestMSE_BAG_L2_FULL       0.060633   0.069827          r2        1.124398            NaN  141.618568                 0.303017                0.643008          73.925251            2       True         14\n",
            "11  NeuralNetFastAI_BAG_L1_FULL       0.001486   0.026864          r2        0.026384            NaN    2.764478                 0.026384                     NaN           2.764478            1       True          8\n",
            "12   KNeighborsUnif_BAG_L1_FULL      -0.046323  -0.033504          r2        0.053507       0.327564    0.012110                 0.053507                0.327564           0.012110            1       True          1\n",
            "13   KNeighborsDist_BAG_L1_FULL      -0.072575  -0.025804          r2        0.039705       0.348137    0.010895                 0.039705                0.348137           0.010895            1       True          2\n",
            "14   NeuralNetTorch_BAG_L1_FULL      -0.186556  -0.123001          r2        0.039592            NaN    4.916945                 0.039592                     NaN           4.916945            1       True         10\n",
            "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
            "\t526s\t = DyStack   runtime |\t1474s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=0.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
            "Beginning AutoGluon training ... Time limit = 1474s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_082123\"\n",
            "Train Data Rows:    7996\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10224.30 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 6): ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', 'oz115']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 6 | ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
            "\t1.1s = Fit runtime\n",
            "\t56 features in original data used to generate 56 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 108 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1472.43s of the 1472.39s of remaining time.\n",
            "\t-0.023\t = Validation score   (r2)\n",
            "\t0.04s\t = Training   runtime\n",
            "\t0.76s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1471.54s of the 1471.49s of remaining time.\n",
            "\t-0.0193\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.47s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1471.01s of the 1470.96s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.25%)\n",
            "\t0.0789\t = Validation score   (r2)\n",
            "\t32.54s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1435.88s of the 1435.83s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.26%)\n",
            "\t0.0782\t = Validation score   (r2)\n",
            "\t29.1s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1403.33s of the 1403.29s of remaining time.\n",
            "\t0.0775\t = Validation score   (r2)\n",
            "\t45.63s\t = Training   runtime\n",
            "\t0.51s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1356.67s of the 1356.63s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.35%)\n",
            "\t0.0789\t = Validation score   (r2)\n",
            "\t52.41s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1301.54s of the 1301.49s of remaining time.\n",
            "\t0.0924\t = Validation score   (r2)\n",
            "\t18.15s\t = Training   runtime\n",
            "\t0.71s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1281.89s of the 1281.84s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.28%)\n",
            "\t0.0407\t = Validation score   (r2)\n",
            "\t95.73s\t = Training   runtime\n",
            "\t0.32s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1183.31s of the 1183.26s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.41%)\n",
            "\t0.0765\t = Validation score   (r2)\n",
            "\t31.39s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1148.7s of the 1148.65s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.15%)\n",
            "\t-0.1227\t = Validation score   (r2)\n",
            "\t117.51s\t = Training   runtime\n",
            "\t0.65s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1028.36s of the 1028.31s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.56%)\n",
            "\t0.0842\t = Validation score   (r2)\n",
            "\t38.47s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 986.88s of the 986.84s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.37%)\n",
            "\t0.0743\t = Validation score   (r2)\n",
            "\t41.56s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 940.59s of the 940.54s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.15%)\n",
            "\t0.0289\t = Validation score   (r2)\n",
            "\t212.82s\t = Training   runtime\n",
            "\t0.81s\t = Validation runtime\n",
            "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 722.25s of the 722.21s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.32%)\n",
            "\t0.0865\t = Validation score   (r2)\n",
            "\t40.21s\t = Training   runtime\n",
            "\t0.63s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 676.13s of the 676.08s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.28%)\n",
            "\t0.0456\t = Validation score   (r2)\n",
            "\t192.36s\t = Training   runtime\n",
            "\t0.37s\t = Validation runtime\n",
            "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 481.07s of the 481.03s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.94%)\n",
            "\t0.0862\t = Validation score   (r2)\n",
            "\t162.0s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 316.06s of the 316.02s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
            "\t0.0762\t = Validation score   (r2)\n",
            "\t70.73s\t = Training   runtime\n",
            "\t2.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 242.27s of the 242.23s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.15%)\n",
            "\t0.043\t = Validation score   (r2)\n",
            "\t211.94s\t = Training   runtime\n",
            "\t0.66s\t = Validation runtime\n",
            "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 22.46s of the 22.42s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.81%)\n",
            "\t0.0868\t = Validation score   (r2)\n",
            "\t31.85s\t = Training   runtime\n",
            "\t0.41s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -12.96s of remaining time.\n",
            "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.278, 'LightGBMLarge_BAG_L1': 0.167, 'NeuralNetFastAI_r191_BAG_L1': 0.167, 'KNeighborsDist_BAG_L1': 0.111, 'NeuralNetFastAI_BAG_L1': 0.111, 'RandomForestMSE_BAG_L1': 0.056, 'XGBoost_BAG_L1': 0.056, 'LightGBM_r131_BAG_L1': 0.056}\n",
            "\t0.1112\t = Validation score   (r2)\n",
            "\t0.45s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 1487.14s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 557.1 rows/s (1000 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.04s\t = Training   runtime\n",
            "\t0.76s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.47s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\t1.6s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\t0.39s\t = Training   runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t45.63s\t = Training   runtime\n",
            "\t0.51s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\t3.02s\t = Training   runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t18.15s\t = Training   runtime\n",
            "\t0.71s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
            "\tStopping at the best epoch learned earlier - 9.\n",
            "\t2.99s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\t0.28s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
            "\t4.12s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
            "\t0.77s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r177_BAG_L1_FULL ...\n",
            "\t2.1s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1_FULL ...\n",
            "\t14.58s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_r131_BAG_L1_FULL ...\n",
            "\t1.57s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L1_FULL ...\n",
            "\tStopping at the best epoch learned earlier - 15.\n",
            "\t10.67s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r9_BAG_L1_FULL ...\n",
            "\t11.18s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_r96_BAG_L1_FULL ...\n",
            "\t3.61s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1_FULL ...\n",
            "\t16.73s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_r33_BAG_L1_FULL ...\n",
            "\t2.01s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L1': 0.278, 'LightGBMLarge_BAG_L1': 0.167, 'NeuralNetFastAI_r191_BAG_L1': 0.167, 'KNeighborsDist_BAG_L1': 0.111, 'NeuralNetFastAI_BAG_L1': 0.111, 'RandomForestMSE_BAG_L1': 0.056, 'XGBoost_BAG_L1': 0.056, 'LightGBM_r131_BAG_L1': 0.056}\n",
            "\t0.45s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 80.86s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_082123\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold \n",
            "\n",
            " 1: {'r2': 0.07766114711953387, 'root_mean_squared_error': -0.025582537197990585, 'mean_squared_error': -0.000654466209486572, 'mean_absolute_error': -0.01846112297459743, 'pearsonr': 0.29126989693397143, 'median_absolute_error': -0.013395467689514162} \n",
            "\n",
            "\n",
            "                               model  score_test  score_val eval_metric  \\\n",
            "0          CatBoost_r177_BAG_L1_FULL    0.084717        NaN          r2   \n",
            "1          LightGBM_r131_BAG_L1_FULL    0.084520        NaN          r2   \n",
            "2           WeightedEnsemble_L2_FULL    0.077661        NaN          r2   \n",
            "3            CatBoost_r9_BAG_L1_FULL    0.077225        NaN          r2   \n",
            "4             LightGBMXT_BAG_L1_FULL    0.076856        NaN          r2   \n",
            "5               LightGBM_BAG_L1_FULL    0.076227        NaN          r2   \n",
            "6            XGBoost_r33_BAG_L1_FULL    0.073999        NaN          r2   \n",
            "7           LightGBM_r96_BAG_L1_FULL    0.073765        NaN          r2   \n",
            "8               CatBoost_BAG_L1_FULL    0.071711        NaN          r2   \n",
            "9          LightGBMLarge_BAG_L1_FULL    0.065711        NaN          r2   \n",
            "10               XGBoost_BAG_L1_FULL    0.063510        NaN          r2   \n",
            "11              ExtraTreesMSE_BAG_L1    0.058363   0.092425          r2   \n",
            "12         ExtraTreesMSE_BAG_L1_FULL    0.058363        NaN          r2   \n",
            "13       RandomForestMSE_BAG_L1_FULL    0.046418        NaN          r2   \n",
            "14            RandomForestMSE_BAG_L1    0.046418   0.077460          r2   \n",
            "15    NeuralNetTorch_r22_BAG_L1_FULL   -0.033351        NaN          r2   \n",
            "16  NeuralNetFastAI_r191_BAG_L1_FULL   -0.034585        NaN          r2   \n",
            "17       NeuralNetFastAI_BAG_L1_FULL   -0.041426        NaN          r2   \n",
            "18             KNeighborsUnif_BAG_L1   -0.123922  -0.022992          r2   \n",
            "19        KNeighborsUnif_BAG_L1_FULL   -0.123922        NaN          r2   \n",
            "20        KNeighborsDist_BAG_L1_FULL   -0.174566        NaN          r2   \n",
            "21             KNeighborsDist_BAG_L1   -0.174566  -0.019320          r2   \n",
            "22        NeuralNetTorch_BAG_L1_FULL   -0.402185        NaN          r2   \n",
            "23    NeuralNetTorch_r79_BAG_L1_FULL   -0.874443        NaN          r2   \n",
            "24               WeightedEnsemble_L2         NaN   0.111162          r2   \n",
            "25                XGBoost_r33_BAG_L1         NaN   0.086786          r2   \n",
            "26              LightGBM_r131_BAG_L1         NaN   0.086542          r2   \n",
            "27                CatBoost_r9_BAG_L1         NaN   0.086238          r2   \n",
            "28              LightGBMLarge_BAG_L1         NaN   0.084214          r2   \n",
            "29                 LightGBMXT_BAG_L1         NaN   0.078912          r2   \n",
            "30                   CatBoost_BAG_L1         NaN   0.078910          r2   \n",
            "31                   LightGBM_BAG_L1         NaN   0.078199          r2   \n",
            "32                    XGBoost_BAG_L1         NaN   0.076475          r2   \n",
            "33               LightGBM_r96_BAG_L1         NaN   0.076171          r2   \n",
            "34              CatBoost_r177_BAG_L1         NaN   0.074269          r2   \n",
            "35       NeuralNetFastAI_r191_BAG_L1         NaN   0.045600          r2   \n",
            "36         NeuralNetTorch_r22_BAG_L1         NaN   0.042958          r2   \n",
            "37            NeuralNetFastAI_BAG_L1         NaN   0.040678          r2   \n",
            "38         NeuralNetTorch_r79_BAG_L1         NaN   0.028856          r2   \n",
            "39             NeuralNetTorch_BAG_L1         NaN  -0.122673          r2   \n",
            "\n",
            "    pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  \\\n",
            "0         0.020502            NaN    2.095866                 0.020502   \n",
            "1         0.123615            NaN    1.567085                 0.123615   \n",
            "2         1.300584            NaN   80.530296                 0.007182   \n",
            "3         0.043572            NaN   11.176448                 0.043572   \n",
            "4         0.022348            NaN    1.604950                 0.022348   \n",
            "5         0.006700            NaN    0.386817                 0.006700   \n",
            "6         0.060089            NaN    2.008086                 0.060089   \n",
            "7         0.245275            NaN    3.609251                 0.245275   \n",
            "8         0.006605            NaN    3.024330                 0.006605   \n",
            "9         0.079309            NaN    0.766720                 0.079309   \n",
            "10        0.013555            NaN    0.279339                 0.013555   \n",
            "11        0.385983       0.705230   18.152731                 0.385983   \n",
            "12        0.532128       0.705230   18.152731                 0.532128   \n",
            "13        0.398062       0.507206   45.626063                 0.398062   \n",
            "14        0.473032       0.507206   45.626063                 0.473032   \n",
            "15        0.072610            NaN   16.729488                 0.072610   \n",
            "16        0.063153            NaN   10.673667                 0.063153   \n",
            "17        0.032806            NaN    2.994904                 0.032806   \n",
            "18        0.049064       0.764467    0.041860                 0.049064   \n",
            "19        0.054833       0.764467    0.041860                 0.054833   \n",
            "20        0.050775       0.469676    0.019112                 0.050775   \n",
            "21        0.054412       0.469676    0.019112                 0.054412   \n",
            "22        0.048483            NaN    4.121732                 0.048483   \n",
            "23        0.097315            NaN   14.578253                 0.097315   \n",
            "24             NaN       3.274999  462.409082                      NaN   \n",
            "25             NaN       0.407657   31.847896                      NaN   \n",
            "26             NaN       0.633728   40.208432                      NaN   \n",
            "27             NaN       0.201572  161.997231                      NaN   \n",
            "28             NaN       0.152054   38.468569                      NaN   \n",
            "29             NaN       0.285126   32.544585                      NaN   \n",
            "30             NaN       0.053792   52.414744                      NaN   \n",
            "31             NaN       0.092721   29.104887                      NaN   \n",
            "32             NaN       0.108011   31.390437                      NaN   \n",
            "33             NaN       2.018269   70.725529                      NaN   \n",
            "34             NaN       0.085440   41.561961                      NaN   \n",
            "35             NaN       0.365479  192.362083                      NaN   \n",
            "36             NaN       0.663407  211.944837                      NaN   \n",
            "37             NaN       0.324236   95.730980                      NaN   \n",
            "38             NaN       0.809588  212.815485                      NaN   \n",
            "39             NaN       0.652275  117.512638                      NaN   \n",
            "\n",
            "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
            "0                      NaN           2.095866            1       True   \n",
            "1                      NaN           1.567085            1       True   \n",
            "2                      NaN           0.450674            2       True   \n",
            "3                      NaN          11.176448            1       True   \n",
            "4                      NaN           1.604950            1       True   \n",
            "5                      NaN           0.386817            1       True   \n",
            "6                      NaN           2.008086            1       True   \n",
            "7                      NaN           3.609251            1       True   \n",
            "8                      NaN           3.024330            1       True   \n",
            "9                      NaN           0.766720            1       True   \n",
            "10                     NaN           0.279339            1       True   \n",
            "11                0.705230          18.152731            1       True   \n",
            "12                0.705230          18.152731            1       True   \n",
            "13                0.507206          45.626063            1       True   \n",
            "14                0.507206          45.626063            1       True   \n",
            "15                     NaN          16.729488            1       True   \n",
            "16                     NaN          10.673667            1       True   \n",
            "17                     NaN           2.994904            1       True   \n",
            "18                0.764467           0.041860            1       True   \n",
            "19                0.764467           0.041860            1       True   \n",
            "20                0.469676           0.019112            1       True   \n",
            "21                0.469676           0.019112            1       True   \n",
            "22                     NaN           4.121732            1       True   \n",
            "23                     NaN          14.578253            1       True   \n",
            "24                0.009381           0.450674            2      False   \n",
            "25                0.407657          31.847896            1      False   \n",
            "26                0.633728          40.208432            1      False   \n",
            "27                0.201572         161.997231            1      False   \n",
            "28                0.152054          38.468569            1      False   \n",
            "29                0.285126          32.544585            1      False   \n",
            "30                0.053792          52.414744            1      False   \n",
            "31                0.092721          29.104887            1      False   \n",
            "32                0.108011          31.390437            1      False   \n",
            "33                2.018269          70.725529            1      False   \n",
            "34                0.085440          41.561961            1      False   \n",
            "35                0.365479         192.362083            1      False   \n",
            "36                0.663407         211.944837            1      False   \n",
            "37                0.324236          95.730980            1      False   \n",
            "38                0.809588         212.815485            1      False   \n",
            "39                0.652275         117.512638            1      False   \n",
            "\n",
            "    fit_order  \n",
            "0          32  \n",
            "1          34  \n",
            "2          40  \n",
            "3          36  \n",
            "4          23  \n",
            "5          24  \n",
            "6          39  \n",
            "7          37  \n",
            "8          26  \n",
            "9          31  \n",
            "10         29  \n",
            "11          7  \n",
            "12         27  \n",
            "13         25  \n",
            "14          5  \n",
            "15         38  \n",
            "16         35  \n",
            "17         28  \n",
            "18          1  \n",
            "19         21  \n",
            "20         22  \n",
            "21          2  \n",
            "22         30  \n",
            "23         33  \n",
            "24         20  \n",
            "25         19  \n",
            "26         14  \n",
            "27         16  \n",
            "28         11  \n",
            "29          3  \n",
            "30          6  \n",
            "31          4  \n",
            "32          9  \n",
            "33         17  \n",
            "34         12  \n",
            "35         15  \n",
            "36         18  \n",
            "37          8  \n",
            "38         13  \n",
            "39         10  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_085627\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.12 GB / 12.67 GB (79.9%)\n",
            "Disk Space Avail:   45.85 GB / 78.19 GB (58.6%)\n",
            "===================================================\n",
            "Presets specified: ['high_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
            "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 500s of the 2000s of remaining time (25%).\n",
            "\t\tContext path: \"AutogluonModels/ag-20240702_085627/ds_sub_fit/sub_fit_ho\"\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                          model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0        LightGBMXT_BAG_L1_FULL       0.098127   0.070159          r2        0.016974            NaN    1.513478                 0.016974                     NaN           1.513478            1       True          3\n",
            "1      WeightedEnsemble_L2_FULL       0.096576   0.095788          r2        0.616429            NaN   65.609128                 0.004711                     NaN           0.318275            2       True         11\n",
            "2          LightGBM_BAG_L1_FULL       0.094954   0.070617          r2        0.009279            NaN    0.301830                 0.009279                     NaN           0.301830            1       True          4\n",
            "3          CatBoost_BAG_L1_FULL       0.094131   0.069967          r2        0.007109            NaN    3.780681                 0.007109                     NaN           3.780681            1       True          6\n",
            "4           XGBoost_BAG_L1_FULL       0.091628   0.063922          r2        0.007762            NaN    0.209306                 0.007762                     NaN           0.209306            1       True          9\n",
            "5     ExtraTreesMSE_BAG_L1_FULL       0.088885   0.078621          r2        0.247201       0.440647   15.647173                 0.247201                0.440647          15.647173            1       True          7\n",
            "6   RandomForestMSE_BAG_L1_FULL       0.088876   0.058444          r2        0.259794       0.444371   41.536919                 0.259794                0.444371          41.536919            1       True          5\n",
            "7        LightGBMXT_BAG_L2_FULL       0.086124   0.095945          r2        0.707943            NaN   68.781300                 0.005482                     NaN           0.384741            2       True         12\n",
            "8      WeightedEnsemble_L3_FULL       0.083473   0.101495          r2        0.953093            NaN  144.126785                 0.004470                     NaN           0.192183            3       True         15\n",
            "9          LightGBM_BAG_L2_FULL       0.062291   0.089869          r2        0.707142            NaN   68.827540                 0.004681                     NaN           0.430981            2       True         13\n",
            "10  RandomForestMSE_BAG_L2_FULL       0.022918   0.074963          r2        0.938460            NaN  143.118880                 0.235998                0.469371          74.722321            2       True         14\n",
            "11  NeuralNetFastAI_BAG_L1_FULL      -0.017543   0.036388          r2        0.024652            NaN    2.289263                 0.024652                     NaN           2.289263            1       True          8\n",
            "12   KNeighborsUnif_BAG_L1_FULL      -0.107222  -0.044623          r2        0.052833       0.316760    0.011805                 0.052833                0.316760           0.011805            1       True          1\n",
            "13   KNeighborsDist_BAG_L1_FULL      -0.135346  -0.043021          r2        0.038947       0.336081    0.012203                 0.038947                0.336081           0.012203            1       True          2\n",
            "14   NeuralNetTorch_BAG_L1_FULL      -0.189951  -0.132925          r2        0.037910            NaN    3.093903                 0.037910                     NaN           3.093903            1       True         10\n",
            "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
            "\t526s\t = DyStack   runtime |\t1474s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=0.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
            "Beginning AutoGluon training ... Time limit = 1474s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_085627\"\n",
            "Train Data Rows:    7996\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9942.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['oz115']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 6): ['oz46', 'oz50', 'oz69', 'oz100', 'oz107', 'oz111']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 6 | ['oz46', 'oz50', 'oz69', 'oz100', 'oz107', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 13 | ['oz40', 'oz42', 'oz71', 'oz73', 'oz79', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 13 | ['oz40', 'oz42', 'oz71', 'oz73', 'oz79', ...]\n",
            "\t1.1s = Fit runtime\n",
            "\t55 features in original data used to generate 55 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.66 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 108 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1473.28s of the 1473.26s of remaining time.\n",
            "\t-0.0337\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.71s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1472.49s of the 1472.47s of remaining time.\n",
            "\t-0.035\t = Validation score   (r2)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.82s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1471.57s of the 1471.55s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.27%)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4ff082708404>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# jsut the three first to start with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_gluon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'high_quality'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaderboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_gluon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Fold \\n\\n {fold_number}: {test_score} \\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-1ace18ba1541>\u001b[0m in \u001b[0;36mfit_gluon\u001b[0;34m(train_dataset, problem_type, hyperparameters, eval_metric, presets, time_limit, auto_stack, num_bag_folds, num_bag_sets, num_stack_levels, num_trials, verbosity, ag_args_fit, feature_prune, excluded_model_types)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'excluded_model_types'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexcluded_model_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/utils/decorators.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mother_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/predictor/predictor.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save predictor to disk to enable prediction and training after interrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mag_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_post_fit_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mag_post_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/learner/abstract_learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learner is already fit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_fit_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     def _fit(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/learner/default_learner.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         trainer.fit(\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/tabular/trainer/auto_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         self._train_multi_and_ensemble(\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_rows_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2588\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_cols_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2589\u001b[0;31m         model_names_fit = self.train_multi_levels(\n\u001b[0m\u001b[1;32m   2590\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36mtrain_multi_levels\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0mcore_kwargs_level\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore_kwargs_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit_core\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0maux_kwargs_level\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_kwargs_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit_aux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             base_model_names, aux_models = self.stack_new_level(\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36mstack_new_level\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size, full_weighted_ensemble, additional_full_weighted_ensemble)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0mcore_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_suffix\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name_suffix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0maux_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_suffix\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name_suffix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         core_models = self.stack_new_level_core(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36mstack_new_level_core\u001b[0;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;31m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         return self._train_multi(\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi\u001b[0;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_repeat_start\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m             \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             model_names_trained = self._train_multi_initial(\n\u001b[0m\u001b[1;32m   2540\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi_initial\u001b[0;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m             \u001b[0mtime_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhpo_time_ratio\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhpo_enabled\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m             models = self._train_multi_fold(\n\u001b[0m\u001b[1;32m   2389\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m                 \u001b[0mhyperparameter_tune_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter_tune_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_multi_fold\u001b[0;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m                     \u001b[0mtime_start_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m                     \u001b[0mtime_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime_start_model\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2496\u001b[0;31m             model_name_trained_lst = self._train_single_full(\n\u001b[0m\u001b[1;32m   2497\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameter_tune_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter_tune_kwargs_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_single_full\u001b[0;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m                 )\n\u001b[1;32m   2268\u001b[0m                 \u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbagged_model_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m             model_names_trained = self._train_and_save(\n\u001b[0m\u001b[1;32m   2270\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_and_save\u001b[0;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1902\u001b[0m                     \u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_pseudo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1903\u001b[0m                     \u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_pseudo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1904\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_resources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_resources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m             \u001b[0mfit_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\u001b[0m in \u001b[0;36m_train_single\u001b[0;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \"\"\"\n\u001b[0;32m-> 1844\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_resources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_resources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_fit_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_fit_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mtime_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_stack_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_column_prefix_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m                     \u001b[0;31m# Reserve time for final refit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_limit\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfolds_to_fit\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfolds_to_fit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             self._fit_folds(\n\u001b[0m\u001b[1;32m    289\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\u001b[0m in \u001b[0;36m_fit_folds\u001b[0;34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold_fit_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfold_fit_args_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfold_fitting_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule_fold_model_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfold_fit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         \u001b[0mfold_fitting_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_all_folds_scheduled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\u001b[0m in \u001b[0;36mafter_all_folds_scheduled\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_pseudo_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pseudo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pseudo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_base_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_node_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pseudo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pseudo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_base_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_limit_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_node_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate_all_unfinished_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munfinished_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\u001b[0m in \u001b[0;36m_run_parallel\u001b[0;34m(self, X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0munfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mfinished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfinished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_returns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0mfold_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_fold_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\u001b[0m in \u001b[0;36mauto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mauto_init_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2896\u001b[0m         \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m         \u001b[0mtimeout_milliseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m         ready_ids, remaining_ids = worker.core_worker.wait(\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0mray_waitables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## First training the model speratly on all 10 folds to see that it is consistent\n",
        "\n",
        "for fold_number in range(1, 2): # jsut the first to start with\n",
        "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed)\n",
        "    model = fit_gluon(train_dataset, time_limit=2000, presets='high_quality')\n",
        "    test_score, leaderboard = evaluate_gluon(model, test_dataset)\n",
        "    print(f'Fold {fold_number}\\n\\n: {test_score} \\n\\n')\n",
        "    print(leaderboard)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.feature_importance(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "LJ6ZM33xsU1u",
        "outputId": "aa6dafb0-27c2-4238-d208-3d1b784a1e88"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "These features in provided data are not utilized by the predictor and will be ignored: ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', 'oz115']\n",
            "Computing feature importance via permutation shuffling for 56 features using 888 rows with 5 shuffle sets...\n",
            "\t372.15s\t= Expected runtime (74.43s per shuffle set)\n",
            "\t44.29s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       importance    stddev       p_value  n  p99_high   p99_low\n",
              "oz251    0.215600  0.014740  2.605287e-06  5  0.245950  0.185251\n",
              "oz249    0.151272  0.006864  5.072224e-07  5  0.165405  0.137140\n",
              "oz151    0.117863  0.006892  1.396675e-06  5  0.132054  0.103672\n",
              "oz246    0.102660  0.002641  5.251960e-08  5  0.108098  0.097222\n",
              "oz247    0.096256  0.006216  2.075437e-06  5  0.109055  0.083457\n",
              "oz13     0.084232  0.004374  8.690227e-07  5  0.093238  0.075227\n",
              "oz250    0.081146  0.004392  1.026169e-06  5  0.090190  0.072102\n",
              "oz248    0.069838  0.001882  6.316304e-08  5  0.073712  0.065963\n",
              "oz175    0.069092  0.003302  6.237928e-07  5  0.075890  0.062294\n",
              "oz150    0.061265  0.004218  2.680104e-06  5  0.069951  0.052579\n",
              "oz125    0.057687  0.005007  6.742344e-06  5  0.067996  0.047378\n",
              "oz3      0.051098  0.002003  2.828814e-07  5  0.055223  0.046973\n",
              "oz124    0.048523  0.002653  1.067399e-06  5  0.053985  0.043062\n",
              "oz183    0.044821  0.001177  5.692769e-08  5  0.047244  0.042398\n",
              "oz149    0.043490  0.002905  2.375076e-06  5  0.049471  0.037508\n",
              "oz172    0.042754  0.002868  2.415690e-06  5  0.048659  0.036849\n",
              "oz165    0.042494  0.004623  1.654058e-05  5  0.052012  0.032976\n",
              "oz12     0.037574  0.001374  2.139291e-07  5  0.040402  0.034746\n",
              "oz5      0.033078  0.002984  7.866451e-06  5  0.039224  0.026933\n",
              "oz4      0.032903  0.001961  1.507264e-06  5  0.036941  0.028865\n",
              "oz1      0.032837  0.001528  5.610529e-07  5  0.035984  0.029691\n",
              "oz171    0.030378  0.001500  7.106469e-07  5  0.033466  0.027290\n",
              "oz176    0.028000  0.001538  1.087360e-06  5  0.031166  0.024833\n",
              "oz197    0.026069  0.001477  1.230490e-06  5  0.029110  0.023028\n",
              "oz177    0.023733  0.001706  3.181206e-06  5  0.027245  0.020220\n",
              "oz127    0.023245  0.000515  2.894701e-08  5  0.024306  0.022184\n",
              "oz6      0.022917  0.000573  4.694663e-08  5  0.024097  0.021737\n",
              "oz126    0.022333  0.001042  5.670859e-07  5  0.024478  0.020187\n",
              "oz131    0.022045  0.001002  5.101462e-07  5  0.024108  0.019983\n",
              "oz128    0.020824  0.001962  9.351807e-06  5  0.024865  0.016784\n",
              "oz10     0.020284  0.001455  3.151173e-06  5  0.023279  0.017289\n",
              "oz185    0.019685  0.001771  7.776118e-06  5  0.023331  0.016039\n",
              "oz9      0.017086  0.001100  2.051568e-06  5  0.019352  0.014821\n",
              "oz133    0.015005  0.001824  2.571430e-05  5  0.018762  0.011249\n",
              "oz11     0.014792  0.000882  1.510784e-06  5  0.016608  0.012975\n",
              "oz173    0.012770  0.000529  3.538008e-07  5  0.013861  0.011680\n",
              "oz2      0.012520  0.001234  1.118505e-05  5  0.015061  0.009979\n",
              "oz31     0.010701  0.001460  4.057554e-05  5  0.013708  0.007695\n",
              "oz178    0.007664  0.000465  1.611096e-06  5  0.008621  0.006708\n",
              "oz181    0.006454  0.000965  5.831186e-05  5  0.008442  0.004467\n",
              "oz87     0.006312  0.000266  3.802878e-07  5  0.006861  0.005763\n",
              "oz83     0.004763  0.001080  2.964808e-04  5  0.006986  0.002539\n",
              "oz234    0.000797  0.000151  1.469081e-04  5  0.001108  0.000487\n",
              "oz79     0.000679  0.000679  4.457621e-02  5  0.002077 -0.000720\n",
              "oz96     0.000326  0.000092  6.806423e-04  5  0.000515  0.000137\n",
              "oz135    0.000186  0.000085  4.081597e-03  5  0.000361  0.000010\n",
              "oz71     0.000153  0.000437  2.389816e-01  5  0.001052 -0.000747\n",
              "oz206    0.000004  0.000184  4.809513e-01  5  0.000384 -0.000375\n",
              "oz113    0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz112    0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz108    0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz73     0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz42     0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz40     0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz46     0.000000  0.000000  5.000000e-01  5  0.000000  0.000000\n",
              "oz222    0.000000  0.000000  5.000000e-01  5  0.000000  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-041948df-158f-444a-98d8-0694ab7cb3e1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "      <th>stddev</th>\n",
              "      <th>p_value</th>\n",
              "      <th>n</th>\n",
              "      <th>p99_high</th>\n",
              "      <th>p99_low</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>oz251</th>\n",
              "      <td>0.215600</td>\n",
              "      <td>0.014740</td>\n",
              "      <td>2.605287e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.245950</td>\n",
              "      <td>0.185251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz249</th>\n",
              "      <td>0.151272</td>\n",
              "      <td>0.006864</td>\n",
              "      <td>5.072224e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.165405</td>\n",
              "      <td>0.137140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz151</th>\n",
              "      <td>0.117863</td>\n",
              "      <td>0.006892</td>\n",
              "      <td>1.396675e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.132054</td>\n",
              "      <td>0.103672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz246</th>\n",
              "      <td>0.102660</td>\n",
              "      <td>0.002641</td>\n",
              "      <td>5.251960e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.108098</td>\n",
              "      <td>0.097222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz247</th>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.006216</td>\n",
              "      <td>2.075437e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.109055</td>\n",
              "      <td>0.083457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz13</th>\n",
              "      <td>0.084232</td>\n",
              "      <td>0.004374</td>\n",
              "      <td>8.690227e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.093238</td>\n",
              "      <td>0.075227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz250</th>\n",
              "      <td>0.081146</td>\n",
              "      <td>0.004392</td>\n",
              "      <td>1.026169e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.090190</td>\n",
              "      <td>0.072102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz248</th>\n",
              "      <td>0.069838</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>6.316304e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.073712</td>\n",
              "      <td>0.065963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz175</th>\n",
              "      <td>0.069092</td>\n",
              "      <td>0.003302</td>\n",
              "      <td>6.237928e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.075890</td>\n",
              "      <td>0.062294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz150</th>\n",
              "      <td>0.061265</td>\n",
              "      <td>0.004218</td>\n",
              "      <td>2.680104e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.069951</td>\n",
              "      <td>0.052579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz125</th>\n",
              "      <td>0.057687</td>\n",
              "      <td>0.005007</td>\n",
              "      <td>6.742344e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.067996</td>\n",
              "      <td>0.047378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz3</th>\n",
              "      <td>0.051098</td>\n",
              "      <td>0.002003</td>\n",
              "      <td>2.828814e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.055223</td>\n",
              "      <td>0.046973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz124</th>\n",
              "      <td>0.048523</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>1.067399e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.053985</td>\n",
              "      <td>0.043062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz183</th>\n",
              "      <td>0.044821</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>5.692769e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.047244</td>\n",
              "      <td>0.042398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz149</th>\n",
              "      <td>0.043490</td>\n",
              "      <td>0.002905</td>\n",
              "      <td>2.375076e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.049471</td>\n",
              "      <td>0.037508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz172</th>\n",
              "      <td>0.042754</td>\n",
              "      <td>0.002868</td>\n",
              "      <td>2.415690e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.048659</td>\n",
              "      <td>0.036849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz165</th>\n",
              "      <td>0.042494</td>\n",
              "      <td>0.004623</td>\n",
              "      <td>1.654058e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.052012</td>\n",
              "      <td>0.032976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz12</th>\n",
              "      <td>0.037574</td>\n",
              "      <td>0.001374</td>\n",
              "      <td>2.139291e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.040402</td>\n",
              "      <td>0.034746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz5</th>\n",
              "      <td>0.033078</td>\n",
              "      <td>0.002984</td>\n",
              "      <td>7.866451e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.039224</td>\n",
              "      <td>0.026933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz4</th>\n",
              "      <td>0.032903</td>\n",
              "      <td>0.001961</td>\n",
              "      <td>1.507264e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.036941</td>\n",
              "      <td>0.028865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz1</th>\n",
              "      <td>0.032837</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>5.610529e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.035984</td>\n",
              "      <td>0.029691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz171</th>\n",
              "      <td>0.030378</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>7.106469e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.033466</td>\n",
              "      <td>0.027290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz176</th>\n",
              "      <td>0.028000</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>1.087360e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.031166</td>\n",
              "      <td>0.024833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz197</th>\n",
              "      <td>0.026069</td>\n",
              "      <td>0.001477</td>\n",
              "      <td>1.230490e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.029110</td>\n",
              "      <td>0.023028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz177</th>\n",
              "      <td>0.023733</td>\n",
              "      <td>0.001706</td>\n",
              "      <td>3.181206e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.027245</td>\n",
              "      <td>0.020220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz127</th>\n",
              "      <td>0.023245</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>2.894701e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024306</td>\n",
              "      <td>0.022184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz6</th>\n",
              "      <td>0.022917</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>4.694663e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024097</td>\n",
              "      <td>0.021737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz126</th>\n",
              "      <td>0.022333</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>5.670859e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024478</td>\n",
              "      <td>0.020187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz131</th>\n",
              "      <td>0.022045</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>5.101462e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024108</td>\n",
              "      <td>0.019983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz128</th>\n",
              "      <td>0.020824</td>\n",
              "      <td>0.001962</td>\n",
              "      <td>9.351807e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.024865</td>\n",
              "      <td>0.016784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz10</th>\n",
              "      <td>0.020284</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>3.151173e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.023279</td>\n",
              "      <td>0.017289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz185</th>\n",
              "      <td>0.019685</td>\n",
              "      <td>0.001771</td>\n",
              "      <td>7.776118e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.023331</td>\n",
              "      <td>0.016039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz9</th>\n",
              "      <td>0.017086</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>2.051568e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.019352</td>\n",
              "      <td>0.014821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz133</th>\n",
              "      <td>0.015005</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>2.571430e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.018762</td>\n",
              "      <td>0.011249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz11</th>\n",
              "      <td>0.014792</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>1.510784e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.016608</td>\n",
              "      <td>0.012975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz173</th>\n",
              "      <td>0.012770</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>3.538008e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.013861</td>\n",
              "      <td>0.011680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz2</th>\n",
              "      <td>0.012520</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>1.118505e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.015061</td>\n",
              "      <td>0.009979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz31</th>\n",
              "      <td>0.010701</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>4.057554e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.013708</td>\n",
              "      <td>0.007695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz178</th>\n",
              "      <td>0.007664</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>1.611096e-06</td>\n",
              "      <td>5</td>\n",
              "      <td>0.008621</td>\n",
              "      <td>0.006708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz181</th>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.000965</td>\n",
              "      <td>5.831186e-05</td>\n",
              "      <td>5</td>\n",
              "      <td>0.008442</td>\n",
              "      <td>0.004467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz87</th>\n",
              "      <td>0.006312</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>3.802878e-07</td>\n",
              "      <td>5</td>\n",
              "      <td>0.006861</td>\n",
              "      <td>0.005763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz83</th>\n",
              "      <td>0.004763</td>\n",
              "      <td>0.001080</td>\n",
              "      <td>2.964808e-04</td>\n",
              "      <td>5</td>\n",
              "      <td>0.006986</td>\n",
              "      <td>0.002539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz234</th>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>1.469081e-04</td>\n",
              "      <td>5</td>\n",
              "      <td>0.001108</td>\n",
              "      <td>0.000487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz79</th>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>4.457621e-02</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002077</td>\n",
              "      <td>-0.000720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz96</th>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>6.806423e-04</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.000137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz135</th>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>4.081597e-03</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz71</th>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>2.389816e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.001052</td>\n",
              "      <td>-0.000747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz206</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>4.809513e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000384</td>\n",
              "      <td>-0.000375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz113</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz112</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz108</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz73</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz42</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz40</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz46</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oz222</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-041948df-158f-444a-98d8-0694ab7cb3e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-041948df-158f-444a-98d8-0694ab7cb3e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-041948df-158f-444a-98d8-0694ab7cb3e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-56fa2c50-5342-4a5d-9c9c-9444f1cb0cfd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-56fa2c50-5342-4a5d-9c9c-9444f1cb0cfd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-56fa2c50-5342-4a5d-9c9c-9444f1cb0cfd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"model\",\n  \"rows\": 56,\n  \"fields\": [\n    {\n      \"column\": \"importance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.041489531815625945,\n        \"min\": 0.0,\n        \"max\": 0.2156004769030802,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.044820989343228625,\n          0.00018577897714791013,\n          4.189857937464403e-06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stddev\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002472255254947716,\n        \"min\": 0.0,\n        \"max\": 0.01473987211071361,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.0011765672200878216,\n          8.513356626801783e-05,\n          0.0001843387783256428\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18508670257496138,\n        \"min\": 2.894700773908691e-08,\n        \"max\": 0.5,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          5.692768928615341e-08,\n          0.004081596718087056,\n          0.48095130413361187\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p99_high\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04626988176072916,\n        \"min\": 0.0,\n        \"max\": 0.2459500773057223,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.047243557511276596\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p99_low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03679245906880237,\n        \"min\": -0.0007468321459798294,\n        \"max\": 0.18525087650043812,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.04239842117518065\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBt09mtJc-57",
        "outputId": "249f54ba-66db-4ebb-fa7f-8e76be1b97c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_093443\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.92 GB / 12.67 GB (78.3%)\n",
            "Disk Space Avail:   45.84 GB / 78.19 GB (58.6%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_093443\"\n",
            "Train Data Rows:    7996\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10163.55 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 6): ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', 'oz115']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 6 | ['oz50', 'oz69', 'oz100', 'oz107', 'oz111', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) : 14 | ['oz40', 'oz42', 'oz46', 'oz71', 'oz73', ...]\n",
            "\t0.8s = Fit runtime\n",
            "\t56 features in original data used to generate 56 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.84s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7196, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 99.16s of the 99.16s of remaining time.\n",
            "\t-0.1737\t = Validation score   (r2)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 99.09s of the 99.09s of remaining time.\n",
            "\t-0.1778\t = Validation score   (r2)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 99.03s of the 99.03s of remaining time.\n",
            "\t0.0871\t = Validation score   (r2)\n",
            "\t1.84s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 97.13s of the 97.13s of remaining time.\n",
            "\t0.078\t = Validation score   (r2)\n",
            "\t1.33s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 95.77s of the 95.77s of remaining time.\n",
            "\t0.0786\t = Validation score   (r2)\n",
            "\t39.66s\t = Training   runtime\n",
            "\t0.23s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 55.01s of the 55.01s of remaining time.\n",
            "\t0.0897\t = Validation score   (r2)\n",
            "\t4.85s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ... Training model for up to 50.15s of the 50.15s of remaining time.\n",
            "\t0.0865\t = Validation score   (r2)\n",
            "\t15.01s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 34.62s of the 34.62s of remaining time.\n",
            "\t0.0536\t = Validation score   (r2)\n",
            "\t10.14s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 24.4s of the 24.4s of remaining time.\n",
            "\t0.0654\t = Validation score   (r2)\n",
            "\t2.44s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 21.93s of the 21.93s of remaining time.\n",
            "\t-0.2942\t = Validation score   (r2)\n",
            "\t9.61s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 12.26s of the 12.26s of remaining time.\n",
            "\t0.0878\t = Validation score   (r2)\n",
            "\t4.15s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 99.16s of the 8.05s of remaining time.\n",
            "\tEnsemble Weights: {'ExtraTreesMSE': 0.286, 'NeuralNetFastAI': 0.19, 'LightGBMLarge': 0.19, 'LightGBMXT': 0.143, 'RandomForestMSE': 0.143, 'CatBoost': 0.048}\n",
            "\t0.1103\t = Validation score   (r2)\n",
            "\t0.22s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 92.23s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1950.9 rows/s (800 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_093443\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: {'r2': 0.08900780402347996, 'root_mean_squared_error': -0.025424691438009624, 'mean_squared_error': -0.0006464149347179998, 'mean_absolute_error': -0.018338991526840775, 'pearsonr': 0.3020016838586487, 'median_absolute_error': -0.013147535247802766}\n",
            "                  model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0   WeightedEnsemble_L2    0.089008   0.110349          r2        0.655178   \n",
            "1              CatBoost    0.089001   0.089654          r2        0.009564   \n",
            "2         LightGBMLarge    0.079066   0.087833          r2        0.011603   \n",
            "3               XGBoost    0.077817   0.065368          r2        0.016926   \n",
            "4            LightGBMXT    0.077281   0.087110          r2        0.021298   \n",
            "5              LightGBM    0.072527   0.078010          r2        0.008025   \n",
            "6         ExtraTreesMSE    0.062203   0.086513          r2        0.251512   \n",
            "7       RandomForestMSE    0.048234   0.078561          r2        0.310097   \n",
            "8       NeuralNetFastAI    0.008027   0.053579          r2        0.046535   \n",
            "9        KNeighborsUnif   -0.119236  -0.173739          r2        0.047116   \n",
            "10       KNeighborsDist   -0.192835  -0.177775          r2        0.040982   \n",
            "11       NeuralNetTorch   -0.305328  -0.294174          r2        0.040774   \n",
            "\n",
            "    pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0        0.410075  75.862564                 0.004568                0.000766   \n",
            "1        0.005538   4.848163                 0.009564                0.005538   \n",
            "2        0.007546   4.146707                 0.011603                0.007546   \n",
            "3        0.006859   2.444231                 0.016926                0.006859   \n",
            "4        0.013212   1.835371                 0.021298                0.013212   \n",
            "5        0.005351   1.332516                 0.008025                0.005351   \n",
            "6        0.126851  15.012934                 0.251512                0.126851   \n",
            "7        0.225204  39.660418                 0.310097                0.225204   \n",
            "8        0.030959  10.143753                 0.046535                0.030959   \n",
            "9        0.044363   0.015936                 0.047116                0.044363   \n",
            "10       0.039468   0.014273                 0.040982                0.039468   \n",
            "11       0.050454   9.612742                 0.040774                0.050454   \n",
            "\n",
            "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0            0.215217            2       True         12  \n",
            "1            4.848163            1       True          6  \n",
            "2            4.146707            1       True         11  \n",
            "3            2.444231            1       True          9  \n",
            "4            1.835371            1       True          3  \n",
            "5            1.332516            1       True          4  \n",
            "6           15.012934            1       True          7  \n",
            "7           39.660418            1       True          5  \n",
            "8           10.143753            1       True          8  \n",
            "9            0.015936            1       True          1  \n",
            "10           0.014273            1       True          2  \n",
            "11           9.612742            1       True         10  \n"
          ]
        }
      ],
      "source": [
        "for fold_number in range(1, 2): # jsut the three first to start with\n",
        "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed)\n",
        "    model = fit_gluon(train_dataset, time_limit=100)\n",
        "    test_score, leaderboard = evaluate_gluon(model, test_dataset)\n",
        "    print(f'Fold {fold_number}: {test_score}')\n",
        "    print(leaderboard)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygraphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LxTWhbD2etB",
        "outputId": "ba40f447-24ea-4bae-d7af-30dbf0043e93"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygraphviz\n",
            "  Downloading pygraphviz-1.13.tar.gz (104 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pygraphviz\n",
            "  Building wheel for pygraphviz (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygraphviz: filename=pygraphviz-1.13-cp310-cp310-linux_x86_64.whl size=168498 sha256=dda629724aacb2340be2d172bed30f0fa20acbb6c272f994ae749cd762ed08ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/10/6c25add1fffc368b1927252bf73b63fcb938de8f4486e23691\n",
            "Successfully built pygraphviz\n",
            "Installing collected packages: pygraphviz\n",
            "Successfully installed pygraphviz-1.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "gUVczlZwc-58",
        "outputId": "bbb4957a-6ec2-460e-ce14-3c7e676eda8e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7sAAADFCAYAAACVWuXaAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdd1xV9RvA8Q/TAbhz4MxZ/ty4EMQRORByBZiVpaamJm5NM9PcI3GVMxuWCy0VcIEbxAWuzJGamCkqblFZ9/7++MZStsDhwvN+ve4L7rnnfs9z7z33nPuc7zLS6/V6hBBCCCGEEEKIvMPLWOsIhBBCCCGEEEKIrCbJrhBCCCGEEEKIPEeSXSGEEEIIIYQQeY6p1gEIIYTIW/bs2cPdu3e1DkMYsNjYWCpXroytra3WoQghhDBgkuwKIYTIMvPmzWPkyJFahyHygLfffptdu3ZpHYYQQggDJsmuEEKILOHp6cnIkSOZ0RU+76B1NMIQnboOjp5gBFhaWmodjhBCCAMnya4QQohX5unpyYgRIyTRFZkWl+jWLgclLcHUVH6iCCGEeDUyQJUQQohXIomueFVxie6b5cB3CJjKrxMhhBBZQE4nQgghMk0SXfGqEie624aAZQGtIxJCCJFXSBshIYQQmbJ7925GjBgBwLjf1U2IjKpUAmqVgR0eUNhc62iEEELkJZLsCiGEyJS46YXW99M4EGHQ3FfAzK6S6AohhMh6kuwKIYR4JW6N07detS/gSjhUKQm/D4QGFV9eZ+EemOQNMToIGA31KiRfVuPpMPddaF0z9W22XwC7/oT7nlCscPriTI1XMPT9GdxsYGUvmL4dpvjC8+ik69lVg4Axr769jKo1Ee4+gfB5Lz/msQ6+2w/r+sG7jTJXftzrjYpRn1/wF0kfv/EA3vKEq3fBqiB0rg+L34MCqfzacF8BJtKpSgghRDaQZFcIIUSOODQWyo8F78+gjnXC8g9Xweo+6n+PtmBuCqWtUk50AY6PT982dw4F04EpP+6xDhb2SF9ZAK42EHoXzoep++M7wqNn6v7mQaDXQ1QsTNya/jKz0s6h0Hha8o8t7AEXb79a+S++3hc9jQL3xqr/9u3H0GEhfLcPhju+2naFEEKIzJBrqUIIIXJEmSLw1huwKSRh2dGraoCiy3cSlu2/CE51Mr+dqJik20iJTg9bT2e8fONUzpxGRqoWc1a3jJebVUxNtNt29dIwyQUKmqm+uC2qwqU7aT9PCCGEyA6S7AohhMgx7zeD304k3L8aDgMcYM1RdT8iEooWUskSwNRtUHY0lBgO07apZYv2gtlAWHcsoZxp26DMKCgwGMqMVklnYl9uVeWUHQ0H/1LLui1RtbRGA2BjSMrbA1iwG14fr8r/2if115i4VnfIOlX+SC9VbrnREHRFPXb7MdjNhoKD1fKrd1OOYcAvqpzK41QMFkOg7mQoPQrMB4HTIpW8A9yLUM8v9BnYzkqohU5OSq83q1y7B/VTqaEXQgghspMku0IIIXJM1wbw1y1VkxsVA3qgW8OExHXnn2odgCN/q+UnJsCfk+Hbfep5Q9rAW28mlHnpNszYAYFjIPwblVz9cy/pdqe8A9dnqRrjdcfVsqUfqFpY/TLVhzWl7R29CuM3w6994clCGNXu5de15ZRKRo0GqCQ2zqIeqj/ql53gn5kq7o3B6rGNwVCzDDxcAFsHqyl3Uoph2X+xbhsCjxeo5tQNKsKlqermfw7+faDKtSgAZyfBnW9Uc/FBa5L/LFLaVla5Eq5i+qBZ1pUphBBCZIT02RVCCJFjrAqCS31Vu1uzjBpgqkwRKGEBIddUreeMrmrdo1fh7A2wTjTQ0/kwqPZa0jLN/mu2G9e8WK9PvimvqTHULpdyTWdK2/vjBrSrDS2qqWUFkzlzdq6f0If1K+/kyzczUcnn9fvqfotqMGGL+r9PCyhlCWuPpf6ayxRRfZrrWKvlRQqqWwkLiIxWr9vECEpaqPUHtQaHORl7vS++v5kREan6Q28cIKMsCyGE0I4ku0IIIXJUz6Ywc4f627m+WvZuI/j5sGrCbJooaXWpp2o9U1O5JHxkC3UmqYTX8Q342DbjcaW0vVPXUx9N+EWTXdK3XoOKcHEKrD8OPVbCN++m/zWnV2Q0FEoh2czqbcV5/BwGr1WjZb9RNmvLFkIIITJCmjELIYTIUR3/p5oyWyRKwro3gmUH4O1EzZObVIE9F9SAVVExcOcxxOpeLu/Rc1VDGT5PNTPePEg15U2LERAdq6bLeR6d8vZsKqnpi05dhwdPIfha6uXq9LDhOOw+n/p6x66q7QxwgA+bqZrt9L7m1OhRz330HBbsUYOCJScrtgXqOVO3qT7H95+Cx3qY3kUSXSGEENqTml0hhBA5ytxUzc3rXC9hWYXi8E59NT9tHNuqMLodvLdSJW7dG6q5bUduUH1Uj1+FskWhQQW49UgN2gSqFrZTXVjfHwavUcmY23JY4K7miY2MgY9bqPIbVoKaE2FOdxjYKvnttf+fWr/tPDVwVq0ycPgKfNhcNbtesEcly0YDkr7O4+NVU9647c95F2btVAl2L1u4EKbmAY6IgnrlYe0najTj5GIY/KuKu+08WPK+Si4jY8ChJvj9qV7/xz/ClsGqeXjJEWqQrja1VH9fgGH/vW/BoarJdOuayW8rtcGcZ+5I+fUOaqUGGvvxkLrFafY6HP48gzuJEEIIkQWM9Hq9XusghBBCGJ4NGzbg7u6Ofpm2cdx4ALN3wXw3df/RczXX7NpPwKaytrGJtBkNgPX91AWQOG7LgaqubNiwQbO4hBBCGDwvqdkVQghh0LafVU1wHz9XIx/vuwAPn8HrpbSOzPBcvw8VU6iF7Wunan6FEEIIQyHJrhBCCIPWpYGaxsd6jGoiXL8CrOunRigWGVOhOJrX1AshhBBZRZJdIYQQBq2kBWz30DoKIYQQQuQ2MhqzEEIIIYQQQog8R5JdIYQQQgghhBB5jiS7QgghRA7wWAemA2FjSPrW330eqk+Awp/BRz+o/sgp2XsB3piY/uUplX3rEdT8EgoMhlIjoNcP8Cw6ffEKIYQQuY0ku0IIIUQOWNgDHN9M37oRkdBjBczqBpenwbkw8PRPft3Fe2H9cbh0J33LUyv7WbSaAujBfAiZAMeuwrd7M/QyhRBCiFxDkl0hhBAil9l7AUpbQfdGUK4oDG0LXsHJr/tZG5jglP7lqZVdpSRM7QyFzKBSCXCoAdcfZN3rEkIIIXKSJLtCCCEMwu3HYDcbCg6GcqPh6l21/IdDqklugf+Wn/lXLV+8F6qMBysPeHt+wvoDfgGjAbD0ANSaCEf+hqnboOxoKDEcpm1LO5aKn4Pxp9Bvtbo/epNqorzioLpvOwvMB6nyFqVSM9p8JqwKfHn5hVtQ7bWE+zXKwMVbaceVHukpO0YH58Ng/0XoXD9rtiuEEELkNEl2hRBCGISNwVCzDDxcAFsHg2UBOPkPfLYWfvgIHi2Axe9BrA6CQ+Erb9g8EK7PgvLF1HoAyz4AE2No9jos6QmHr8C6Y3BiAvw5Gb7dB5fvpBoKfsNU7eckF3V/ghN8bAv9Wqr7QWMh6jv4qXdCApycw59DH7uXlz+NgoJmCfctzNWyrJCesqtPgLqTwaU+tKyRNdsVQgghcpoku0IIIQxCi2qw5SR8+is8j4ZSlnDgL2hTSyVkBUxV09wGFSHgErSqqf4vWgiGtIHAy0nLe70ktH0DjI3h7A2wHqNqhm8+VLWaqXmjLLjUgxnb1f1v98Gg1ur/O4/BaREUHQpdl0BUTMZfq0UB9RrjRERBScuMl5PZsv+aovrz3n2i+vcKIYQQhkiSXSGEEAahQUW4OAUaV4YeK1VtrE4PxkZpP1cPpLSaXq8SV/2yhFunummX+WUn+D5QNQG+EAaNKqnlqw6puK5MVzW8mVGrDPx1O+H++TCoXCJzZWWmbDMT1Wd3gAPsOJs12xVCCCFymiS7QgghDMKxq6qWdIADfNgMQq5By+rgfw62/aGa4j5+Dg+fqZrefRdUM+eHz1T/3VY1ky+3SRXYc0H1T42KUTWzsbq04/mfNTjVAUdP6GufsDwyGgqaqprmxEllRrSpBeFP1DRFNx/Cwj3QtWHmyspI2b8cga2n1FREdyNgZQDUr5A12xVCCCFymiS7QgghDML5MKgzGSyGqHli+7cEm8rg6abmsC0+HNrOg+v3VS3r5HfgnW/VYFJhj9TUP6AGqIrVqcGhHj4D26owuh28txJKjIBRG1XNbHp82Qkql1SjFsdxb6IS8SrjYN9FuHgbZu+EYRtUYj7wV7UcoNkMVTv8osLmsPYTGLMJqn0BdaxhhKN67EkklB+b0Cy71w/QYIp6TeXHwhebU1+eWtkVi6u+zRZD1GM3HsDqPun/jIQQQojcxEiv16fzlC6EEEIk2LBhA+7u7uiXaR2JMGRGA2B9PzW/bxy35UBVVzZs2KBZXEIIIQyel6nWEQghhBC5zfX7qkY4OX3tYGWvnI1HCCGEEBknya4QQgjxggrFkRprIYQQwsBJn10hhBBCCCGEEHmOJLtCCCGEEEIIIfIcSXaFEEIIIYQQQuQ5kuwKIYTIUyIioeNCKDhYzRnbeHrCVD+vKivLSs2l22qU4sQ3k0/V/LgAlh5JH/vlSPLlLNoLlcepqYQ+WAXPo7M/diGEECK3kAGqhBBC5Cm/HIFyReHZYjAygnfqJ33cY13CnLtpeXHd4+OzLs60rOylRn4GePQchq2HUpbq/if2MN8t9ecHXoYJm2H3CChTBBw94Rs/+MIpe+MWQgghcgup2RVCCJGnnA+Dqq+pRPdFOj1sPZ2+cjKyblarXjoh0QVYsh/62mesDP9z0K42NK4MFYvDpw6qplsIIYTILyTZFUIIkWd8u08lhl9uUc17p/iC2UBYd0w93m0JhN5Vj20MSb2sF9ddtDdpWQN+UY9VHgcFBqumwnUnQ+lRYD4InBaphBlg6jYoOxpKDIdp2xK20XwmrApMPY7IGDh8BeyqJSxbf0xtr9gwtZ2rd19+XlQMFDJPuP8/a7h8R/1/+zHYzVZNvcuNTv75QgghhKGTZsxCCCHyjMGt4dxNKFsUJvzXXDfwcsLjSz+AHWfh+bdpl5Xcur5nEv5f9gH8FATbhkCN0tD/F4jVwbc94cFTqD4B/n0ANx6oBPnEBFXb3Ggq9GgC1V6Dw5+nHcfPQfBek6TLDn+ummrfjYAh62Dgr7DdI+k6zavC96vhwi2oXELFEhmjHtsYDDXLwJ4RcPo6WBZIOw4hhBDC0EiyK4QQQryCMkXA3BTqWKsm1EUKqlsJC4iMhqNX4ewNsB6T8JzzYSrZTYtOD17BLyeylUuqv+WKwsi3ocOCl5/rUg96NlW1x3o9VCiuYgJoUQ0mbFH/92mR0BdYCCGEyEsk2RVCCCGykV6vEs+tgzP+3N9PQMc6YJJKp6OY2KTNlROb56puAD8fVuUBNKgIF6fA+uPQYyV8866qbRZCCCHyEumzK4QQIt8wAqJjVdPitKbhyci6qWlSBfZcgP0XVT/aO49Vc+f0WHpAjbycWMAl1Tc5KgbCHsGcXdCp7svP3XdRNbOOjoWQa6r/8sBW6rFjV9XzBzjAh83U40IIIUReIzW7Qggh8owtp+DHIPX/4+eqj6r/OTh+VfXjbVkdGlaCmhNhTveE5C85pSyTrnvhVtKy1h1T5bedB0veV4NQRcaAQ03w+xNuPYKPf4SAMTC6Hby3Uk0h1L2hmlbIBGg2A/o7JB15Oc7eC1CvPFgVTLq8tJWaQmiEl2ou7VQH5r6rHnsSCbUmwob+ULQQjPsdPvkZKpaAiZ3U6MygmlG3XwARUWobaz95tfddCCGEyI2M9Hq9XusghBBCGJ4NGzbg7u6OfpnWkQhDZjQA1vcDt8YJy9yWA1Vd2bBhg2ZxCSGEMHheUrMrhBAiX7p+HyqmMBpyXztV+yqEEEIIwyXJrhBCiHypQnGkVloIIYTIwyTZFUKIfCwqKoqjR49SqVIlrK2tMTWV04IwPHfv3iU0NJRr164RGhrKoEGDMDMz0zosIYQQGpNfNUIIkY+ZmZnRpUsX7t69i4mJCa+99hqVKlWiZs2aVKxYkUqVKlGpUiUqV65M5cqVsbSUCVlFzoqNjeXmzZuEhoZy9epVrl27xrVr17hy5QqXL1/m33//5fnz5/HrV6xYkaFDh2oYsRBCiNxCkl0hhMjHjIyMaNq0KTt27CA2NpawsDDCwsI4fvw4pqam6HQ6YmJi4te3tLTE2tqaatWqETe+4Yl/oGFFrV5B8ryCoe/P4GaTub63jaerEY5b18z62F6Vxzr4bj+s6wfvNkp7/d3nYcAvagol1//eDzOTlNd/9BzqTobdw6F66YTley/AwF/h/Nev/hpSc+wqHL0KEaF7KV++PLdu3SI2NhYAY2NjzMzM0Ol0REe/PB+UiYkJdnbJDG0thBAiX5J5doUQIp+zt7d/qcmnTqcjKioqSaIL8OTJEy5evMj27dvZvXs3AHWscyzUdHO1UVPtpJfHuqT3j4/PnYkuwMIe4Phm+taNiIQeK2BWN7g8Dc6Fgad/6s/52gduPky6bPFeWH8cLt3JXMwZ0aQKVCwOERER3LhxIz7RBbVfRkZGJpvogrp4Y2trm/1BCiGEMAiS7AohRD5na2tLVFRUutY1NjbGyMiI9u3b4+npCaReS6gl43Se4XR62Ho6e2PRyt4Lal7e7o2gXFEY2lbVeqdk6yloWgXMX2j39VkbmOCUraEmUa4odOzYka+++gozM7N09yWPiYmhRYsW2RydEEIIQyHJrhBC5GMXLlzg4sWLGKcjMzQ1NaVEiRL8+OOP7Nixg9deey3L4vj0VzAdCHazodBn0HwmXLylHhvwi5qLdekBqDURjvwNU7dB2dFQYjhM25ZQzoLd8Pp4KDBY1VCmR7clEHpXbWNjCCzaC2YDYd2xpNuvPE6VazFENfMtPQrMB4HTIpUwQ8pxpaTi52D8KfRbre6P3qTehxUH1X3bWWobJYaruFLSfCasCnx5+YVbUC3Rx1SjTML7+qI7j1WT58Tz3abl9mP1mRUcDOVGw9W76X9uWkxMTJg0aRJ//PEHdnZ2GBkZYWRklOZzVq1axZo1awgNDc26YIQQQhgk6bMrhBD5xLNnzzh+/DiHDh0iMDCQoKAgwsPDKViwIMWKFePevXvJPs/U1JTY2Fh69+7N3LlzKVKkSJbHNt8Nlh2A9f2gaCHwWA/DNsC2IbDsA/g+EJq9Dkt6wuErKhE9MQGMjKDRVOjRBO5GwPjN4DdMNYWdtROuhqe97aUfwI6z8PzbhGW+ZxL+X/YB/BSkYqlRGvr/ArE6+LYnPHgK1SfAvw9Un9jk4qqWyjUBv2FgMw0muaj7E5zgfgT0a6nuB41Vf71PwxebYUib5Ms5nMJ8wU+joGCiFuoW5mpZcr7yhimdU441ORuDoWYZ2DMCTl8HywIZe3561KxZk71797J69WqGDRvGkydPUmzGXLFiRY4fP87KlSuJjo7G2tqaFi1a0KJFC2xtbWnUqBHm5uZZH6QQQohcSWp2hRAijwoLC8Pb25vPP/8ce3t7SpQogYODA/Pnz0ev1zNq1CgOHjzI/fv3cXd3TzYJMDY2platWhw+fJjly5dnS6KbWNmiYFUQBrWCoCtJH3u9JLR9QzVPPnsDrMeo2sSbD+F8mGqy2642tKimmlYXzOLLuWWKqOa9dayhgCkUKQiVSkAJC4iMVoMqJRdXat4oCy71YMZ2df/bfTCotfr/zmNVa1x0KHRdAlExKZWSMosC8DxRXhgRBSWTGVB79WHo8D8oaZGx8ltUgy0nVc3882golU2DdRsZGdGrVy8uX77MgAEDMDIyeqlps7m5Oa6urhw9epSIiAiOHz/OmDFjMDIyYsaMGdja2mJpaUnjxo0ZOnQoP//8M9euXcuegIUQQuQKUrMrhBB5QGxsLOfPnycwMJCAgACCg4P5888/MTExoVatWtjY2NCrVy/s7OyoXbv2S81BmzdvzrJly+Lvm5mZxTcjHTVqFCYmOdsxNyoWTFJosarXqwRx6+Cky09dV0moVlKKKy1fdlKjP3u0hQthML6jWr7qkGoefWU6XLkDH67KeEy1yiQ0iQaVfFcu8fJ6qwJh38Wky2p8Cd6DwbleyuU3qAgXp6jBq3qshG/eVbXZ2aV48eIsWrQINzc3PvnkEy5fvhw/gFV0dDTNmzcH1P5rY2ODjY1N/DREN27ciP9+BAYGsnjxYnQ6HeXKlcPe3h47OztsbGxo1qyZzNErhBB5hCS7QghhgB4/fsypU6eS/Hi/f/8+lpaW1K9fHxcXF2bOnIm9vT3FixdPszxbW1t0Oh1GRkbo9XqcnZ1ZvHgx1tY5O9RyZDQ808PCPdC6VvLrNKmimivvvwi2VeHhM1W7alMJ5u5SSW/lEhB8TTXbTYsREB2rmiGXsEja7DcjUorLJI02VP+zBqc64OgJv/RNWB4ZrWqnC5jCX7czF1ObWhD+RPVFtqum3tfkpivaOzLpfUsPODkh6dRDyTl2FcoXgwEO8M89CLmWvclunJYtW3LmzBnmzJnD119/TWxsLLGxsTRr1izF51hbW+Pq6oqrqyvw8ndo8uTJSb5DcQmwnZ0dJUokc4VACCFErifJrhBCGIAXa6VOnDiRpFbqq6++wt7enoYNG6ZrsKkXVa9eneLFi2NhYcGyZctwcsrBoXcTKT9WJZ521WDVR2rZgP/6yDafCUfGqURydDt4b6WaE7Z7QzV3bPv/wcctoO08lbDWKqP6937YHFqlMo1QKUtoWAlqToQ53dWgTv7n4PhV1ax63TGIjFHlLnlfDUIVGQMONcHvT7j1CD7+EQLGJB9XeurEv+wEQ9aBQ42EZe5NYGUAVBkH3RrBxdsweyfceKjiCw5VsbeuCc1mQH8H6PvCFLOFzWHtJ6qfcdhDNfjUCEf12JNINeDXhv7q/U5Nrx9g2xn1OZQfCx/bwrQuqqa4/QLVPLpeebWtnGJubs4XX3zBe++9x6effsrZs2cpX758up9vZWWFvb099vb2jB07Nr51RHBwMIGBgXh7ezN79mz0ej1Vq1bFzs4uPgFOrnWEEEKI3MdIr9frtQ5CCCFEgujoaEJCQggMDOTgwYMcOnSI27dvU7BgQRo3boytrS12dnbY2tpSunQaVW8Z4OXlRadOnShcuHC61t+wYQPu7u7ol6W9blqeR6tRmGOXgrHkEPmK0QA1MFniUaDdlgNVXdmwYUO6yzl9+jT16qXS5joTbt26RVBQEIcOHSIoKIjjx4/z/PlzSpcuTYsWLWjZsiV2dnY0atRImj4LIUTu4yU1u0IIobEnT54QFBREQEAABw8e5MiRIzx9+pTSpUtjZ2fH2LFjsbW1xcbGJltHko1r3qklnT7rk93r99UUP8npa6dqX7OLltvOb7I60QUoU6YMXbp0oUuXLgBERUUREhLCoUOHOHjwILNmzeL27dsULlyYZs2axdf8tmjRAisrqyyPRwghRMZIsiuEEDns1q1bHD16NL5Z8rFjx4iKiopvkjx9+nTs7e1p1KhRvmkq2esH9bf+1xAyIWsHmqpQnCypfTa0bYusZ25uTvPmzWnevDkjRowA4MqVK/HdCzZt2sTUqVPjRzGPS34dHByoUqWKtsELIUQ+JMmuEEJks7j+tv7+/gQEBHDu3LkkP4b79+9Pq1atqFy5stahamZDf60jECJzqlatStWqVenVS1XTv3gx68cff0xyMSuu729m+9cLIYRIP0l2hRAiC704BdD+/fu5du0aZmZm1KtXL8OjJAuR35z8B/ZcNMW1SUmtQ8mUMmXK4OLigouLCwARERGcOHEi/pgwadIkHjx4QJEiRWjatGl88mtvb0/BggU1jl4IIfIWGaBKCCFewdOnT+MHkwoICCAgICDZH7ItW7akQIECWoebpbJygCqRfyUeoOrUdXBcYErt+s3w3bYDS0tLrcPLcmldEIs7ZrRt25aSJQ0z4RdCiFzCS5JdIYTIgPDwcAICAjhw4ACBgYGEhIQQExNDlSpVaNmyZXwNzZtvvpnn+9tKsiuyQlyyW6ts3k90U/LXX38lmVrs/PnzGBsbU6dOnfiLZQ4ODjk+77UQQhg4GY1ZCCFSk7j/nb+/f/z8tlWrVsXR0ZHBgwfTsmVLXn/9da1DFcJghd6DwetNebNe03yX6ALUqFGDGjVq8PHHHwNw586d+KnHAgMDWb58OTExMVSvXh0HBwdatWolg14JIUQ6SM2uEEIkcvPmzfjmyHE1t8bGxjRo0CC+eeFbb71FiRIltA5Vc6nV7EbHgplJzsf0qs6Hqdjrltc6kozZcRbsqoGVgXX5jIqBAoPBqpAJDWyasW37znyX6KZH4u4ScQPdPX/+/KVBr/LTCO5CCJEO0oxZCJG/vThS8p9//ompqSn169eP/wH59ttvU6xYMa1DzTUiIiK4du0aa9euZcqUKVqHI/IAMzNT3nyzNpUrV6Zy5cqUK1eOChUqYG1tjbW1NRUqVKBIkSJah5lrxMTEcOrUqfjj1sGDB3n48CGlS5emadOm2Nvb4+joKCM+CyHyO0l2hRD5S+I5MXft2sXVq1fjB4ZxdHTEzs6OVq1a5fsf1ocOHeLMmTPcuHGDf/75h3/++YfQ0FBu3LhBRERE/HrGxsaYmKgq3OjoaADee+89unbtqkncmREREcGSJUs4evQoVatWZebMmVqHlGFPnjyhb9++GBkZ4erqSteuXQ0myYmMjGTp0qUEBgYCYGpqirGxMTExMeh0uvj1ChYsSNmyZbG2tqZatWqUL18ea2trnJycqFatmlbh5wqxsbGcPHky/ti2e/du7t27h5WVFc2aNYs/tjVt2hRzc3OtwxVCiJwiya4QIm+7cuVKfO3Hvn37+OeffyhcuDANGzaMr/2QKT9e5unpyYgRIzA3N0en0xETE5Ou582cOZOxY8dmc3RZ5+DBg7i7uxMeHk5MTAwTJ05k0qRJWoeVKc2aNePo0aOYmJjQuHFj1q5dazB9yZ8+fYqTkxOBgYHp2tfiLrAYGxvz999/U768gbU7z2aJR3z29/dn7969hIeHY2Fhga2tbQfhFh8AACAASURBVJ4eJV4IIRKRZFcIkXek9APP0tKS5s2byw+8DIiIiKBChQo8ePAg3c+ZNm0a48ePz8aosk5MTAxTp05lypQpGBkZERsbC0BQUBDNmzfXOLrMmTZtGpMnTyY6OhpTU1PMzc3x9PSkf//+WoeWLpGRkXTt2hU/P790JbxmZmb06tWLlStX5kB0hi/xhb+46Y5evPBnZ2dHoUKFtA5VCCGyiiS7QgjDFRMTw/Hjxzlw4AAHDhwgICCAhw8fUrJkSezt7WndujUODg7Ur18/viZIpN/kyZOZOnVquhKP6dOnM27cuByI6tVdu3aNHj16cPTo0fgkF8DKyor79+8b7L4SEhKCjY1NkmVGRkZ07dqVFStWGMSgalFRUXTr1o2dO3emud8ZGxtz/vx5atSokUPR5S0XL17k4MGD7N+/nwMHDhAaGoq5uTlNmzaNH/HZzs4OCwsLrUMVQojMkmRXCGE4dDod586di6+59fPz48GDBzIoSza5d+8eFSpU4NmzZymuY2RkhKenJ0OHDs3ByDJv06ZN9O7dm+fPn8f3MQbVLLZr1654eXlpGN2r0ev1lC5dmvDw8CTLzczMKFmyJGvXrqV169baBJcBUVFRdO/ene3btye5GJGYmZkZnTt3NujPK7cJDQ3lwIED7N+/n4MHD3Lx4kXMzMxo1qwZbdq0oU2bNtja2kqXDyGEIZFkVwiRe+n1es6ePcuePXvYu3cvBw4c4N69e5QqVYrWrVvH/wB78803tQ41z4mKimLdunVMmDCBsLCwJIlhHCMjI+bPn4+Hh4cGEWbMs2fPGDNmDIsXL8bIyIgXT30mJiYsX76cPn36aBRh1ujTpw+//PLLS5+XiYkJOp2OIUOGMGfOnFw/SFFUVBSurq74+vqmmPDWr1+fSZMm8c4778jFrWxw69YtDhw4kOxI9Y6Ojjg6OkqXECFEbifJrhAid0k8WvK2bdu4fv16fJ/buB9YUnObfR49esTSpUvx9PTkwYMH9OjRgzVr1hAVFZVkPSMjIxYsWMCQIUM0ijT9goODcXV15fr168km7XGuXbtGxYoVczCyrLdx40bc3NxeSubjmJiYUKdOHdavX0+tWrVyOLqMiY6Oxs3NDW9v7yQJr6mpKc2bN6dUqVJs3bqVN954g7Fjx/Lee+9hZmamYcR5W9wc5P7+/vEj2b/Y59fBwSHXX0gRQuQrkuwKIbSV+AfUzp07CQ0NpXDhwrRo0SJ+QKlWrVrJj9hsFh4ezuLFi1m0aBFRUVH06dOHsWPHYm1tzYABA/jxxx/jE14jIyMWLVrE4MGDNY46dXq9nrlz58YPmpVaH9CaNWty4cKFnAot28T1WU+pNhRUsmhqasrChQvp169fDkaXcbGxsXz44Yds2LAhyWs6cOAALVu25NKlSyxatIhly5ZRtmxZhg8fTr9+/ShcuLCGUecPieco37FjB9euXYsf7TlusKtmzZrJsVsIoSVJdoUQOUuaxuUuYWFhzJ8/n8WLF1OoUCEGDx6Mh4dHksGMrly5Qo0aNdDpdBgZGbF06VKDGOE3PDwcd3d39uzZk+p65ubmeHh4MGfOnByKLHvZ29tz6NChVGt39Xo9EydOZMKECbl+QK6YmBh69uzJb7/9hl6vp3Hjxhw5ciTJOqGhocybN4+VK1dSuHDhZPdjkb0Sj/a8d+9eaZUjhMgNJNkVQmSvO3fucPjw4fgagJCQEExMTJIktzLPbc67cuUKCxYsYPny5RQrVoxhw4bx2WefpTjyas+ePVm3bh0rVqygb9++ORxt5un1elasWIGHhwexsbEp1u76+fnh6OiYw9Flj1mzZvHll18m22TbzMyM1157jXXr1tGyZUsNosuc2NhYPvjgA9atW4e3tzfOzs7JrpdcC4UxY8bIPLwaiEt+/f392bNnD3fv3sXKyopmzZrFH/sbNWqEkZGR1qEKIfIuSXaFEFnryZMnHD58OP5HzokTJzAyMqJBgwbxzZLbtWtH0aJFtQ41Xzp9+jRz585l7dq1VKpUCQ8PDwYMGJDmxYY//viDY8eO0bt37xyKNGudO3cOV1dXzp8//1IT3wIFCvDw4cM805rgzJkz1KtXL8myuEG5unbtyqpVqyhWrJhG0WVebGwss2bNYty4cWkmSE+ePOH7779nzpw53LlzB3d3dyZMmEDNmjVzKFrxosTJr7+/P/fv36d06dLxUxzZ29tL8iuEyGqS7AohXs3Tp0/j+9zu3buXEydOoNfrqVevHm3atKFt27Y4ODhQpEgRrUPN1wICApg1axa+vr7UrVuXkSNH8v777+f6JqxZyc3NjeDgYK5evYqRkRGxsbEYGxvz9ttvs2PHDq3Dy1LlypUjLCwMULW55ubmWFpa0rdvX6ZNm6ZxdDknblTxadOmcenSJZycnJg0adJL8xGLnBUbG0twcDB79+5l3759HDx4kIiICMqXL0/btm3ja36tra21DlUIYdgk2RVCZIxOpyMkJAQ/Pz/8/f0JDAwkMjKS2rVr07ZtW9q0aUOrVq0oWbKk1qEKVJI7efJk/P39sbOzY+zYsTg7O+e72pPVq1fz0Ucf4evri6WlJe+//z5hYWHodDo8PT0NYlTpjBgwYAArV64EwM7Ojl9//RVfX18GDx7M7t27DWK+3ayk0+nw9fVl8uTJBAcH4+joyOTJk2nRooXWoQnUyNtHjx5l37597NmzJ8l5JS7xbd26NVZWVlqHKoQwLJLsCiHSllzfq7jmZ46OjnTo0IFKlSppHab4T9wP+ylTpnDs2DHs7OyYPHkyb731ltahaeL69evUq1ePDz/8kAULFgBqiqUhQ4bw888/c+HChTzXvHXLli24uroydepURo0aFT8oUOfOnQkJCeH06dMUL15c4yi14e/vz1dffcWhQ4fy9QWg3OzZs2fx4zy82B1GBjIUQmSAJLtCiJc9fvyYI0eO4O/vj7e3N3/++Wf8dEAysEjuFR0dzdq1a5kxYwYXL17EycmJiRMn0qRJE61D04xOp8PR0ZGwsDCCg4MpVKhQksd37dpFu3btNIou+zx58oSLFy/SqFGjJMvDw8OpV68ednZ2eHl5aRRd7pBc0/6ePXtiamqqdWjiBeHh4ezduxd/f3/8/Pz4+++/5ZwkhEgPSXaFEGpqj1OnTsVfRd+3bx96vV6uohuIyMhIfvrpJ6ZOnUpYWBg9evRg/PjxvPHGG1qHprm5c+cybtw4Dh06lK+T/sR27dpFhw4dWL16Ne+//77W4Wgus4O2Ce0kbm20e/du7t27J62NhBDJkWRXiPwq8Y+FHTt28PjxY6pWrRqf3Do6OubbZo6G4vHjx6xatYpZs2bx6NEj+vbty6hRo6hYsaLWoeUKZ8+epXHjxkycOJFx48ZpHU6uMnToUH766SdOnjxJlSpVtA4nV0g8HVfRokX59NNPGT58uIwcn8vpdDpOnDgRfz4LCAjg+fPnSc5nb7/9tkGOQC6EeGWS7AqRX4SFhXHw4EH8/f3x9fXl33//pVSpUrRp0wZHR0fatWsnP3oNxO3bt/nuu+9YsGABsbGx9O7dm3HjxlG2bFmtQ8s1IiMjadq0KZaWlhw4cCBfjTqdHnHvj5WVFfv375f3J5Fbt26xZMkS5s+fj16vZ+DAgYwePVoG3TMQ0t9XCJGIJLtC5FUvzncb11/Rzs4u/oTfsGHD+IFrRO4XGhrKvHnzWLFiBVZWVgwcOJBhw4ZJjUUyRo0axdKlSzlx4gQ1atTQOpxc6ezZszRp0oTx48czYcIErcPJdR49esSSJUuYM2cOz58/l5YTBkr6+wqRr0myK0ReERMTQ1BQEH5+fvj5+XHs2DH0ej0NGzaMP6Hb29tLPzQDdOnSJebMmcOqVasoX748w4cPp3///i8NtiSUgwcP0qZNG1asWEHv3r21DidX8/T0ZMyYMRw8eJDmzZtrHU6uFBERwcqVK/nmm2+4desW7u7u0ifegP3111/xF4H37t3L/fv3KVeuXHxz53bt2lGmTBmtwxRCZA1JdoUwZLdu3WLnzp34+Pjg5+fHgwcPKFeuHPb29jg6OuLs7Iy1tbXWYYpMOnnyJPPmzWPNmjW8/vrrjB49mj59+shosal4+PAh9evXp0GDBmzevFnrcHI9vV6Ps7Mz58+f5+TJkzKPaSqioqJYt25dktHOv/zyS5o2bap1aCKTYmNjCQ4Ojk9+AwMDiY6OpmHDhnTo0IH27dvTokULOeYKYbgk2RXCkMTExHD48GF8fHzw9/cnJCSEggULJmmabGNjo3WY4hXFTYni4+NDgwYNGD58OO+//770q0yHDz/8kF27dnH69GmpnUmn27dvU69ePZydnVm5cqXW4eR6cfNYT506laNHj8bP1evi4qJ1aOIVPX36lEOHDiWZds/CwgJbW1ucnZ3p0qULlStX1jpMIUT6SbIrRG73999/4+fnh7+/Pzt37uTRo0dJRpns2LEjlpaWWocpXpFer8fHx4fp06dz+PBh+QGdCb/99hvvvvsuW7duxdnZWetwDMqWLVvo0qUL69evx83NTetwDEbiC1ONGjVi6NChfPDBBzIWQh6R2qwFzs7OvP3229I1SIjcTZJdIXKbF0eSDA4Ojh9Mw9nZmc6dO8uoyXlIXC3RpEmTOHHiBJ06deKLL76Q/pMZdOPGDerVq4erqytLlizROhyD1L9/fzZu3MjJkydljtIMCgkJYf78+fz666/Url2b0aNH07NnT2n+modIyyohDJIku0LkBomvHm/fvp0nT55QtWpVnJ2dcXFxkWkS8qC4/n/Tpk3j0qVLODk5MXnyZBo1aqR1aAZHr9fTqVMnLl68yMmTJ6WlQyZFRERgY2ND2bJl2bNnj9ROZsIff/zB7NmzWbt2LRUqVGDYsGH069ePwoULax2ayGJhYWHs2rUryZgZiVtdtW/fniJFimgdphD5nSS7Qmghcb+gLVu2cP78eSwtLWndujUuLi507NhRprfIo548ecL333/PnDlzuHPnDu7u7kyYMIGaNWtqHZrBWrhwISNHjpQRhbNASEgItra2TJ8+nZEjR2odjsG6evUqnp6erFy5EgsLCwYNGsTQoUMpXry41qGJbBAbG8vJkyfj+/oGBQVhbGxMs2bNcHFxkemNhNCOJLtC5JQrV67g7e2Nj48PBw8eJDIyktq1a8efCFu1aoWZmZnWYYpsEh4ezuLFi1m0aBFRUVH06dOHMWPGUL58ea1DM2jnzp3DxsaGsWPH8tVXX2kdTp4wY8YMJk+ezOHDh2nQoIHW4Ri0O3fu8O2337Jw4UJiYmLo3bs3Y8eOlVHy87g7d+6wb9+++OT35s2blClTBgcHh/gWW3LhQ4gcIcmuENklIiKCPXv24OPjw/bt2/nnn38oVaoUbdq0kWmB8pFbt26xZMkSPD09MTc3Z/DgwXh4eFCiRAmtQzN4MTExtGjRAp1OR1BQkFwsyiI6nQ5HR0fCwsIIDg6W+ZyzwOPHj1m1ahWzZ8/m7t27uLm58eWXX1KjRg2tQxPZTKfTceLEifiuSvv370en09GgQYP43wItWrSQbgNCZA9JdoXISmfOnMHHx4dt27Zx+PBh9Ho9zZo1o0OHDnTo0AEbGxs5oeUTf//9N/Pnz2f58uUULVqUTz/9lBEjRkgfriw0fvx4FixYQEhICLVq1dI6nDzl+vXr1K9fn549e7Jo0SKtw8kzIiMjWb9+PVOnTuXy5ct0796dr776iv/9739ahyZyyL179/Dz82PHjh3s3LmTmzdvUrp0adq3b4+Liwvt2rWjaNGiWocpRF4hya4QryIyMpJ9+/bFN08ODQ2lTJkyODk50aFDBxwdHaUGL585c+YMc+bMYe3atVSsWJGhQ4cyYMAAmZ4iix06dAgHBwcWL17Mp59+qnU4edLGjRtxc3Njy5YtMgVWFtPpdGzatInJkyfz559/0qlTJ8aPH4+tra3WoYkcpNfrOXXqFDt37mTbtm0EBgZibGwc39zZ2dmZ6tWrax2mEIZMkl0hMio8PJy9e/fi7e3Nli1bePToUXzfW2mOlH8FBgYyc+ZMfH19qVOnDqNGjZKpR7JJREQEDRs2pGrVqmzfvl0GfclGvXr1YseOHZw+fZqyZctqHU6eEze/9owZMwgKCoqfX9vZ2Vn263zo3r177N69O/4C+v3795PMzCBjewiRYZLsCpEeiQeX2rdvH6amptjb2+Ps7EzXrl1lTsp8LCAggFmzZuHj4yM/VHNI37592bp1K2fOnJEELJs9efKEhg0bUr16dbZt2yb7dTaKO5b4+vpSr149RowYwfvvv4+JiYnWoQkNxMbGEhQUFD+vb3BwMMWLF4/v5yuDXAmRLpLsCpGcxJPH//7771y8eJFSpUrRsWNHXFxcZP68fE6n0+Hr68vUqVM5evRofJIrTT2z35YtW+jSpQteXl68++67WoeTLwQGBtKqVSsWLlzIoEGDtA4nzzt16hTffPMNa9asoXLlynh4eEhXCMGVK1fiR3fetWsXsbGxNGjQID7xtbGx0TpEIXIjSXaFiBMeHs62bdvw8fFh586d0jxZvCQ6Opq1a9cyc+ZMLly4gJOTE19++SVNmzbVOrR84c6dO9StW5dOnTrx/fffax1OvjJx4kTmzJnD0aNHqVu3rtbh5AuXL19m4cKFLF++nGLFijFgwAAZ5E4ASWd72Lp1K2FhYVStWjW+1rd9+/aYm5trHaYQuYEkuyJ/O3v2LD4+PvGTwJubm8c3T+7WrRsVK1bUOkSRC0RGRvLTTz8xbdo0bt68SY8ePRg3bhxvvvmm1qHlKy4uLpw9e5aTJ0/KD/4cFhMTg4ODA48fP+bYsWNSy5iDwsLCWLp0qUxfJpIVGxvLyZMn47taBQcHY2FhQZs2bXBxceGdd96R7h4iP5NkV+Qvz58/JyAgAG9vbzZv3sy1a9d47bXX6NChAy4uLnTo0AErKyutwxS5RNzcmLNmzeLevXt89NFHTJgwQS6CaGDp0qUMHjyYPXv20KpVK63DyZeuXLlCw4YN6d+/P3PmzNE6nHwnPDycxYsXs2jRIqKioujTpw+jR4+mQoUKWocmcpGrV6+ya9cuvL298fPzIzo6moYNG0pzZ5FfSbIr8j5pniwy6s6dO3z77bcsWLCA2NhYevfuzeeff065cuW0Di1funz5Mg0aNMDDw4Np06ZpHU6+9sMPP/DJJ5+wa9cu3nrrLa3DyZeePHnC999/z9y5c7l9+zbu7u588cUXMte0eMnTp0/ZvXt3fAu2mzdvUqVKFdq1a4ezszPt2rWjQIECWocpRHaSZFfkTWfOnGHLli14e3tz/PhxChQowFtvvRU/b1358uW1DlHkQqGhocybN4+VK1diYWHBoEGDGDZsGMWKFdM6tHwrJiaGli1bEhkZyeHDh6UfWi7Qo0cPAgICOHXqFCVLltQ6nHwrKiqKdevWMX36dP766y+cnJyYOHEiTZo00To0kQvpdDqOHDmCt7c3vr6+nD59GisrK9q3b88777xDp06dpGm8yIsk2RV5g06nIygoiM2bN/P7779z+fJlypUrF197+9Zbb1G4cGGtwxS51OXLl5k9ezY//PAD1tbWDB8+nH79+sk+kwtMmjSJWbNmycBIuciDBw+oX78+NjY2/Pbbb1qHk+/FjQ4/ZcoUjh07JqPDi3QJDQ3F19eXrVu3sm/fPmJjY2nVqhVdunShc+fO0l1H5BWS7ArDFTc9kJeXFxs3buTGjRu8/vrruLi44OrqKs2TRZpOnjzJvHnzWLNmDVWqVGHMmDH07t0bMzMzrUMTQHBwMLa2tsydOxcPDw+twxGJHDhwgLZt27J8+XL69OmjdTjiPzLvt8iMuObOXl5ebN26lYcPH1K7dm1cXV2ln68wdJLsCsMiB2SRFeJ+EPr6+lK/fn2GDx/O+++/j4mJidahif88ffqURo0aUbFiRXbt2iU/1nOhMWPG8N133xESEkLNmjW1DkckEhgYyMyZM/H19aVOnTqMGjWKnj17YmpqqnVoIpdLrSLBxcWF1q1by34kDIkkuyL3ixtgysvLCz8/P2JiYmjevDkuLi50796d6tWrax2iMAB6vR4fHx9mzJhBUFCQ1HrkcgMHDmTdunWcPn1amtPlUtHR0djZ2aHX6zl06JC0iMiFzpw5w5w5c1i7di0VKlRg2LBh9O/fn0KFCmkdmjAAOp2OEydO4O3tzfr16zl//jylSpWiY8eOuLq6ygBXwhBIsityp9DQUDZt2sSmTZs4fPgwBQsWpH379nTp0gVnZ2cZREGkW1x/tsmTJxMSEkKnTp0YP348tra2WocmUrBz5046duzImjVr6NGjh9bhiFScO3eOxo0bM3LkSL7++mutwxEpuHr1Kp6enqxYsQIrKysGDhwog++JDDtz5gybN29m8+bNhISEUKRIETp16kT37t3p2LGjjHMhciNJdkXucfnyZTZu3MimTZs4fvw4RYsWxcXFhW7dutGuXTs5iIoMiRupdNq0aVy6dAknJycmTZokTd1zufDwcOrWrYujoyOrV6/WOhyRDt999x1DhgyROZANwO3bt/nuu+9YsGABOp2Ojz/+WKZVE5kSGhrKli1b2LRpEwEBARQsWBAnJye6d+9Op06dsLKy0jpEIUCSXaG1v//+m61bt+Ll5cWhQ4coVqwYzs7OuLq60r59e5lmRGRYcnNQTpgwQfoUGgg3NzeCgoI4ffo0xYsX1zockQ56vZ7OnTtz8uRJTp06JZ+bAXj06BE//PADs2bN4t69e3z00Ud88cUXVKpUSevQhAG6e/cuvr6+eHl5sXPnTkxMTHB0dMTFxYWuXbvy2muvaR2iyL8k2RU57+zZs3h5eeHj40NwcDAlS5bEyckJV1dXOnToIP2+RKY8evSIJUuWMHv2bKKioujTpw9jxoyROZUNyKpVq+jXrx9+fn60bdtW63BEBty5c4d69epJjbyBiYyM5KeffmLatGncvHmTHj168Pnnn1O7dm2tQxMG6v79+3h7e780zoqrqyvu7u6ULVtW6xBF/iLJrsgZcQnuhg0bOHfuHK+99hodOnTA1dWVjh07ysh+ItNu3brFkiVLmD9/Pqampnz22Wd4eHhIv24Dc+PGDd5880369evH3LlztQ5HZMK2bdtwdnZm06ZNdO3aVetwRAZER0ezdu1aZs2axfnz53FycuKLL76gefPmWocmDFjiGTR+//13nj17Fp/4urq6Ym1trXWIIu/zQv+C9evX6wG5yS3N2/r161/cfZI4e/asfsKECfpq1arpAX3lypX1I0aM0AcGBupjY2NTfa7Wr01uhnGrW7euvkCBAvry5cvrv/nmG/3jx49T3a8y4t1339X89cnNMG7ZRc7HckvvLa3zcXrFxsbqN23apG/SpIke0Ds6Omr+2uRmGLd333031X3r8ePH+vXr1+vd3Nz0FhYWemNjY33r1q31y5Yt09+9ezfV58r5WG7pvSVjQ4rVaetnpPSIEOA+Lvnl165dY+3ataxdu5ZTp05RoUIF3N3dcXd3p3Hjxhma4mVYT7Ctm0UBizzHcw2YFi3KokWL6NWrV7ZMf9C8LgzvmeXFijwi6AzMX5P925HzsUhNSufjzDA2NqZbt25069Ytfj5ykPOxSJ1nOo6DlpaWuLm54ebmxrNnz9ixYwfr1q1j2LBhDBkyhHbt2vHee+/RuXNnLCwsXnq+nI9FalI7H6eY7Lq9nV3hiLwg8cn13r17+Pj4sHr1anbv3h0/yNSUKVNeqYmybV3ZD0XKvPyB4mXp169ftm2jQmnZB0Xq5ufANmQfFKnJymQ3MXt7e+zt7TEyMpLzsUiVl3/G1i9UqBBdu3ala9euPHv2DB8fH37++Wc+/vjj+MGtevXqRefOneMHKpXzsUhLSudj6SgpMu3w4cOsXr2anTt3YmpqyltvvcX69euTHJyEEEIIIYRITqFCheL78MYNbrV69Wp69OhBkSJFcHFx4ebNm5TN+sZbIp8w1joAYbgWLFjA8+fPWblyJbdv38bb2xtXV1dJdIUQQgghRIYUL16cXr164efnR2hoKJMmTeLKlSscOXKEWJ3W0QlDJcmuyLSVK1fi5+dHr169sLS01DocIYQQQgiRB1SoUIGhQ4cSEBBAhw4dMDXROiJhqCTZFZmW3AACQgghhBBCZJWCBQtqHYIwYJLsCiGEEEIIIYTIc3JNsjt9FRRqASZNweaD5Ndp/CHsC067rPafgVFjePA4fduO1cHc1VDXXcVQqAVUcILuY16Oz6ixupk2hSouMPPH5Nc5f/Xl7UxZqR6r4gLXwuDQaSjZVi2r1Aku/QN/31D/mzeH2T+r59n3Tdjui7cOQ9L3GvObFz+vArZqv9pxKOu24eUPRRzgkylZV+aLccfd7Ptm3TYyEkN69mNQ72vjD6GwnXpPGvZ8uSytXk9+otOpKSDq9VDvewFbsO4Afb7OWDnpOd6JvEWL76oW5zY5vuUuid/D/SEJyz/4Uh13anZL/2+5zPKYo7a1cXf2xJQT+8npv1SZ3295+bHlv4GVAxg3AedhWbfNvCI/5h/pec15Sa5Jdsf3gaHvgUtLCP4l+XWOr4bWNmmXtXMxmKTyyjzmJL3fbyosXAdTBsKNHRC+G3YsggJmL8fXuRXoj8ODffD1pzBuMQScTLqOqYk6uCQWFQ0rN6v/D/8IlcpCi3pwfhOUKwWfdIHqFeF1a3BvB9sXwpheav2WDSEiAGKOwvsdYZArPD8ED/dDg5ppvx/50YufV9hO6NgCXD+HZ5FZsw1XR5iYxbPevBi37pj6rO3qZ+12XpT4O5HR/TgyCtw+h1lD4P5e+GszvPm6tq8nvxo8Czx/VcemGzvg3h7Y9W3qx8M4ye0DqR3vstqLx2WRs171u5qZz0+Lc5sc33KXuPewWgUYMpv4QYh+mQJdWsPF36CYVfbG/yupRQAAIABJREFUsHA0ODbL2phSO56+yn6S0vfs1+3q+/PD1pcf698NpnwKTf8HPjkxV5qByY/5R3pec16Sa5LdVxUVDZv2pL2eTgdbDyTc/+uaOjjEHcSKFwGLQlCnGqyZlnI5loWhVycoVADuPkz6WLe28JMPPI9KWLZuF7Rp/HI5rxVX2576vbqCuGorVCwDbzVNWGfGZ1C4YMIXyMgICphDEQuYKTW76VK8CPR+B548hbC7WVeucTZ/g+I+61ke2beNF78TcdK7H0fHQMRzddGmgDmUKZHydycnXk9+deVfWPYb/Dr15WPZigmpPzelfSBOase7rJDW9kXOy8h3NbOfn5bnNjm+5S5LxqmWbUs2ah1JgszGlNb3IbP7SUrl6nRw5wGM/hCCzqjftSLnGGr+kR3S+17kNINJdhetB7Nm6kOLM20VlGmnmuqVaQdGLzznyyVQtp26HTyhlnUbDaE3VXX+xt3qqkgRC3WFOSMinsF3XuoKydvNkj428F14FJF0ku3lv8EQ9+TLatsERn2orh4f+QM8emQsFpG2h0/Ula2KZdQtjm1v1WS8RFu1j4G6kmvUGEZ6qn2nXHsIOp3wnAVr4fV31H739Yqk21m8QTUTsXKAtwfB1Rtq+YDpqszKzup5Fvaq2Urpt9X2nTzUgTA5U79Pf/lLN0Gtbmo/mvq9ir9EW/VdAbh9D+z6QMEW6nVdvfHydyJOevdjy8IwoS/YfwKfL4KLaZxoJy5N/XGROQdCwKpw2sey5Pb5lPaBOCkd71LaH1N7LCP7oNBW4u/quMXq86n6Dty4AxWd1O151MufX3LHo+T2u7Sk97gG6T/exZHjW+7yujXMG67ev/AHya+T3GfcZaRq1gkqkbBsCRO+U/dfdT/MbEzpOZ4lPq8nF1NGjpP+R1UyU8Ua7BvAjz6pvy6RMXk5/0hNcvtlct+p1N6LlI7XOc1gkt0h7klrOy/9AzN+gMDvVbV//Rrwz62kz5kyEK5vByf7hJ106Xh1RU1/HN59C+49ghJF1ZW29NiyX33Qli1hzEL4sNPL61Qora7SLP3vauC+YLB5E0oWTf313X+kbiLrxH1exVqrE+FWT5IMXx/0A0Qdhp8mwYrf1bJFY1RNw5efwD/b1H4Xd2I5ehbGf6tqz54cVBcp4gSfg6+WwuZv4Po2KF8aPputHlv23363bSE8PqCaQDeoBZc2q5v/Ufj3zstxGzUGn4PpK9/EGJrVUVejD5+BdTvhxBr40wu+3QCXr6vXUbMyPNyn3gvLwi9/J+JkZD+ePAACVkJMLDT+AIbOTf5zMGqsTuIi6919mL7mfsnt8yntA6kd71LbH1N7LCP7oMh5KX1XZ3wG43qr45ZFIbUv/LkRCpq//Pm9eDzS65Pf79KS3uPakT/Sf7yLI8e33KdfV7Ctq86xL0rpM/5xUsI67zjAZ24J97NiP8xMTGkdTxOf1yH5mDJynNy0B7q2Vv/36vT/9s49zuZq///PwbgMIvnJ5J775SQajEujJCJ3YyZFHCmXIzmivpyUjlvS5SRKv5AKmUthTtRxCZUMhvLtiEaJkopyyW0MM/v7x9s89t4ze3/ms/fs23z2+/l4eJjPWmuvz/p89mu/12etz3u9F7zzofvJc8VzrD7+cIcrXeb/TR360f29cPf7CAbFZrCbn8hS8n+eG6kNXO7BVaokNKsn625cUbkinDorRjCPn0/I7HVEjMxcOJLnM395B+x+R2ZsHBeS5/FoogSg+u/3sCAJJj7g/lqu5ogff9pLsO5z1wEGFO/I+75mjBH3ivo17XknT8sb1Uqdof8kyc9PZClxKbl8LW9LBnSLlfXWkaXkQS+Pz7+CzrfJWrNKFUQD2/c513djFSgdKXWWiZRZvdrVocp1zhrNa7ctA3p2NF9/vZvEU6BECdh/WIITRXeHX36XoAUdWorBHD0Hsi5D1crG988THTe7GV6YABnLxR0/bzYz//XceIPxORXvqFxRXPSNHnLMaN4RI3tnpEejPE81qAQWo9/qrLFi8+IeloFvxSjXdeSRZ4/q1/RMd+7qcWfXdu33zt6pfQs9Fk+DNVtlwswRd9+xGYqqQ1+1yVW/7s4mm7WTFy7Jet2KcfLMOnIGHDshE+iKf7DS+MMdhT0r5P2m8t5Ku7oXRfnN+ppiO9itEw3DekGLBLmJ11eE4b09r6dza7iYBeu329NqVIO9K+TvtS+5/lzpSAlS8VBf2LzLWawAca3hlobw5HyZkasT7b4NT7wC4xLhng7yVvGxFwp3lVI8Y/K1N7Bjn7OnLU2DXBscToMdy8zVk5MjM3NmsNnMz9gZ4S4IllH9Nhv0jrN3rLYMuLeTDD4yP4CYpnDfVGe3HFd4ouM8GtWWwBuHf3ad/+yowutQPCeulawvXPe5+zLeaB4Kt3dQuB7z8jzVoBI88v9Wr1yVybkykbDyY/P1eKu7/Liza97aO7VvoUd0VXh1sniCOJoZd9+xJ3irQ3+0Ka9fd9cms3bygy2yvt2xDW2awbJ/m78+xTOsNP5wh9nfitG98MVv1leE9GA3J1f8vR3X2eTx5wXY/728Nj//mbjLlS9XeJ0REdJhHz8pa40a1ILHh0hEtDVbpd6sbPezD7k2eXOSmyuv49/4QAyLq4e8RxPhoy+cXV3zs+zfMtvYubUcj+gjrjiDp3o++624p0xpeGUSLF8P76yTtMvZ8paiTKT5gA63NYUN6bAvU0LLO8703t5KXEa+ypQ1wguS7d+rL/Ck/jbN4JPdEvQs+4rM0uXkwu5v5HjUQBjaE/YeLPibyE9hOj5yXB4Cfjsl9ezaL/enbXPX5XNzIXmjGGnFdzSsLS58j8yCDz4RfV7NkQArQ5+WMu40704DRvbOSI9Ged5oUAkOjr/VqznSHz8+RCK6vrjc/hsu7Pvzxta6wp1d88Te5UftW+iR2A3qRsPGdHuau++4dKT8v+eA7LRwymApWFF06EmbPLFn7tpk1k6mbZPBhiPjEmH1Fv9v2WRlwmH8YXTNZn8rRvfC3e8jGJQKzmkL8twyCfyTlS2v7x0ZOwgmvAibdkLGN1D9Bpn1+u2UBPoBGczc2wmS5sj2Gzm5EvDplUkw+y354ob3hva3QKvGsk/avMdkMfeccVDzRnjmDZlBK1VSznH/PXbXEcf2lbzmu1+pgiwOXzLNucyKj2RG5oEe4s7Xor4Yqn6PS7kOf4Wt/x9eWiGLvhvXkUjBUWXFTXbTLhFFw/7wxVKZ6QGIHS4GPSICDh+T9Z+Kaxa9L+Hcs69IkIftS8VtqE8cjHhW1lw80EOCVtXtLRHsMn+UvY2P/WbXz7zHYO7bYqAevBe6t4fhvaDLaChbRr679K9l7UTn1jKr3+fv0sl0aGmPgjt6tmiwy2hZ5zBziRzHtYaNO0XLw6fDkJ72dt82BDa+Ji7OAK2buK9/1Gxpc+xw2Pm26HzyUJk0+fMCDOwirlgHj8g+cBcuyczfe7NE446/ibPnPdNxhSjYvR8a9ZeHjfo1xd2vaT3j33XGu/7XQbgxf7KspXn2Tbj/Kbh6Vb7fmGaSn9jNteYfH+JaA0b2zkiPRnlmNDgmPqC3TaHw32rscPj2qLjS/XFWAq30GC/90J0xzt/fV5nO9sid7vK213PXt5m1a57YO0/7abVv/iX/99G6iaQv/B/41CGojrvvOKos3NcNOoyQ9YIVoiBpg+jsjQ/M6fD4SXm+3HNAbFH610VrU357FhHhvl9316boqoXbyQ8/g493iI3deC0oV/rXsr44KxuaDZL278uEaYvkLWLfie7fGIYr4Tj+WPmx8TW70+X3x5x/Uzab+3vh7vdhZitEXxNhszm/AE9OTiYxMRFbRuAb4wnHT8qN/9e1L/DPC7Lp83uz5O2b4l8iYiApKYmEhITCC3tTf0QESXMg4W6/VK9YgEFPAtfHk5KS4p/6Bw2C06mkzPVL9YoFSN4IiVPA5sqv2xf1F5P+WAku2h8rwSac+mMdf9gJpXth0B+nhMybXU/56At5+3nuoswSbN0jbyPq1Qh2yxRFURRFURRFsRo6/rBTXO5FsR3s9rtDQrPfdI+4mLZsCKtm211DFEVRFEVRFEVRfIWOP+wUl3tRbAe7N1SCj3TNqqIoiqIoiqIoAUDHH3aKy70I6WjMiqIoiqIoiqIoiuINOtg1wfh5UKqtvKo3w+Zd0KAfRHWEYc/Iq31Py739IdTrIxuF9/67+MSDRBGOiHH+V7It/H7G/tktGdBkoHfXqoQugdZhYVpzp1GQMgOfkDqrdJFIgUrxJ5RsIUg0+zq9JBLkkGn27TjM2EmleBJqGjSydRVud9bg8vUeX64SogRDh+709NspifBbpj1UvQsefFoih+eh/bE1CSVbWJTnxUCgg10TzJ8MXduZK3vhkoQPnzsevl8LB36Al1d6Vu7EKdlyYdl0+GmdfX/DPBZPs2/QfPbaHmt5IcoXJEvY/e9+KtIlKyFIoHUI7rVWmEaHTJO1G79thC9XyFYlSvEnlGzh9n3w1Gvw/vNw8H3ZHubF5fZ6jeykUnwJJQ2Csa0b2c+uQVuGbC2nWINg9Mfu9HQpCxK6wpmtsHeF7NG7MNn+Oe2PrUmo2UJvnxcDgQ52fcyWDKhWRfaTiq4Kjw2GlE2elfvhOFxXXvZtrVxR9qzKvLapc4Na8FBfez2vpzofj0uAp0b67/qU4oEvdGikNSONZv4I3xyGaSOhYhTUiZY95ZTwwt+2cNNO6BYrewjXuhFGx0PaNskrzE4q4YG/Nai2TjGDL3RoRN2bYOZYKFcGaleHuNZw7ITkqUYVCO7YxOhzgSIgg90Tp6DjCCjbAaK7w5Hjkv5WmrwqL9Ne0r/+TtIXJMtGxhXj4O6x9vKjZsur8UXvQ+MBsPO/MjtQvZu4ZsxaWnhbavWEEm3g4ZlyPPkVcQN4c7Uct/8rlI6V+l5Ncl9P7HBYmlYw/dujsul8Hg1rQ+ZRz8o1vxkiS8Gu/XD+ImxMh65tC9ZxOVs2EO/Y0vCSlWuoDr0vl19rRhrd9V84f0lc6ct1kHv7yW731xBOqAY9K2eks+yr8nCXR/ObZcP7/KiddEY16Fm5oti6pA3iYl/5Dug53n7vFNWhN/1xYXq6mgMHj8C2PdC3s6Rpf+we1aBn5bwdm5j9nD8JyGA3dTM0qgNnt0Lay1AhCr7KhHHPw1vPwJ/bYMGTkJMLew7AM4tgzYtwbD3UqCblAN6YKvs4tWsBr0+Rm7nqP/DlSvgmRdw2XD3sOLLxNXlAmv6IHD/1EAzvDQ/3l+Mdb0F2Orw93S4yV6QvgxF9CqZfzIKype3H5ctJmiflKkTB7L9Bu2Hyozr6K4yJL1jHO+tgcHejq1UcUR16Xy6/1ow0euY81KwGHy+A01vlmkb80/01hBOqQc/KGekstgVsSJfOOSsbfj4Bl68UrF/tpDOqQc/KFcXWpS+D01vgQCpElYUxzxnfj3BCdeh5ucL01KAf/CUResfB7a0kTftj96gGPSvn7djE7Of8SUAGux1awtptMHoOZF0WH+5P98KdMfKDLFNaXpnf2gg+/wo63yZ/V6oAjybK2ixH6t0EXdpAiRKw/7Ds7xTdHX75XWa1jGhSVwzBnGVyvDAZxg6Sv0+eltmySp2h/yTIdvHgVBjly9mDpID4v99QybNyew7IrNCh1XDuU+h5bZG4I7m54low8C7P2xiuqA69K+dKa0YaLV8WSpWU+1O2NAztCUd/gTPnPL8Oq6Ea9Kyckc56x8H998hMdvVusl43/95+aicLohr0rFxRbF2daCgdKe6Ajw+BL/Ldu3BGdeh5ucL0dGi1rLP84yzcN+Vandofu0U16Fk5b8cmZj7nbwIy2L21EWR+ADFNZeHzqg1yQ0qYOLvNBhER7vN6xzkv2L+3U+F1ThsJS9aIz/i3R6F1E0lfmga5NjicBjuWmb48JxrXgUMOvugHj4iB8qTc5t3QqrH4wFeIkhmS9dudP796K/ToKLNJijlUh96Vc6U1I402ry/1XL5mMK/myL1zdDkNV1SDnpUrzBa+NFHedJzZCk8Ms7c/D7WTBVENelbOV7buao7aQEdUh96XA9d6iiwla3ZHDYCPd0ia9sfuUQ16Vs7bsYmZz/mbgDwC7P5GZiJGDZRZpb0HZdZk00654ItZcO4inD0v6Vv3iCvB2fPiI9+5tet62zSTtQfb9kr9J0+Lu0FhNL8ZenaCrmOcF1RfzpaZrzKRzl+2J9wZI6G2UzfLbM78VdD/Ts/K1Y2We/bDcbk3qzbAzTWcP78oVSLzKeZRHXpXzpXWjDTappkEzHj2Tfjzgqx/6XCLzJKGO6pBz8oZ6WzrHtnO4MpVuY8zFhd0jVI7WRDVoGflvLV1n38lb2eyr8Cvf8C8d8098IYLqkPPyhnpafl6SPtUbOEfZ2HxGom+nHc/tD92jWrQs3Lejk3MfM7fBGSwe/AItEiUhfWbd8Mj/eG2pvDy47JP1PV3QpfREj2udRN4dhT0+bss2P71DwmvDbIIPCdX3NbOnof2t8DkoTB4qizanvQvmZUxw7SRMlsR5yDWxG4i9rq9RdSZP8Lz78CEF0X8Y+ZIOojv+ZK1BeuNKgvvzYYnXoH6faFFfZj4gOSdvwg1eojrg1G5+LugVyc5x//rKude6vDKf0sG3NJQIuvl58Gn4dbBcp9q9IB/vGbufoQDqkPJM6tDcK81I41GREDqXNmr7ca7ZYZ58TRz98PqqAYlzxe2sFIFmLJA9gKMfwL+MUKiM+dhZCfDGdWg5PlCg0a2rloVca2vGCfrKK+vCC9M8OSbsjaqQ8kzq0MjPdWqDuPmyr2s3xeOn4R3Z0ie9sfuUQ1Knr/HJoV9LhBE2Gw2m2NCcnIyiYmJ2DIC2xCleBERA0lJSSQkJPin/ogIkuZAwt1+qV6xAIOeBK6PJyUlxT/1DxoEp1NJmeuX6hULkLwREqdAvm7Ud/Vrf6yYQPtjJdhof6wEG4P+OKVUMBrkT46dkFkXVzzUV2e0lMCgOlSCjWpQCTaqQSUUUB0qwUY1GFwsN9itWQ2dBVeCjupQCTaqQSXYqAaVUEB1qAQb1WBw0RiViqIoiqIoiqIoiuXQwa6iKIqiKIqiKIpiOXSwqyiKoiiKoiiKoliOkB3sXrgEPcZD2Q6yf1jMUHto7aLiy7rMsCUDmgw0LvP2h1Cvj4SV7/132ZdLCTxW0N1vp6DRACjTHqreJdtRXbpsz381Cer0knD7Q6ZBVnbBOr77SSJ8Ov4r2Vb2X1MCixU06cifF0R/3/3kOl9tYWhgJd2564Mr3O5s45avd/35zbugQT/Z5mrYM7KfqeJ/wkGDnmjLzLOk4l+spEkovD8Ga+guZAe7yz+C6KpwaTv0iYOMd+GO2+z54+eZryt/2fx1+ZMFyZC0wVhIJ07JPl3LpsNP6+BqDsxcEpj2Kc5YQXeXsiChK5zZCntXyGbeC5Mlb/s+eOo1eP95OPg+7N4ve/e5YvE0Cahgy4Cz22BYL6ha2f/tV5yxgiYd+eebslm9K9QWhg5W0Z1RHzyyn93G2TJgiItoqRcuwX1TYe54+H4tHPgBXl7p/3Yr1tegJ9oy8yyp+B+raDIPo/4YrKO7kB3sHjwCN9eQDbHzk5srMypm8KSsPxiXAE+NNC7zw3G4rjx0bg2VK8K9nWTTaCXwWEF3dW+CmWOhXBmoXV02Jz92QvI27YRusRDTDGrdCKPjIW1bwToa1JJw+Hm8nup8rAQOK2gyj7RPoW1zKB3pOl9tYehgFd2Z6YON2JIB1arAwC7ykPvYYEjZ5Lv2Ke6xugY90VZRdaz4BqtoEgrvj8E6ugvJwe7CZHm4nva6uBbNWAyR7WDVBskfMBmO/iJ5qZuN68pf9tUk57pGzZa8Or3E7bN8J/hLIlS7G0rHQs/xIkqQNwzVu0GVLjBrqf0cscNhaZr319v8ZogsBbv2w/mLsDEduraVvBOnoOMIcZmI7g5Hjnt/HsUYq+nuao4Y5m17oG9nScu+KoPgPJrfDN8fM76Wy9mQ/jV0bCnHqsnAYSVNnjwtLnsJd7tvo9rC0MBKujMiaYOcr/Idch5Xmvr2KNSvaT9uWBsyj8rfqkn/EQ4aNNKWWVSDgcNKmjTTHxtR3HQXkvvs/i0BDhyB6jfAUw9J2vZ99vxFU+HjHZD1ReF1uSq77nP7329MlTVi6+dDw1rwyCzIyYWFT8KZc7KW4ueTcPwkrPoPfLlSZnRaPwD3dRNDlb6saNdbIQpm/w3aDZPjlo1gTLz8nboZGtWBTxbB/34nZRX/YDXdNegHP5+ACffD7a0kLbYFLFkjnWydaMm/fMW4nnfWweDu9mPVZOCwkiafeQNmjDFuo9rC0MBKujMifZm8UfvjLDz6PIx5Dj6a71zmYhaULW0/Ll9O0kA16U/CQYNG2jKLajBwWEmTZvpjI4qb7kLyzW4wuLGKvMpvUR/KRIorXe3qUOU6ebO1az/sPww33SOzGL/8Lm/NfMGeAzIzc2g1nPsUel4LVADQoSWs3Qaj50DWZV0zaTX8qbtDq2Ud0B9n4b4pktY7Du6/R2b8qneT9bpVrnNfR26uuFUNvMueppq0Nv7Q5Lvr4Z4OcEMl43JqC8OXYPTBdaLlnNFV4fEh8MW+gmXKl3MO4nfhkl3HqklrEWgNGmnLLKpBaxPM/tiI4qY7HeyaxGaTgYJjMIt7O/mm7s27oVVjWSdZIUreoq3fLnm3NoLMDyCmqQQyyHNxUMKDouguspQYxVEDZAYxj5cmwuktEsDqiWHQuon7OlZvhR4doaSDpVBNhjfeaHLpWug70R719sIlaNgfPvzMuZzaQsUd/uyDQZZ9OC7xyKNxHTjksG784BEZJINqMtzwtQaNtGUW1WB448/+2IjiprtiOdiNiJDw7MdPut42xduyRrRpBp/shm17IfuK+Lvn5HpfnyN1oyVi7g/HxYVl1QZZAA+Snn0FRg2EoT1h70HfnFPxnOKiu+XrJfDAlavyVnfxGmjZUPK27hHXmCtXRUszFtvdRF2xKFUiljqimgwdiosmt7zh3BmXLydvb3vd7lxObWHxoLjozojPv5I1eNlX4Nc/YN67rh8S74yRLddSN8tbk/mroP+dkqeaDB5W0KCRtsyiGgwdiosmzfbHRhQ33YXkmt2122DZv+XvcxdkTeGmnZDxjfjK395KZv8bDYB5jxk/rFet7Fz226POda36j7gCdBkNr08RF7rL2RLBduNO2bN0+HT4fAlMHgqDp8q+VAO7yNYsJUvI+rJHBriOVvvg0/JmIicXavSA4b1h1lgJvtJ4ICQ/B/F3wWdfSj0XLom7wtJrrnsHj0D3cZJ+S0N4b5av77aSh1V0V6s6DJ0mD3BRZSWw1LszJK9SBZiyAEbOkGjMTz8s0ZnBWZMdW0qkyFsaQsV8azFUk4HDKpo0Qm1h6GEl3bnrg6tVkWUcE18W18CeHeGFCfKZ/LbwvdnwyEyxqQl3w8QHpJxq0n+EgwajyrrXVn4NuqtDNRg4rKRJd1hVdxE2m83mmJCcnExiYiK2jGA1SSkORMRAUlISCQkJ/qk/IoKkOd5HilOsz6AngevjSUlJ8U/9gwbB6VRS5vqlesUCJG+ExCmQrxv1Xf3aHysm0P5YCTbaHyvBxqA/TgnJN7uecOwE1HKxETzIbMbiaYFtjxIeqO6UUEM1qQQD1Z0SbFSDSqihmgwtiv1gt2Y1dNZbCTiqOyXUUE0qwUB1pwQb1aASaqgmQ4tiGaBKURRFURRFURRFUYzQwa6iKIqiKIqiKIpiOXSwqyiKoiiKoiiKoliOkBvspmyC6+JkaxRviBkqe4mGIuPnQam2sqeaGTbvggb9IKojDHtG9uTyptyWDGgysGhtDydUg3bMaPD4SWgaD+U6QLW74eGZEiIfJDx+owFQpj1UvUvC2F+67LvrsTKqQztqC4ODatCOWQ2Ca52pLfQO1aCdotpB1aD3qA7tFMf+OOQGu4O6yt6fZhk/z/k441244zbftslXzJ8MXduZK3vhEtw3FeaOh+/XwoEf4OWVnpdbkAxJG+C7n3xzDeGAalAwq8GLWZDYDU5vlWvfvg9eS5W8S1mQ0BXObIW9K2Qj8oXJvroaa6M6FNQWBg/VoGBWg+BeZ2oLvUM1KPjCDqoGvUd1KBTX/jjkBrsAJUy2KjcX0j71b1uCxZYM2fR+YBeIrgqPDZaZJU/LjUuAp0YGrt1WQTVoXoMNasH0R6BsaahdHTrcYjdgdW+CmWOhXBnJi2stIfkVc6gO1RYGG9WgeQ2Ce52pLfQe1aBv7KBqsGioDotvf1zkwe7o2fL6u+MIcWOMHQ6ZP0reqNmy2fmi96HxANj5X5i5BKp3gypdYNZSez2vvAf1+oh7xT/fNHfuAZPh6C9yjtTN8GoSRLaDVRucz1+nl9RbvhP8JVFcLUvHQs/xIkpw3y531OoJJdqIyybA5FfkPry5Wo7b/1XOUaWLtMsdscNhaVrB9G+PQv2a9uOGtSHzqPfl8nPilHxnZTtAdHc4crzwz4QqqkE5DpYG8/Pjr9CyoXPa1Rw4eAS27YG+nSXNShoE1WGwdai2UDUYbA2aweq2UDUox6FsB62uQVAdFgcdusJfOizyYPdfkyAnF5KegxOboGk9mPCC5L0xFUqWgHYt4PUpkP41rPoPfLkSvkkR94nvj8Gu/TB1IayYCec/g0lDzZ170VQoU1r2soq/Cx5NhLva2vPfuJa/fj6c+1TcEG5tDN+tkX+bdsHPJ0XortplxMbXZHZs+iNy/NRDMLw3PNxfjne8Bdnp8PZ0u8hckb4MRvQpmH4xS96U5VG+nKR5Wy4/qZuhUR04uxXSXoYKUYV/JlRRDcpxsDToyOGf5XqG5NtMvUE/Mea94+D2VpJmJQ2C6jDYOlRbqBoMtgbNYHVbqBqU41C2g1bXIKgOi4MOXeEvHfrMjbn6DVAxCsa27UZwAAAFC0lEQVQOgh1fO+fVuwm6tBEXgP2H4aZ7ZMT+y+8yu7QlA7rFivtjZCnnG+QLbqwCpSOhRX0oEwnXlRcXjirXSSCdXftdt8uIJnXFUMxZJscLk+XaAU6elpmZSp2h/yTIvuJ5m8uXg6xs+/GFS3BDJe/L5adDS1i7DUbPgazLULWy520MNVSDwdGgY/74eZA6F6LKOucdWi3rNv44C/dNkTQrahBUh2oLg49qMLi20IhwsYWqwdC1g+GiQVAdhrIOXeEvHfp8zW72FZkxcYXNJl+CLcP+795OkJMjsxzBwl27CmPaSFiyRlwjvj0KrZtI+tI0yLXB4TTYscy7NjWuA4d+tB8fPAJ1or0vl59bG0HmBxDTVBaR57lXWAHVYGA1CHDuIoyZAy9MkBnU/ESWEiM+agB8vEPSrKxBUB2C2sJgoxoMvC0sjHCzharB0LOD4aZBUB1C6OnQFf7Soc8Gu5ez5YF3/ir3EcfaNINPdsO2vSK8k6fFzeC2prAhHfZlwplzsOeAuXNGREgo6+MnnWcQPMVduwqj+c3QsxN0HQMP9bWnX86WGaAykc5ftifcGQO/n5FX+r/8Lve1/53el8vP7m/kWkcNhKE9Ye9B79oZSqgG7emB1ODpP+WN7uxxMqvoyPL1EqjhylWZRV68xr6e14oaBNWh2sLgoxq0pwdSg0aEmy1UDdrTQ8UOhpsGQXUYijo0wl869Nlgt0YPWUR9+k/xlQdZhJ2TKwudz56H9rfA5KEweKosjp70L1mE3b09DO8FXUZD8wTZC2zlx/IFG1G1MrRqLPuGvZUGE16ETTth3FzZz2r0bPlyu4yW7VBmLoEVH8E762Do03Ke4dPdt8sM00bKbEVca3taYjf5gur2lnZk/gjPv2Nv35g59v222g2DJWsL1htVFt6bDU+8AvX7ipvDxAck7/xFud/b9xmXA9lH7dbB8j3U6AH/eE3SDx6BFomyMH7zbnikv7nrDWVUg/a0QGpw5cew7N8SGCEiRv7FDpdytarLvSjfSeo4fhLevbZPnRU1CKpDtYXBRzVoTwukBsG9zsLNFqoG7WmhYgfDTYOgOgxFHULg++MIm81mc0xITk4mMTERW4a5CrKyJdJZzi7zYbmV4k9EDCQlJZGQkOCf+iMiSJoDCXcXXlY1GJ4MehK4Pp6UlBT/1D9oEJxOJWWuufKqw/AjeSMkToF83ajv6tf+WDGB9sdKsNH+WAk2Bv1xSilfnSTX5vsFwMdOyNsiVzzUFxZP8/EJQ+TcineoBpVQQHWoBBvVoBJsVINKKKA6VACKPNh98Gn5v+Vg2Lvct4u5a1bD9Iy2rwnmuRXPUA0qoYDqUAk2qkEl2KgGlVBAdag4UuTBbvJzvmiGoniPalAJBVSHSrBRDSrBRjWohAKqQ8UR9WRXFEVRFEVRFEVRLIcOdhVFURRFURRFURTLoYNdRVEURVEURVEUxXLoYFdRFEVRFEVRFEWxHG4DVA16MpDNUJSCvLwSUjYFuxVKqJL+NcTG+f8cagsVdxw7EZjzqAaVYKP9sWKE9sdKsDHqjwsMdmvVqkV8fLw/26NYgPh40Yr/6lcNKsbExkH79u39Vr8/61asQc3rIb6x/+rX/lgxg/bHSrDR/lgJNkb9cYTNZrMFtjmKoiiKoiiKoiiK4ldSdM2uoiiKoiiKoiiKYjl0sKsoiqIoiqIoiqJYDh3sKoqiKIqiKIqiKJbj/wDnTjEwRbfdFgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "path_to_png = model.plot_ensemble_model()\n",
        "from IPython.display import Image, display\n",
        "display(Image(filename=path_to_png))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "93vNiSqRc-58",
        "outputId": "abf7c73d-1365-43a8-d1fb-b4da2a70d5bd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_fold' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d7833e3bc953>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfull_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfull_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_fold' is not defined"
          ]
        }
      ],
      "source": [
        "## Making sets of all 10 folds\n",
        "\n",
        "full_train = None\n",
        "full_test = None\n",
        "\n",
        "for fold_number in range(1, 11):\n",
        "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed)\n",
        "    if full_train is None:\n",
        "        full_train = train_dataset\n",
        "        full_test = test_dataset\n",
        "    else:\n",
        "        # Use pd.concat to combine TabularDatasets\n",
        "        full_train = pd.concat([full_train, train_dataset])\n",
        "        full_test = pd.concat([full_test, test_dataset])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Training the model on all 10 folds\n",
        "\n",
        "model = fit_gluon(full_train, time_limit=100)\n",
        "test_score, leaderboard = evaluate_gluon(model, full_test)\n",
        "print(f'Full Train: {test_score}')\n",
        "display(Image(filename=model.plot_ensemble_model()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f6AT7fM8Q7E",
        "outputId": "20123959-4591-43b0-a183-3ea511364623"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240702_102619\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Tue Jun 18 14:18:04 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.06 GB / 12.67 GB (79.4%)\n",
            "Disk Space Avail:   45.45 GB / 78.19 GB (58.1%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ... Time limit = 100s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240702_102619\"\n",
            "Train Data Rows:    79965\n",
            "Train Data Columns: 62\n",
            "Label Column:       oz252\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10330.72 MB\n",
            "\tTrain Data (Original)  Memory Usage: 27.15 MB (0.3% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 20 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 11): ['oz42', 'oz46', 'oz50', 'oz100', 'oz108', 'oz111', 'oz112', 'oz113', 'oz115', 'oz222', 'oz234']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('category', []) : 11 | ['oz42', 'oz46', 'oz50', 'oz100', 'oz108', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  9 | ['oz40', 'oz69', 'oz71', 'oz73', 'oz79', ...]\n",
            "\t\t('float', [])    : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['oz1', 'oz2', 'oz3', 'oz4', 'oz5', ...]\n",
            "\t\t('int', ['bool']) :  9 | ['oz40', 'oz69', 'oz71', 'oz73', 'oz79', ...]\n",
            "\t1.6s = Fit runtime\n",
            "\t51 features in original data used to generate 51 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 26.31 MB (0.3% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.66s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.03126367785906334, Train Rows: 77465, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 98.34s of the 98.34s of remaining time.\n",
            "\t0.9947\t = Validation score   (r2)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t1.22s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 97.01s of the 97.01s of remaining time.\n",
            "\t0.9947\t = Validation score   (r2)\n",
            "\t0.1s\t = Training   runtime\n",
            "\t1.22s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 95.66s of the 95.66s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's l2: 0.000349576\tvalid_set's r2: 0.560283\n",
            "[2000]\tvalid_set's l2: 0.000223508\tvalid_set's r2: 0.718859\n",
            "[3000]\tvalid_set's l2: 0.00014857\tvalid_set's r2: 0.81312\n",
            "[4000]\tvalid_set's l2: 0.000104798\tvalid_set's r2: 0.868179\n",
            "[5000]\tvalid_set's l2: 7.54596e-05\tvalid_set's r2: 0.905082\n",
            "[6000]\tvalid_set's l2: 5.56457e-05\tvalid_set's r2: 0.930006\n",
            "[7000]\tvalid_set's l2: 4.17541e-05\tvalid_set's r2: 0.947479\n",
            "[8000]\tvalid_set's l2: 3.21521e-05\tvalid_set's r2: 0.959557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tRan out of time, early stopping on iteration 8280. Best iteration is:\n",
            "\t[8280]\tvalid_set's l2: 2.9896e-05\tvalid_set's r2: 0.962395\n",
            "\t0.9624\t = Validation score   (r2)\n",
            "\t97.06s\t = Training   runtime\n",
            "\t1.33s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 98.34s of the -3.91s of remaining time.\n",
            "\tEnsemble Weights: {'KNeighborsUnif': 1.0}\n",
            "\t0.9947\t = Validation score   (r2)\n",
            "\t0.05s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 104.04s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2053.2 rows/s (2500 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240702_102619\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Train: {'r2': 0.9903062537074445, 'root_mean_squared_error': -0.0029292002708376065, 'mean_squared_error': -8.580214226675108e-06, 'mean_absolute_error': -0.0002855074196261087, 'pearsonr': 0.9951420695870777, 'median_absolute_error': -1.546478267666629e-08}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Training the model again without the least important columns\n",
        "\n",
        "columns_to_drop = ['oz173', 'oz1', 'oz4', 'oz181', 'oz11', 'oz31', 'oz178', 'oz83', 'oz87', 'oz234', 'oz96', 'oz135', 'oz206', 'oz40', 'oz73', 'oz42', 'oz112', 'oz113', 'oz71', 'oz222', 'oz108', 'oz46' ]\n",
        "\n",
        "full_train_drop = full_train.drop(columns=columns_to_drop)\n",
        "full_test_drop = full_test.drop(columns=columns_to_drop)\n",
        "\n",
        "model = fit_gluon(full_train_drop, time_limit=100)\n",
        "test_score, leaderboard = evaluate_gluon(model, full_test_drop)\n",
        "print(f'Full Train: {test_score}')\n",
        "display(Image(filename=model.plot_ensemble_model()))\n"
      ],
      "metadata": {
        "id": "XntN0zoN-4kC",
        "outputId": "d7a5b102-029b-4c63-a4ea-f0619360fa96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'full_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6e5efe152274>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'oz173'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz181'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz11'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz31'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz178'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz83'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz87'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz234'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz96'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz135'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz206'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz40'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz73'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz42'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz112'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz113'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz71'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz222'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz108'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oz46'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfull_train_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfull_test_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'full_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing to see if TabPFN works well\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tabpfn import TabPFNClassifier\n",
        "\n"
      ],
      "metadata": {
        "id": "S9V5pNpp-MiG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqrx-Gv5c-58"
      },
      "outputs": [],
      "source": [
        "## Creating an ensemble of the 10 models??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kroZsfJbc-59"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
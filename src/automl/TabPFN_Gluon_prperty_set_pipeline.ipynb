{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline** for running tabPFN and AutoGluon together and make an ensemble. Made to run in Colab or StudioLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### installments\n",
    "\n",
    "!pip install autogluon\n",
    "!sudo apt-get install graphviz graphviz-dev\n",
    "!pip install pygraphviz\n",
    "!pip install tabpfn\n",
    "!pip install sklearn\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to drive for dataaccess\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "base_path = '/content/drive/My Drive/361092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for loading one of the 10 folds of the property dataset and concatinating the X and y values for train and test respectively.\n",
    "\n",
    "#base_path = '../../data/361092' # Use this when running locally\n",
    "\n",
    "def load_fold(fold_number, random_seed=random_seed, sample_size=None, concat_test=False):\n",
    "    df_X_train = pd.read_parquet(f'{base_path}/{fold_number}/X_train.parquet')\n",
    "    df_y_train = pd.read_parquet(f'{base_path}/{fold_number}/y_train.parquet')\n",
    "    df_X_test = pd.read_parquet(f'{base_path}/{fold_number}/X_test.parquet')\n",
    "    df_y_test = pd.read_parquet(f'{base_path}/{fold_number}/y_test.parquet')\n",
    "\n",
    "    # concatinating the X and y values for train (and test), but keeping for test\n",
    "    df_train = pd.concat([df_X_train, df_y_train], axis=1)\n",
    "    \n",
    "\n",
    "    # Convert to AutoGluon's TabularDataset\n",
    "    if concat_test:\n",
    "        df_test = pd.concat([df_X_test, df_y_test], axis=1)\n",
    "        train_dataset = TabularDataset(df_train)\n",
    "        test_dataset = TabularDataset(df_test)\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "    train_dataset = TabularDataset(df_train)\n",
    "    test_dataset_X = TabularDataset(df_X_test)\n",
    "    test_dataset_y = TabularDataset(df_y_test)\n",
    "\n",
    "    return train_dataset, test_dataset_X, test_dataset_y\n",
    "\n",
    "# Also instantiate the target column\n",
    "label_property = 'oz252'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Isak\\Miniconda3\\envs\\automl-tabular-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Function to fit the model using AutoGluon\n",
    "\n",
    "def fit_gluon(train_dataset, problem_type='regression', hyperparameters=None, eval_metric='r2', presets='medium_quality', time_limit=100, fit_weighted_ensemble=None, num_cpus = None, num_gpus=None, auto_stack=None, num_bag_folds=None, num_bag_sets=None, num_stack_levels=None, num_trials=None, verbosity=None, ag_args_fit=None, feature_prune=None, excluded_model_types=None, keep_only_best=None):\n",
    "    predictor = TabularPredictor(label=label_property, problem_type=problem_type, eval_metric=eval_metric)\n",
    "\n",
    "    fit_args = {\n",
    "        'train_data': train_dataset,\n",
    "        'presets': presets,\n",
    "        'time_limit': time_limit,\n",
    "    }\n",
    "\n",
    "    if hyperparameters is not None:\n",
    "        fit_args['hyperparameters'] = hyperparameters\n",
    "    if auto_stack is not None:\n",
    "        fit_args['auto_stack'] = auto_stack\n",
    "    if num_bag_folds is not None:\n",
    "        fit_args['num_bag_folds'] = num_bag_folds\n",
    "    if num_bag_sets is not None:\n",
    "        fit_args['num_bag_sets'] = num_bag_sets\n",
    "    if num_stack_levels is not None:\n",
    "        fit_args['num_stack_levels'] = num_stack_levels\n",
    "    if num_trials is not None:\n",
    "        fit_args['num_trials'] = num_trials\n",
    "    if verbosity is not None:\n",
    "        fit_args['verbosity'] = verbosity\n",
    "    if ag_args_fit is not None:\n",
    "        fit_args['ag_args_fit'] = ag_args_fit\n",
    "    if feature_prune is not None:\n",
    "        fit_args['feature_prune'] = feature_prune\n",
    "    if excluded_model_types is not None:\n",
    "        fit_args['excluded_model_types'] = excluded_model_types\n",
    "    if fit_weighted_ensemble is not None:\n",
    "        fit_args['fit_weighted_ensemble'] = fit_weighted_ensemble\n",
    "    if num_cpus is not None:\n",
    "        fit_args['num_cpus'] = num_cpus\n",
    "    if num_gpus is not None:\n",
    "        fit_args['num_gpus'] = num_gpus\n",
    "    if keep_only_best is not None:\n",
    "        fit_args['keep_only_best'] = keep_only_best\n",
    "\n",
    "    predictor.fit(**fit_args)\n",
    "    return predictor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to evaluate the predictor\n",
    "\n",
    "def evaluate_gluon(model, test_dataset):\n",
    "\n",
    "    leaderboard = model.leaderboard(test_dataset, only_pareto_frontier=True)\n",
    "\n",
    "    y_test = test_dataset[label_property]\n",
    "    x_test = test_dataset.drop(columns=[label_property])\n",
    "    y_pred = model.predict(x_test)\n",
    "    test_score = model.evaluate_predictions(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    path_to_png = model.plot_ensemble_model()\n",
    "    L2_diagram = Image(filename=path_to_png)\n",
    "\n",
    "\n",
    "    return test_score, leaderboard, L2_diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making sets of all 10 folds\n",
    "\n",
    "full_train = None\n",
    "full_test = None\n",
    "\n",
    "for fold_number in range(1, 11):\n",
    "    train_dataset, test_dataset = load_fold(fold_number, random_seed=random_seed)\n",
    "    if full_train is None:\n",
    "        full_train = train_dataset\n",
    "        full_test = test_dataset\n",
    "    else:\n",
    "        # Use pd.concat to combine TabularDatasets\n",
    "        full_train = pd.concat([full_train, train_dataset])\n",
    "        full_test = pd.concat([full_test, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to turn the target variable into 10 equally distributed classes\n",
    "\n",
    "\n",
    "def create_interval_classes(df, target_column, n_intervals=10):\n",
    "    \"\"\"\n",
    "    Create equally distributed interval classes for the target variable.\n",
    "    \n",
    "    :param df: DataFrame containing the target variable\n",
    "    :param target_column: Name of the target variable column\n",
    "    :param n_intervals: Number of intervals to create (default 10)\n",
    "    :return: DataFrame with new 'interval_class' column and interval boundaries\n",
    "    \"\"\"\n",
    "    # Extract target values\n",
    "    target_values = df[target_column].values\n",
    "    \n",
    "    # Calculate interval boundaries\n",
    "    interval_boundaries = np.percentile(target_values, np.linspace(0, 100, n_intervals+1))\n",
    "    \n",
    "    # Create interval labels\n",
    "    interval_labels = [f'Interval_{i+1}' for i in range(n_intervals)]\n",
    "    \n",
    "    # Assign interval classes\n",
    "    df['interval_class'] = pd.cut(df[target_column], \n",
    "                                  bins=interval_boundaries, \n",
    "                                  labels=interval_labels, \n",
    "                                  include_lowest=True)\n",
    "    \n",
    "    return df, interval_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create interval classes for the full dataset \n",
    "\n",
    "full_train_w_interval, boundaries = create_interval_classes(full_train, 'oz252')\n",
    "train_for_tabpfn = full_train_w_interval.drop(columns=['oz252'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to fit the model using TabPFN\n",
    "\n",
    "def fit_tabpfn(data=full_train_w_interval, n=1000, device='cpu', N_ensemble_configurations=None, random_seed=radnom_seed):\n",
    "    if N_ensemble_configurations is None:\n",
    "        classifier = TabPFNClassifier(device=device)\n",
    "    else:\n",
    "        classifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=32)\n",
    "    \n",
    "    sample_full_train = train_for_tabpfn.sample(n=n, random_state=random_seed)\n",
    "    \n",
    "    X_train = sample_full_train.drop(columns=['interval_class'])\n",
    "    y_train = sample_full_train['interval_class']\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to predict using the TabPFN classifier and add the prediction as a new column in the data\n",
    "\n",
    "def predict_tabpfn(classifier, data):\n",
    "    y_pred = classifier.predict(data)\n",
    "    data['interval_class_pred'] = y_pred\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the TabPFN model, predict the training data w/o original target and the test data, and then add the intervall prediction as feature for both sets.\n",
    "\n",
    "classifier = fit_tabpfn(data=full_train_w_interval, n=1000, device='gpu', N_ensemble_configurations=None, random_seed=random_seed)\n",
    "\n",
    "tabpfn_preicted_train = predict_tabpfn(classifier, train_for_tabpfn)\n",
    "tabpfn_preicted_test = predict_tabpfn(classifier, full_test)\n",
    "\n",
    "train_with_tab_pred = pd.concat([full_train, tabpfn_preicted_train['interval_class_pred']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the tabpfn classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the AutoGluon predictor\n",
    "\n",
    "model = fit_gluon(train_with_tab_pred, time_limit=100)\n",
    "# Get the leaderboard\n",
    "leaderboard = model.leaderboard(extra_info=['r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "test_score, leaderboard, L2_diagram = evaluate_gluon(model, full_test)\n",
    "\n",
    "# # Convert the leaderboard to a DataFrame\n",
    "df = leaderboard.copy()\n",
    "\n",
    "# Set the style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for validation score (r2)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model', y='score_val', data=df)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Validation Score (R2) by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Validation Score (R2)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make different predictiors for eventual ensemble (maybe different metrics, hyperparameters, etc.)\n",
    "\n",
    "predictors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to load predictors\n",
    "\n",
    "def load_predictors(start_time, end_time, folder='AutogluonModels'):\n",
    "    \"\"\"\n",
    "    Load AutoGluon predictors created within a specified time interval.\n",
    "    \n",
    "    :param start_time: Start of the time interval (str in format 'YYYYMMDD_HHMMSS')\n",
    "    :param end_time: End of the time interval (str in format 'YYYYMMDD_HHMMSS')\n",
    "    :param folder: Folder containing the AutoGluon models\n",
    "    :return: List of loaded predictors\n",
    "    \"\"\"\n",
    "    start_datetime = datetime.strptime(start_time, '%Y%m%d_%H%M%S')\n",
    "    end_datetime = datetime.strptime(end_time, '%Y%m%d_%H%M%S')\n",
    "    \n",
    "    predictors = []\n",
    "    \n",
    "    for item in os.listdir(folder):\n",
    "        if item.startswith('ag-'):\n",
    "            model_time_str = item.split('-')[1]\n",
    "            model_time = datetime.strptime(model_time_str, '%Y%m%d_%H%M%S')\n",
    "            \n",
    "            if start_datetime <= model_time <= end_datetime:\n",
    "                predictor_path = os.path.join(folder, item)\n",
    "                predictor = TabularPredictor.load(predictor_path)\n",
    "                predictors.append(predictor)\n",
    "    \n",
    "    return predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating easy average ensemble\n",
    "\n",
    "\n",
    "start_time = '20240704_124200'\n",
    "end_time = '20240704_125900'\n",
    "\n",
    "loaded_predictors = load_predictors(start_time, end_time)\n",
    "\n",
    "y_test = full_test[label_property]\n",
    "full_test_X = full_test.drop(columns=[label_property])\n",
    "\n",
    "\n",
    "# Simple averaging ensemble\n",
    "avg_ensemble = 0\n",
    "\n",
    "for i in loaded_predictors:\n",
    "    avg_ensemble += i.predict(full_test_X)\n",
    "\n",
    "ten_fold_ensemble = avg_ensemble / len(loaded_predictors)\n",
    "\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, ten_fold_ensemble)\n",
    "\n",
    "print(f'R2 score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating more complex ensemble..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
